<!doctypehtml><html class=no-js lang=en><meta charset=utf-8><meta content=on http-equiv=x-dns-prefetch-control><link href=//oi-wiki.org rel=dns-prefetch><link href=//search.oi-wiki.org rel=dns-prefetch><link href=//api.github.com rel=dns-prefetch><link href=//www.google-analytics.com rel=dns-prefetch><meta content=width=device-width,initial-scale=1 name=viewport><meta content="Muke's Classical Department"name=description><meta content=Muke name=author><link href=https://yyhmuke.com/topics/nlp/transformer/ rel=canonical><link href=../../../favicon.ico rel=icon><meta content="mkdocs-1.5.3, mkdocs-material-9.4.6"name=generator><title>Transformer - Muke's Classical Department</title><link href=../../../assets/stylesheets/main.f5281797.min.css rel=stylesheet><link href=../../../assets/stylesheets/palette.356b1318.min.css rel=stylesheet><link crossorigin href=https://fonts.gstatic.com rel=preconnect><link href=https://fonts.googleapis.com/css?family=Fira+Sans:300,300i,400,400i,700,700i%7CJetbrains+Mono:400,400i,700,700i&display=fallback rel=stylesheet><style>:root{--md-text-font:"Fira Sans";--md-code-font:"Jetbrains Mono"}@font-face {font-family:'Fira Sans';font-style:normal;font-weight:300;src:local('Fira Sans'),local('FiraSans-Normal'),url(//lib.baomitu.com/fonts/fira-sans/fira-sans-300.eot?#iefix) format('embedded-opentype'),url(//lib.baomitu.com/fonts/fira-sans/fira-sans-300.woff2) format('woff2'),url(//lib.baomitu.com/fonts/fira-sans/fira-sans-300.woff) format('woff'),url(//lib.baomitu.com/fonts/fira-sans/fira-sans-300.ttf) format('truetype'),url(//lib.baomitu.com/fonts/fira-sans/fira-sans-300.svg#FiraSans) format('svg')}@font-face {font-family:'Fira Sans';font-style:normal;font-weight:regular;src:local('Fira Sans'),local('FiraSans-Normal'),url(//lib.baomitu.com/fonts/fira-sans/fira-sans-regular.eot?#iefix) format('embedded-opentype'),url(//lib.baomitu.com/fonts/fira-sans/fira-sans-regular.woff2) format('woff2'),url(//lib.baomitu.com/fonts/fira-sans/fira-sans-regular.woff) format('woff'),url(//lib.baomitu.com/fonts/fira-sans/fira-sans-regular.ttf) format('truetype'),url(//lib.baomitu.com/fonts/fira-sans/fira-sans-regular.svg#FiraSans) format('svg')}@font-face {font-family:'Fira Sans';font-style:italic;font-weight:regular;src:local('Fira Sans'),local('FiraSans-Italic'),url(//lib.baomitu.com/fonts/fira-sans/fira-sans-italic.eot?#iefix) format('embedded-opentype'),url(//lib.baomitu.com/fonts/fira-sans/fira-sans-italic.woff2) format('woff2'),url(//lib.baomitu.com/fonts/fira-sans/fira-sans-italic.woff) format('woff'),url(//lib.baomitu.com/fonts/fira-sans/fira-sans-italic.ttf) format('truetype'),url(//lib.baomitu.com/fonts/fira-sans/fira-sans-italic.svg#FiraSans) format('svg')}@font-face {font-family:'Fira Sans';font-style:normal;font-weight:700;src:local('Fira Sans'),local('FiraSans-Normal'),url(//lib.baomitu.com/fonts/fira-sans/fira-sans-700.eot?#iefix) format('embedded-opentype'),url(//lib.baomitu.com/fonts/fira-sans/fira-sans-700.woff2) format('woff2'),url(//lib.baomitu.com/fonts/fira-sans/fira-sans-700.woff) format('woff'),url(//lib.baomitu.com/fonts/fira-sans/fira-sans-700.ttf) format('truetype'),url(//lib.baomitu.com/fonts/fira-sans/fira-sans-700.svg#FiraSans) format('svg')}@font-face {font-family:'Fira Mono';font-style:normal;font-weight:regular;src:local('Fira Mono'),local('FiraMono-Normal'),url(//lib.baomitu.com/fonts/fira-mono/fira-mono-regular.eot?#iefix) format('embedded-opentype'),url(//lib.baomitu.com/fonts/fira-mono/fira-mono-regular.woff2) format('woff2'),url(//lib.baomitu.com/fonts/fira-mono/fira-mono-regular.woff) format('woff'),url(//lib.baomitu.com/fonts/fira-mono/fira-mono-regular.ttf) format('truetype'),url(//lib.baomitu.com/fonts/fira-mono/fira-mono-regular.svg#FiraMono) format('svg')}</style><link href=../../../_static/css/extra.css?v=14 rel=stylesheet><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head><body data-md-color-accent=red data-md-color-primary=white data-md-color-scheme=default dir=ltr><script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script><input autocomplete=off class=md-toggle data-md-toggle=drawer id=__drawer type=checkbox><input autocomplete=off class=md-toggle data-md-toggle=search id=__search type=checkbox><label class=md-overlay for=__drawer></label><div data-md-component=skip><a class=md-skip href=#introduction> Skip to content </a></div><div data-md-component=announce></div><header class=md-header data-md-component=header><nav class="md-header__inner md-grid"aria-label=Header><a aria-label="Muke's Classical Department"class="md-header__button md-logo"title="Muke's Classical Department"data-md-component=logo href=../../..> <svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M13 19v-3h8v3h-8m-4.5-6L2.47 7h4.24l4.96 4.95c.58.59.58 1.55 0 2.12L6.74 19H2.5l6-6Z"/></svg> </a><label class="md-header__button md-icon"for=__drawer><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg></label><div class=md-header__title data-md-component=header-title><div class=md-header__ellipsis><div class=md-header__topic><span class=md-ellipsis> Muke's Classical Department </span></div><div class=md-header__topic data-md-component=header-topic><span class=md-ellipsis> Transformer </span></div></div></div><form class=md-header__option data-md-component=palette><input aria-label="Switch to dark mode"data-md-color-media="(prefers-color-scheme: light)"class=md-option data-md-color-accent=red data-md-color-primary=white data-md-color-scheme=default id=__palette_1 name=__palette type=radio><label class="md-header__button md-icon"title="Switch to dark mode"for=__palette_2 hidden><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"/></svg></label><input aria-label="Switch to light mode"data-md-color-media="(prefers-color-scheme: dark)"class=md-option data-md-color-accent=blue data-md-color-primary=blue data-md-color-scheme=slate id=__palette_2 name=__palette type=radio><label class="md-header__button md-icon"title="Switch to light mode"for=__palette_1 hidden><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"/></svg></label></form><label class="md-header__button md-icon"for=__search><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg></label><div class=md-search data-md-component=search role=dialog><label class=md-search__overlay for=__search></label><div class=md-search__inner role=search><form class=md-search__form name=search><input aria-label=Search autocapitalize=off autocomplete=off autocorrect=off class=md-search__input data-md-component=search-query name=query placeholder=Search required spellcheck=false><label class="md-search__icon md-icon"for=__search><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg></label><nav aria-label=Search class=md-search__options><button class="md-search__icon md-icon"aria-label=Clear tabindex=-1 title=Clear type=reset><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg></button></nav></form><div class=md-search__output><div class=md-search__scrollwrap data-md-scrollfix><div class=md-search-result data-md-component=search-result><div class=md-search-result__meta>Initializing search</div><ol class=md-search-result__list role=presentation></ol></div></div></div></div></div></nav></header><div class=md-container data-md-component=container><nav aria-label=Tabs class=md-tabs data-md-component=tabs><div class=md-grid><ul class=md-tabs__list><li class=md-tabs__item><a class=md-tabs__link href=../../..> Home </a></li><li class=md-tabs__item><a class=md-tabs__link href=../../../class/ma/> 课程 </a></li></ul></div></nav><main class=md-main data-md-component=main><div class="md-main__inner md-grid"><div class="md-sidebar md-sidebar--primary"data-md-component=sidebar data-md-type=navigation><div class=md-sidebar__scrollwrap><div class=md-sidebar__inner><nav class="md-nav md-nav--primary md-nav--lifted"aria-label=Navigation data-md-level=0><label class=md-nav__title for=__drawer><a aria-label="Muke's Classical Department"class="md-nav__button md-logo"title="Muke's Classical Department"data-md-component=logo href=../../..> <svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M13 19v-3h8v3h-8m-4.5-6L2.47 7h4.24l4.96 4.95c.58.59.58 1.55 0 2.12L6.74 19H2.5l6-6Z"/></svg> </a> Muke's Classical Department</label><ul class=md-nav__list data-md-scrollfix><li class=md-nav__item><a class=md-nav__link href=../../..> <span class=md-ellipsis> Home </span> </a></li><li class="md-nav__item md-nav__item--section md-nav__item--nested"><input class="md-nav__toggle md-toggle"id=__nav_2 type=checkbox> <label class=md-nav__link for=__nav_2 id=__nav_2_label><span class=md-ellipsis> 课程 </span> <span class="md-nav__icon md-icon"></span></label> <nav aria-expanded=false aria-labelledby=__nav_2_label class=md-nav data-md-level=1><label class=md-nav__title for=__nav_2><span class="md-nav__icon md-icon"></span> 课程</label><ul class=md-nav__list data-md-scrollfix><li class=md-nav__item><a class=md-nav__link href=../../../class/ma/> <span class=md-ellipsis> 数学分析 </span> </a></li><li class=md-nav__item><a class=md-nav__link href=../../../class/la/> <span class=md-ellipsis> 高等代数与解析几何 </span> </a></li><li class=md-nav__item><a class=md-nav__link href=../../../class/geo/> <span class=md-ellipsis> 几何学 </span> </a></li></ul></nav></li></ul></nav></div></div></div><div class="md-sidebar md-sidebar--secondary"data-md-component=sidebar data-md-type=toc hidden><div class=md-sidebar__scrollwrap><div class=md-sidebar__inner><nav aria-label="Table of contents"class="md-nav md-nav--secondary"><label class=md-nav__title for=__toc><span class="md-nav__icon md-icon"></span> Table of contents</label><ul class=md-nav__list data-md-component=toc data-md-scrollfix><li class=md-nav__item><a class=md-nav__link href=#introduction> Introduction </a></li><li class=md-nav__item><a class=md-nav__link href=#ideas> Ideas </a> <nav aria-label=Ideas class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#softmax> Softmax </a> <nav aria-label=Softmax class=md-nav><ul class=md-nav__list><li class=md-nav__item><a class=md-nav__link href=#introduction_1> Introduction </a></li><li class=md-nav__item><a class=md-nav__link href=#likelihood-function> Likelihood function </a></li><li class=md-nav__item><a class=md-nav__link href=#derivative> Derivative </a></li></ul></nav></li><li class=md-nav__item><a class=md-nav__link href=#注意力汇聚> 注意力汇聚 </a></li><li class=md-nav__item><a class=md-nav__link href=#注意力评分函数> 注意力评分函数 </a></li><li class=md-nav__item><a class=md-nav__link href=#多头注意力> 多头注意力 </a></li><li class=md-nav__item><a class=md-nav__link href=#位置编码> 位置编码 </a></li><li class=md-nav__item><a class=md-nav__link href=#自注意力机制> 自注意力机制 </a></li></ul></nav></li><li class=md-nav__item><a class=md-nav__link href=#code> Code </a></li></ul></nav></div></div></div><div class=md-content data-md-component=content><article class="md-content__inner md-typeset"><h1>Transformer</h1><h2 id=introduction>Introduction<a title="Permanent link"class=headerlink href=#introduction></a></h2><p>相关链接：<a href=http://nlp.seas.harvard.edu/annotated-transformer/>http://nlp.seas.harvard.edu/annotated-transformer/</a></p><p>论文下载：Attention Is All You Need     <a id=pdffile>镜像</a>     <a href=https://arxiv.org/pdf/1706.03762.pdf>arXiv</a></p><details class=note><summary>Attention Is All You Need</summary> <p><embed id=pdfEmbed type=application/pdf> <script>var currentDomain = window.location.hostname;
    var pdfName = '1706.03762.pdf';
    document.getElementById('pdfEmbed').src = 'https://' + currentDomain + '/document/' + pdfName;
    document.getElementById('pdffile').href = 'https://' + currentDomain + '/document/' + pdfName;</script> <style>embed#pdfEmbed{height:1000px;width:-webkit-fill-available}</style></p></details><h2 id=ideas>Ideas<a title="Permanent link"class=headerlink href=#ideas></a></h2><h3 id=softmax>Softmax<a title="Permanent link"class=headerlink href=#softmax></a></h3><h4 id=introduction_1>Introduction<a title="Permanent link"class=headerlink href=#introduction_1></a></h4><p>考虑图像分类问题：现在有一系列 <span class=arithmatex>\(2\times 2\)</span> 的灰度图像，于是每个图像具有特征 <span class=arithmatex>\(x_1,x_2,x_3,x_4\)</span> 。我们希望对图像进行分类，分类到三个类别之一。通过 one-hot encoding, 我们希望通过 <span class=arithmatex>\(x_1,x_2,x_3,x_4\)</span> 来确定这个图像是 <span class=arithmatex>\((1,0,0)^{\text{T}}, (0,1,0)^{\text{T}},(0,0,1)^{\text{T}}\)</span> 中的哪一个。</p><p>经典的方法是，</p><div class=arithmatex>\[\begin{bmatrix} o_{1}\\ o_{2}\\ o_{3} \end{bmatrix} =\begin{bmatrix} w_{11} & \cdots & w_{14} \\ \vdots & \ddots & \vdots \\ w_{31} & \cdots & w_{34} \end{bmatrix} \begin{bmatrix} x_{1}\\ x_{2}\\ x_{3}\\ x_{4} \end{bmatrix}+\begin{bmatrix} b_{1}\\ b_{2}\\ b_{3} \end{bmatrix}\ ,\ \text{i.e.}\ \mathbf{o}=\mathbf{w}\mathbf{x}+\mathbf{b}\]</div><p>其中，系数矩阵可以通过学习得到，这样我们就获得了未规范化的预测。</p><p>但我们希望获得的实际上是这个图像（或者说这四个特征）在这三个 one-hot vector 上的概率分布。当然，我们的任务可能只需要指出最有可能的类别，但是我们仍然希望得到每个类别的概率。从而我们引入 softmax 运算。</p><div class=arithmatex>\[\hat{\mathbf{y}}=\mathrm{softmax}(\mathbf{o})\ , \text{where}\ \hat{y_j}=\dfrac{\exp(o_j)}{\sum\limits_{k=1}^q \exp(o_k)}\]</div><p>经过 softmax 运算，我们将未规范化的预测变换为一系列和为 <span class=arithmatex>\(1\)</span> 的非负数，从而得到合法的概率分布，并且保持可导性。同时不难发现，softmax 运算没有改变 <span class=arithmatex>\(o_j\)</span> 的相对顺序。</p><h4 id=likelihood-function>Likelihood function<a title="Permanent link"class=headerlink href=#likelihood-function></a></h4><p>先来回顾似然性的概念：似然性代表某个参数为特定值的可能性。举例来说，对于一枚不知道是否“公平”的硬币，我们试图知道它正面向上的概率 <span class=arithmatex>\(p\)</span> 是多少。于是我们通过试验，希望找到这个 <span class=arithmatex>\(p\)</span> 。</p><p>我们考虑事件 <span class=arithmatex>\(A\)</span> ：三次投掷两次为正面。在已经观察到事件 <span class=arithmatex>\(A\)</span> 的情况下，关于 <span class=arithmatex>\(A\)</span> 的似然估计是</p><div class=arithmatex>\[L(p\mid A)=P(A\mid p)=3p^2(1-p)\]</div><p>例如，<span class=arithmatex>\(p=0.5\)</span> 的似然性是 <span class=arithmatex>\(L(p=0.5\mid A)=P(A\mid p=0.5)=0.375\)</span>，而 <span class=arithmatex>\(p=0.6\)</span> 的似然性是 <span class=arithmatex>\(0.432\)</span> ，说明在试验得到 <span class=arithmatex>\(A\)</span> 的前提下，<span class=arithmatex>\(p=0.6\)</span> 的可能性比 <span class=arithmatex>\(p=0.5\)</span> 大。这提示我们，使得 <span class=arithmatex>\(L(p\mid A)\)</span> 最大的 <span class=arithmatex>\(p\)</span> 最有可能就是我们想要找的参数，即最大似然估计。</p><p>现在考虑 softmax 中的情况。</p><p>首先我们考虑，如何衡量我们预测的准确程度。例如，正确的 <span class=arithmatex>\(\textbf{y} = (0,1,0)^{\text{T}}\)</span> ，我们经过 softmax 运算之后的结果是 <span class=arithmatex>\(\hat{\textbf{y}}=(0.1,0.85,0.05)\)</span> , 一个思路是用</p><div class=arithmatex>\[y_1^0\times y_2^1\times y_3^0 = 0.1^0\times 0.85^1 \times 0.05^0=0.85\]</div><p>作为度量。通过这种幂相乘的形式，我们实际上就是在考虑 <span class=arithmatex>\(\hat{\textbf{y}}\)</span> 中与 one-hot vector 相对应的那一位。当然，为了运算方便，我们取其负对数作为损失函数，这样最小化损失函数就相当于是在最大化预测准确度。</p><p>从而我们可以定义：对于标签 <span class=arithmatex>\(\textbf{y}\)</span> 和预测 <span class=arithmatex>\(\hat{\textbf{y}}\)</span> （经过 softmax），我们的损失函数为</p><div class=arithmatex>\[l(\textbf{y}, \hat{\textbf{y}})=-\sum_{j=1}^q y_j \log \hat{y_j}\ge 0\]</div><p>这就是交叉熵损失, cross-entropy loss.</p><p>从而对于一组数据集 <span class=arithmatex>\(\{\textbf{X}, \textbf{Y}\}\)</span> 来说，我们就是要最小化负对数似然：</p><div class=arithmatex>\[-\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}) = \sum_{i=1}^n l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)})\]</div><h4 id=derivative>Derivative<a title="Permanent link"class=headerlink href=#derivative></a></h4><p>现在我们有了损失函数，我们关心它的导数。实际上，</p><div class=arithmatex>\[\begin{aligned} l(\mathbf{y}, \hat{\mathbf{y}}) &= - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum\limits_{k=1}^q \exp(o_k)} \\ &= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j\\ &= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j. \end{aligned}\]</div><p>于是</p><div class=arithmatex>\[\dfrac{\partial l(\mathbf{y}, \hat{\mathbf{y}})}{\partial o_j}=\dfrac{\exp(o_j)}{\sum\limits_{k=1}^q \exp(o_k)}-y_j=\hat{y_j}-y_j\]</div><p>即：通过 softmax 得到的概率与实际 one-hot vector 中的数值的差值。</p><h3 id=注意力汇聚>注意力汇聚<a title="Permanent link"class=headerlink href=#注意力汇聚></a></h3><p>考虑以下的回归问题：通过</p><div class=arithmatex>\[y_i=2\sin x_i + x_i^{0.8} + \epsilon\]</div><p>生成一个数据集，其中噪声 <span class=arithmatex>\(\epsilon\sim N(0, 0.5^2)\)</span> 。 我们现在提供一个点 <span class=arithmatex>\(x\)</span> ，希望预测对应的输出。朴素的想法是平均汇聚，即用</p><div class=arithmatex>\[f(x)=\dfrac{1}{n}\sum_{i=1}^n y_i\]</div><p>作为预测值。但这实际上忽略了 <span class=arithmatex>\(x_i\)</span> 的作用。于是我们引入 Nadaraya-Watson 核回归：</p><div class=arithmatex>\[f(x)=\sum_{i=1}^n \dfrac{K(x-x_i)}{\sum\limits_{j=1}^n K(x-x_j)}y_i=\sum_{i=1}^n \alpha(x, x_i)y_i\]</div><p>其中 <span class=arithmatex>\(x\)</span> 是查询，<span class=arithmatex>\(\alpha(x,x_i)\)</span> 可以理解为注意力权重。权重通过核 <span class=arithmatex>\(K\)</span> 来分配，实际上是有关相对位置的一个函数。例如，我们希望对于一次查询，注意力汇聚在该查询附近的位置上，并且离查询位置越远，注意力就越低。一个可能的核是高斯核，即</p><div class=arithmatex>\[K(u)=\dfrac{1}{\sqrt{2\pi}}\exp (-\dfrac{u^2}{2})\]</div><p>使用高斯核的估计器是</p><div class=arithmatex>\[f(x)=\sum_{i=1}^n \text{softmax}\Big(-\dfrac{1}{2}(x-x_i)^2\Big) y_i\]</div><p>当然，这时的 <span class=arithmatex>\(f(x)\)</span> 是固定的，没有学习的空间。我们引入可学习参数 <span class=arithmatex>\(w\)</span> ，即</p><div class=arithmatex>\[f(x)=\sum_{i=1}^n \text{softmax}\Big(-\dfrac{1}{2}(w(x-x_i))^2\Big) y_i\]</div><h3 id=注意力评分函数>注意力评分函数<a title="Permanent link"class=headerlink href=#注意力评分函数></a></h3><p>注意力汇聚的机制提示我们，我们可以学习注意力评分函数。通过 softmax 处理后，注意力评分函数转化为注意力权重，从而实现我们想要的汇聚效果。</p><p>具体来说，考虑一个查询 <span class=arithmatex>\(\textbf{q}\in \mathbb{R}^q\)</span> 和 <span class=arithmatex>\(m\)</span> 个键值对 <span class=arithmatex>\((\textbf k_1, \textbf{v}_1),\cdots,(\textbf k_m, \textbf{v}_m)\)</span> ，其中 <span class=arithmatex>\(\textbf{k}_i \in \mathbb{R}^k\ ,\ \textbf{v}_i \in \mathbb{R}^v\)</span> ，之前我们已经得到</p><div class=arithmatex>\[f(\mathbf{q}) = \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i \in \mathbb{R}^v\]</div><p>如果注意力评分函数为 <span class=arithmatex>\(a(\mathbf{q}, \mathbf{k})\)</span> ，则注意力权重为</p><div class=arithmatex>\[\alpha(\mathbf{q}, \mathbf{k}_i) = \mathrm{softmax}(a(\mathbf{q}, \mathbf{k}_i)) = \frac{\exp(a(\mathbf{q}, \mathbf{k}_i))}{\sum\limits_{j=1}^m \exp(a(\mathbf{q}, \mathbf{k}_j))} \in \mathbb{R}\]</div><p>现在要解决的问题就是：设计一个注意力评分函数 <span class=arithmatex>\(a(\mathbf{q}, \mathbf{k})\)</span> ，用来衡量 <span class=arithmatex>\(\mathbf{q}\in \mathbb{R}^q\)</span> 与 <span class=arithmatex>\(\mathbf{k}\in \mathbb{R}^k\)</span> 之间的“关联程度”，或者说希望分配的注意力的大小。特别地，对于词向量，我们通过填充没有意义的特殊词元，将 <span class=arithmatex>\(\mathbf{q}\)</span> 和 <span class=arithmatex>\(\mathbf{k}\)</span> 的长度进行了统一，假设为 <span class=arithmatex>\(d\)</span> 。则我们可以引入缩放点积注意力：假设查询和键的所有元素都是独立的随机变量，并且都满足零均值和单位方差，则两个向量的点积的均值为 <span class=arithmatex>\(0\)</span> ，方差为 <span class=arithmatex>\(d\)</span> 。于是</p><div class=arithmatex>\[a(\mathbf{q}, \mathbf{k})=\dfrac{\mathbf{q}\mathbf{k}^{\text{T}}}{\sqrt{d}}\]</div><p>对于查询 <span class=arithmatex>\(\textbf{Q}\in \mathbb{R}^{n\times d}\)</span> ，键 <span class=arithmatex>\(\textbf{K}\in \mathbb{R}^{m\times d}\)</span> ，值 <span class=arithmatex>\(\textbf{V}\in \mathbb{R}^{m\times v}\)</span> ，缩放点积注意力为</p><div class=arithmatex>\[\mathrm{softmax}\left(\frac{\mathbf Q \mathbf K^{\text{T}} }{\sqrt{d}}\right) \mathbf V \in \mathbb{R}^{n\times v}.\]</div><h3 id=多头注意力>多头注意力<a title="Permanent link"class=headerlink href=#多头注意力></a></h3><p>并行使用多组注意力汇聚，其中每个注意力汇聚具有一组可以学习的线性投影，再将 <span class=arithmatex>\(h\)</span> 个注意力汇聚的结果通过一个可以学习的线性投影进行变换得到最终输出。其中，每一个注意力汇聚都被称为一个头。具体来说，首先我们有注意力汇聚 <span class=arithmatex>\(f\)</span> ，例如对于缩放点积注意力，</p><div class=arithmatex>\[f=\mathrm{softmax}\left(\frac{\mathbf Q \mathbf K^{\text{T}} }{\sqrt{d}}\right) \mathbf V\]</div><p>然后我们有查询 <span class=arithmatex>\(\textbf{q}\in \mathbb{R}^{d_q}\)</span> ，键 <span class=arithmatex>\(\textbf{k}\in \mathbb{R}^{d_k}\)</span> ，值 <span class=arithmatex>\(\textbf{v}\in \mathbb{R}^{d_v}\)</span> 。与此同时，对于 <span class=arithmatex>\(i=1,2,\cdots,h\)</span> ，我们有三个可以学习的矩阵，分别是 <span class=arithmatex>\(\textbf{W}_i^{(q)}\in\mathbb{R}^{p_q\times d_q}\)</span> , <span class=arithmatex>\(\textbf{W}_i^{(k)}\in\mathbb{R}^{p_k\times d_k}\)</span> , <span class=arithmatex>\(\textbf{W}_i^{(v)}\in\mathbb{R}^{p_v\times d_v}\)</span> ，从而注意力头</p><div class=arithmatex>\[\mathbf{h}_i = f(\mathbf W_i^{(q)}\mathbf q, \mathbf W_i^{(k)}\mathbf k,\mathbf W_i^{(v)}\mathbf v) \in \mathbb R^{p_v}\]</div><p>然后考虑可学习的变换 <span class=arithmatex>\(\textbf{W}_o\in \mathbb{R}^{p_o \times hp_v}\)</span> 将多头注意力进行整合，即</p><div class=arithmatex>\[\mathbf W_o \begin{bmatrix}\mathbf h_1\\\vdots\\\mathbf h_h\end{bmatrix} \in \mathbb{R}^{p_o}.\]</div><h3 id=位置编码>位置编码<a title="Permanent link"class=headerlink href=#位置编码></a></h3><p>自注意力因为并行计算放弃了顺序操作。为了引入位置的影响，我们通过在输入中加入位置编码来注入位置信息。</p><p>假设输入为 <span class=arithmatex>\(\textbf{X}\in \mathbb{R}^{n\times d}\)</span> 表示 <span class=arithmatex>\(n\)</span> 个词的 <span class=arithmatex>\(d\)</span> 维嵌入表示，我们引入相同形状的矩阵 <span class=arithmatex>\(\textbf{P}\in \mathbb{R}^{n\times d}\)</span> 来进行位置编码，其中，</p><div class=arithmatex>\[p_{k,2j}=\sin\Big(\dfrac{k}{10000^{2j/d}}\Big)\ ,\ p_{k,2j+1}=\cos\Big(\dfrac{k}{10000^{2j/d}}\Big)\]</div><p>我们将 <span class=arithmatex>\(\textbf{X}+\textbf{P}\)</span> 作为新的输入，即完成位置编码。</p><p>令</p><div class=arithmatex>\[\varepsilon_j = \exp{\dfrac{1}{10000^{2j/d}}\mathrm{i}}\]</div><p>由 Euler 公式，</p><div class=arithmatex>\[\varepsilon_j^k=\mathrm{i}\sin\Big(\dfrac{k}{10000^{2j/d}}\Big)+\cos\Big(\dfrac{k}{10000^{2j/d}}\Big)\]</div><p>即生成第 <span class=arithmatex>\(k\)</span> 个词的 <span class=arithmatex>\(2j\ ,\ 2j+1\)</span> 处位置编码。再考虑相对位置 <span class=arithmatex>\(\delta\)</span> ，有</p><div class=arithmatex>\[\varepsilon_j^{k+\delta}=\varepsilon_j^k \times \varepsilon_j^\delta\]</div><p>于是我们有了一种不依赖 <span class=arithmatex>\(k\)</span> 的相对位置表示方式，可以理解为单位圆上的一种旋转。</p><p>从矩阵变换的角度，设 <span class=arithmatex>\(\omega_j = \dfrac{1}{10000^{2j/d}}\)</span> ，即</p><div class=arithmatex>\[ \begin{bmatrix} \cos(\delta \omega_j) & \sin(\delta \omega_j) \\ -\sin(\delta \omega_j) & \cos(\delta \omega_j) \\ \end{bmatrix} \begin{bmatrix} p_{k, 2j} \\ p_{k, 2j+1} \\ \end{bmatrix} = \begin{bmatrix} p_{k+\delta, 2j} \\ p_{k+\delta, 2j+1} \\ \end{bmatrix}, \]</div><h3 id=自注意力机制>自注意力机制<a title="Permanent link"class=headerlink href=#自注意力机制></a></h3><h2 id=code>Code<a title="Permanent link"class=headerlink href=#code></a></h2><p>下面的代码来自 <a href=https://github.com/huggingface/transformers/blob/cadf93a6fc547a4b809c138f0cd6ba5570fdf905/src/transformers/models/bart/modeling_bart.py>huggingface/transformers/bart</a></p><div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>   1</span>
<span class=normal>   2</span>
<span class=normal>   3</span>
<span class=normal>   4</span>
<span class=normal>   5</span>
<span class=normal>   6</span>
<span class=normal>   7</span>
<span class=normal>   8</span>
<span class=normal>   9</span>
<span class=normal>  10</span>
<span class=normal>  11</span>
<span class=normal>  12</span>
<span class=normal>  13</span>
<span class=normal>  14</span>
<span class=normal>  15</span>
<span class=normal>  16</span>
<span class=normal>  17</span>
<span class=normal>  18</span>
<span class=normal>  19</span>
<span class=normal>  20</span>
<span class=normal>  21</span>
<span class=normal>  22</span>
<span class=normal>  23</span>
<span class=normal>  24</span>
<span class=normal>  25</span>
<span class=normal>  26</span>
<span class=normal>  27</span>
<span class=normal>  28</span>
<span class=normal>  29</span>
<span class=normal>  30</span>
<span class=normal>  31</span>
<span class=normal>  32</span>
<span class=normal>  33</span>
<span class=normal>  34</span>
<span class=normal>  35</span>
<span class=normal>  36</span>
<span class=normal>  37</span>
<span class=normal>  38</span>
<span class=normal>  39</span>
<span class=normal>  40</span>
<span class=normal>  41</span>
<span class=normal>  42</span>
<span class=normal>  43</span>
<span class=normal>  44</span>
<span class=normal>  45</span>
<span class=normal>  46</span>
<span class=normal>  47</span>
<span class=normal>  48</span>
<span class=normal>  49</span>
<span class=normal>  50</span>
<span class=normal>  51</span>
<span class=normal>  52</span>
<span class=normal>  53</span>
<span class=normal>  54</span>
<span class=normal>  55</span>
<span class=normal>  56</span>
<span class=normal>  57</span>
<span class=normal>  58</span>
<span class=normal>  59</span>
<span class=normal>  60</span>
<span class=normal>  61</span>
<span class=normal>  62</span>
<span class=normal>  63</span>
<span class=normal>  64</span>
<span class=normal>  65</span>
<span class=normal>  66</span>
<span class=normal>  67</span>
<span class=normal>  68</span>
<span class=normal>  69</span>
<span class=normal>  70</span>
<span class=normal>  71</span>
<span class=normal>  72</span>
<span class=normal>  73</span>
<span class=normal>  74</span>
<span class=normal>  75</span>
<span class=normal>  76</span>
<span class=normal>  77</span>
<span class=normal>  78</span>
<span class=normal>  79</span>
<span class=normal>  80</span>
<span class=normal>  81</span>
<span class=normal>  82</span>
<span class=normal>  83</span>
<span class=normal>  84</span>
<span class=normal>  85</span>
<span class=normal>  86</span>
<span class=normal>  87</span>
<span class=normal>  88</span>
<span class=normal>  89</span>
<span class=normal>  90</span>
<span class=normal>  91</span>
<span class=normal>  92</span>
<span class=normal>  93</span>
<span class=normal>  94</span>
<span class=normal>  95</span>
<span class=normal>  96</span>
<span class=normal>  97</span>
<span class=normal>  98</span>
<span class=normal>  99</span>
<span class=normal> 100</span>
<span class=normal> 101</span>
<span class=normal> 102</span>
<span class=normal> 103</span>
<span class=normal> 104</span>
<span class=normal> 105</span>
<span class=normal> 106</span>
<span class=normal> 107</span>
<span class=normal> 108</span>
<span class=normal> 109</span>
<span class=normal> 110</span>
<span class=normal> 111</span>
<span class=normal> 112</span>
<span class=normal> 113</span>
<span class=normal> 114</span>
<span class=normal> 115</span>
<span class=normal> 116</span>
<span class=normal> 117</span>
<span class=normal> 118</span>
<span class=normal> 119</span>
<span class=normal> 120</span>
<span class=normal> 121</span>
<span class=normal> 122</span>
<span class=normal> 123</span>
<span class=normal> 124</span>
<span class=normal> 125</span>
<span class=normal> 126</span>
<span class=normal> 127</span>
<span class=normal> 128</span>
<span class=normal> 129</span>
<span class=normal> 130</span>
<span class=normal> 131</span>
<span class=normal> 132</span>
<span class=normal> 133</span>
<span class=normal> 134</span>
<span class=normal> 135</span>
<span class=normal> 136</span>
<span class=normal> 137</span>
<span class=normal> 138</span>
<span class=normal> 139</span>
<span class=normal> 140</span>
<span class=normal> 141</span>
<span class=normal> 142</span>
<span class=normal> 143</span>
<span class=normal> 144</span>
<span class=normal> 145</span>
<span class=normal> 146</span>
<span class=normal> 147</span>
<span class=normal> 148</span>
<span class=normal> 149</span>
<span class=normal> 150</span>
<span class=normal> 151</span>
<span class=normal> 152</span>
<span class=normal> 153</span>
<span class=normal> 154</span>
<span class=normal> 155</span>
<span class=normal> 156</span>
<span class=normal> 157</span>
<span class=normal> 158</span>
<span class=normal> 159</span>
<span class=normal> 160</span>
<span class=normal> 161</span>
<span class=normal> 162</span>
<span class=normal> 163</span>
<span class=normal> 164</span>
<span class=normal> 165</span>
<span class=normal> 166</span>
<span class=normal> 167</span>
<span class=normal> 168</span>
<span class=normal> 169</span>
<span class=normal> 170</span>
<span class=normal> 171</span>
<span class=normal> 172</span>
<span class=normal> 173</span>
<span class=normal> 174</span>
<span class=normal> 175</span>
<span class=normal> 176</span>
<span class=normal> 177</span>
<span class=normal> 178</span>
<span class=normal> 179</span>
<span class=normal> 180</span>
<span class=normal> 181</span>
<span class=normal> 182</span>
<span class=normal> 183</span>
<span class=normal> 184</span>
<span class=normal> 185</span>
<span class=normal> 186</span>
<span class=normal> 187</span>
<span class=normal> 188</span>
<span class=normal> 189</span>
<span class=normal> 190</span>
<span class=normal> 191</span>
<span class=normal> 192</span>
<span class=normal> 193</span>
<span class=normal> 194</span>
<span class=normal> 195</span>
<span class=normal> 196</span>
<span class=normal> 197</span>
<span class=normal> 198</span>
<span class=normal> 199</span>
<span class=normal> 200</span>
<span class=normal> 201</span>
<span class=normal> 202</span>
<span class=normal> 203</span>
<span class=normal> 204</span>
<span class=normal> 205</span>
<span class=normal> 206</span>
<span class=normal> 207</span>
<span class=normal> 208</span>
<span class=normal> 209</span>
<span class=normal> 210</span>
<span class=normal> 211</span>
<span class=normal> 212</span>
<span class=normal> 213</span>
<span class=normal> 214</span>
<span class=normal> 215</span>
<span class=normal> 216</span>
<span class=normal> 217</span>
<span class=normal> 218</span>
<span class=normal> 219</span>
<span class=normal> 220</span>
<span class=normal> 221</span>
<span class=normal> 222</span>
<span class=normal> 223</span>
<span class=normal> 224</span>
<span class=normal> 225</span>
<span class=normal> 226</span>
<span class=normal> 227</span>
<span class=normal> 228</span>
<span class=normal> 229</span>
<span class=normal> 230</span>
<span class=normal> 231</span>
<span class=normal> 232</span>
<span class=normal> 233</span>
<span class=normal> 234</span>
<span class=normal> 235</span>
<span class=normal> 236</span>
<span class=normal> 237</span>
<span class=normal> 238</span>
<span class=normal> 239</span>
<span class=normal> 240</span>
<span class=normal> 241</span>
<span class=normal> 242</span>
<span class=normal> 243</span>
<span class=normal> 244</span>
<span class=normal> 245</span>
<span class=normal> 246</span>
<span class=normal> 247</span>
<span class=normal> 248</span>
<span class=normal> 249</span>
<span class=normal> 250</span>
<span class=normal> 251</span>
<span class=normal> 252</span>
<span class=normal> 253</span>
<span class=normal> 254</span>
<span class=normal> 255</span>
<span class=normal> 256</span>
<span class=normal> 257</span>
<span class=normal> 258</span>
<span class=normal> 259</span>
<span class=normal> 260</span>
<span class=normal> 261</span>
<span class=normal> 262</span>
<span class=normal> 263</span>
<span class=normal> 264</span>
<span class=normal> 265</span>
<span class=normal> 266</span>
<span class=normal> 267</span>
<span class=normal> 268</span>
<span class=normal> 269</span>
<span class=normal> 270</span>
<span class=normal> 271</span>
<span class=normal> 272</span>
<span class=normal> 273</span>
<span class=normal> 274</span>
<span class=normal> 275</span>
<span class=normal> 276</span>
<span class=normal> 277</span>
<span class=normal> 278</span>
<span class=normal> 279</span>
<span class=normal> 280</span>
<span class=normal> 281</span>
<span class=normal> 282</span>
<span class=normal> 283</span>
<span class=normal> 284</span>
<span class=normal> 285</span>
<span class=normal> 286</span>
<span class=normal> 287</span>
<span class=normal> 288</span>
<span class=normal> 289</span>
<span class=normal> 290</span>
<span class=normal> 291</span>
<span class=normal> 292</span>
<span class=normal> 293</span>
<span class=normal> 294</span>
<span class=normal> 295</span>
<span class=normal> 296</span>
<span class=normal> 297</span>
<span class=normal> 298</span>
<span class=normal> 299</span>
<span class=normal> 300</span>
<span class=normal> 301</span>
<span class=normal> 302</span>
<span class=normal> 303</span>
<span class=normal> 304</span>
<span class=normal> 305</span>
<span class=normal> 306</span>
<span class=normal> 307</span>
<span class=normal> 308</span>
<span class=normal> 309</span>
<span class=normal> 310</span>
<span class=normal> 311</span>
<span class=normal> 312</span>
<span class=normal> 313</span>
<span class=normal> 314</span>
<span class=normal> 315</span>
<span class=normal> 316</span>
<span class=normal> 317</span>
<span class=normal> 318</span>
<span class=normal> 319</span>
<span class=normal> 320</span>
<span class=normal> 321</span>
<span class=normal> 322</span>
<span class=normal> 323</span>
<span class=normal> 324</span>
<span class=normal> 325</span>
<span class=normal> 326</span>
<span class=normal> 327</span>
<span class=normal> 328</span>
<span class=normal> 329</span>
<span class=normal> 330</span>
<span class=normal> 331</span>
<span class=normal> 332</span>
<span class=normal> 333</span>
<span class=normal> 334</span>
<span class=normal> 335</span>
<span class=normal> 336</span>
<span class=normal> 337</span>
<span class=normal> 338</span>
<span class=normal> 339</span>
<span class=normal> 340</span>
<span class=normal> 341</span>
<span class=normal> 342</span>
<span class=normal> 343</span>
<span class=normal> 344</span>
<span class=normal> 345</span>
<span class=normal> 346</span>
<span class=normal> 347</span>
<span class=normal> 348</span>
<span class=normal> 349</span>
<span class=normal> 350</span>
<span class=normal> 351</span>
<span class=normal> 352</span>
<span class=normal> 353</span>
<span class=normal> 354</span>
<span class=normal> 355</span>
<span class=normal> 356</span>
<span class=normal> 357</span>
<span class=normal> 358</span>
<span class=normal> 359</span>
<span class=normal> 360</span>
<span class=normal> 361</span>
<span class=normal> 362</span>
<span class=normal> 363</span>
<span class=normal> 364</span>
<span class=normal> 365</span>
<span class=normal> 366</span>
<span class=normal> 367</span>
<span class=normal> 368</span>
<span class=normal> 369</span>
<span class=normal> 370</span>
<span class=normal> 371</span>
<span class=normal> 372</span>
<span class=normal> 373</span>
<span class=normal> 374</span>
<span class=normal> 375</span>
<span class=normal> 376</span>
<span class=normal> 377</span>
<span class=normal> 378</span>
<span class=normal> 379</span>
<span class=normal> 380</span>
<span class=normal> 381</span>
<span class=normal> 382</span>
<span class=normal> 383</span>
<span class=normal> 384</span>
<span class=normal> 385</span>
<span class=normal> 386</span>
<span class=normal> 387</span>
<span class=normal> 388</span>
<span class=normal> 389</span>
<span class=normal> 390</span>
<span class=normal> 391</span>
<span class=normal> 392</span>
<span class=normal> 393</span>
<span class=normal> 394</span>
<span class=normal> 395</span>
<span class=normal> 396</span>
<span class=normal> 397</span>
<span class=normal> 398</span>
<span class=normal> 399</span>
<span class=normal> 400</span>
<span class=normal> 401</span>
<span class=normal> 402</span>
<span class=normal> 403</span>
<span class=normal> 404</span>
<span class=normal> 405</span>
<span class=normal> 406</span>
<span class=normal> 407</span>
<span class=normal> 408</span>
<span class=normal> 409</span>
<span class=normal> 410</span>
<span class=normal> 411</span>
<span class=normal> 412</span>
<span class=normal> 413</span>
<span class=normal> 414</span>
<span class=normal> 415</span>
<span class=normal> 416</span>
<span class=normal> 417</span>
<span class=normal> 418</span>
<span class=normal> 419</span>
<span class=normal> 420</span>
<span class=normal> 421</span>
<span class=normal> 422</span>
<span class=normal> 423</span>
<span class=normal> 424</span>
<span class=normal> 425</span>
<span class=normal> 426</span>
<span class=normal> 427</span>
<span class=normal> 428</span>
<span class=normal> 429</span>
<span class=normal> 430</span>
<span class=normal> 431</span>
<span class=normal> 432</span>
<span class=normal> 433</span>
<span class=normal> 434</span>
<span class=normal> 435</span>
<span class=normal> 436</span>
<span class=normal> 437</span>
<span class=normal> 438</span>
<span class=normal> 439</span>
<span class=normal> 440</span>
<span class=normal> 441</span>
<span class=normal> 442</span>
<span class=normal> 443</span>
<span class=normal> 444</span>
<span class=normal> 445</span>
<span class=normal> 446</span>
<span class=normal> 447</span>
<span class=normal> 448</span>
<span class=normal> 449</span>
<span class=normal> 450</span>
<span class=normal> 451</span>
<span class=normal> 452</span>
<span class=normal> 453</span>
<span class=normal> 454</span>
<span class=normal> 455</span>
<span class=normal> 456</span>
<span class=normal> 457</span>
<span class=normal> 458</span>
<span class=normal> 459</span>
<span class=normal> 460</span>
<span class=normal> 461</span>
<span class=normal> 462</span>
<span class=normal> 463</span>
<span class=normal> 464</span>
<span class=normal> 465</span>
<span class=normal> 466</span>
<span class=normal> 467</span>
<span class=normal> 468</span>
<span class=normal> 469</span>
<span class=normal> 470</span>
<span class=normal> 471</span>
<span class=normal> 472</span>
<span class=normal> 473</span>
<span class=normal> 474</span>
<span class=normal> 475</span>
<span class=normal> 476</span>
<span class=normal> 477</span>
<span class=normal> 478</span>
<span class=normal> 479</span>
<span class=normal> 480</span>
<span class=normal> 481</span>
<span class=normal> 482</span>
<span class=normal> 483</span>
<span class=normal> 484</span>
<span class=normal> 485</span>
<span class=normal> 486</span>
<span class=normal> 487</span>
<span class=normal> 488</span>
<span class=normal> 489</span>
<span class=normal> 490</span>
<span class=normal> 491</span>
<span class=normal> 492</span>
<span class=normal> 493</span>
<span class=normal> 494</span>
<span class=normal> 495</span>
<span class=normal> 496</span>
<span class=normal> 497</span>
<span class=normal> 498</span>
<span class=normal> 499</span>
<span class=normal> 500</span>
<span class=normal> 501</span>
<span class=normal> 502</span>
<span class=normal> 503</span>
<span class=normal> 504</span>
<span class=normal> 505</span>
<span class=normal> 506</span>
<span class=normal> 507</span>
<span class=normal> 508</span>
<span class=normal> 509</span>
<span class=normal> 510</span>
<span class=normal> 511</span>
<span class=normal> 512</span>
<span class=normal> 513</span>
<span class=normal> 514</span>
<span class=normal> 515</span>
<span class=normal> 516</span>
<span class=normal> 517</span>
<span class=normal> 518</span>
<span class=normal> 519</span>
<span class=normal> 520</span>
<span class=normal> 521</span>
<span class=normal> 522</span>
<span class=normal> 523</span>
<span class=normal> 524</span>
<span class=normal> 525</span>
<span class=normal> 526</span>
<span class=normal> 527</span>
<span class=normal> 528</span>
<span class=normal> 529</span>
<span class=normal> 530</span>
<span class=normal> 531</span>
<span class=normal> 532</span>
<span class=normal> 533</span>
<span class=normal> 534</span>
<span class=normal> 535</span>
<span class=normal> 536</span>
<span class=normal> 537</span>
<span class=normal> 538</span>
<span class=normal> 539</span>
<span class=normal> 540</span>
<span class=normal> 541</span>
<span class=normal> 542</span>
<span class=normal> 543</span>
<span class=normal> 544</span>
<span class=normal> 545</span>
<span class=normal> 546</span>
<span class=normal> 547</span>
<span class=normal> 548</span>
<span class=normal> 549</span>
<span class=normal> 550</span>
<span class=normal> 551</span>
<span class=normal> 552</span>
<span class=normal> 553</span>
<span class=normal> 554</span>
<span class=normal> 555</span>
<span class=normal> 556</span>
<span class=normal> 557</span>
<span class=normal> 558</span>
<span class=normal> 559</span>
<span class=normal> 560</span>
<span class=normal> 561</span>
<span class=normal> 562</span>
<span class=normal> 563</span>
<span class=normal> 564</span>
<span class=normal> 565</span>
<span class=normal> 566</span>
<span class=normal> 567</span>
<span class=normal> 568</span>
<span class=normal> 569</span>
<span class=normal> 570</span>
<span class=normal> 571</span>
<span class=normal> 572</span>
<span class=normal> 573</span>
<span class=normal> 574</span>
<span class=normal> 575</span>
<span class=normal> 576</span>
<span class=normal> 577</span>
<span class=normal> 578</span>
<span class=normal> 579</span>
<span class=normal> 580</span>
<span class=normal> 581</span>
<span class=normal> 582</span>
<span class=normal> 583</span>
<span class=normal> 584</span>
<span class=normal> 585</span>
<span class=normal> 586</span>
<span class=normal> 587</span>
<span class=normal> 588</span>
<span class=normal> 589</span>
<span class=normal> 590</span>
<span class=normal> 591</span>
<span class=normal> 592</span>
<span class=normal> 593</span>
<span class=normal> 594</span>
<span class=normal> 595</span>
<span class=normal> 596</span>
<span class=normal> 597</span>
<span class=normal> 598</span>
<span class=normal> 599</span>
<span class=normal> 600</span>
<span class=normal> 601</span>
<span class=normal> 602</span>
<span class=normal> 603</span>
<span class=normal> 604</span>
<span class=normal> 605</span>
<span class=normal> 606</span>
<span class=normal> 607</span>
<span class=normal> 608</span>
<span class=normal> 609</span>
<span class=normal> 610</span>
<span class=normal> 611</span>
<span class=normal> 612</span>
<span class=normal> 613</span>
<span class=normal> 614</span>
<span class=normal> 615</span>
<span class=normal> 616</span>
<span class=normal> 617</span>
<span class=normal> 618</span>
<span class=normal> 619</span>
<span class=normal> 620</span>
<span class=normal> 621</span>
<span class=normal> 622</span>
<span class=normal> 623</span>
<span class=normal> 624</span>
<span class=normal> 625</span>
<span class=normal> 626</span>
<span class=normal> 627</span>
<span class=normal> 628</span>
<span class=normal> 629</span>
<span class=normal> 630</span>
<span class=normal> 631</span>
<span class=normal> 632</span>
<span class=normal> 633</span>
<span class=normal> 634</span>
<span class=normal> 635</span>
<span class=normal> 636</span>
<span class=normal> 637</span>
<span class=normal> 638</span>
<span class=normal> 639</span>
<span class=normal> 640</span>
<span class=normal> 641</span>
<span class=normal> 642</span>
<span class=normal> 643</span>
<span class=normal> 644</span>
<span class=normal> 645</span>
<span class=normal> 646</span>
<span class=normal> 647</span>
<span class=normal> 648</span>
<span class=normal> 649</span>
<span class=normal> 650</span>
<span class=normal> 651</span>
<span class=normal> 652</span>
<span class=normal> 653</span>
<span class=normal> 654</span>
<span class=normal> 655</span>
<span class=normal> 656</span>
<span class=normal> 657</span>
<span class=normal> 658</span>
<span class=normal> 659</span>
<span class=normal> 660</span>
<span class=normal> 661</span>
<span class=normal> 662</span>
<span class=normal> 663</span>
<span class=normal> 664</span>
<span class=normal> 665</span>
<span class=normal> 666</span>
<span class=normal> 667</span>
<span class=normal> 668</span>
<span class=normal> 669</span>
<span class=normal> 670</span>
<span class=normal> 671</span>
<span class=normal> 672</span>
<span class=normal> 673</span>
<span class=normal> 674</span>
<span class=normal> 675</span>
<span class=normal> 676</span>
<span class=normal> 677</span>
<span class=normal> 678</span>
<span class=normal> 679</span>
<span class=normal> 680</span>
<span class=normal> 681</span>
<span class=normal> 682</span>
<span class=normal> 683</span>
<span class=normal> 684</span>
<span class=normal> 685</span>
<span class=normal> 686</span>
<span class=normal> 687</span>
<span class=normal> 688</span>
<span class=normal> 689</span>
<span class=normal> 690</span>
<span class=normal> 691</span>
<span class=normal> 692</span>
<span class=normal> 693</span>
<span class=normal> 694</span>
<span class=normal> 695</span>
<span class=normal> 696</span>
<span class=normal> 697</span>
<span class=normal> 698</span>
<span class=normal> 699</span>
<span class=normal> 700</span>
<span class=normal> 701</span>
<span class=normal> 702</span>
<span class=normal> 703</span>
<span class=normal> 704</span>
<span class=normal> 705</span>
<span class=normal> 706</span>
<span class=normal> 707</span>
<span class=normal> 708</span>
<span class=normal> 709</span>
<span class=normal> 710</span>
<span class=normal> 711</span>
<span class=normal> 712</span>
<span class=normal> 713</span>
<span class=normal> 714</span>
<span class=normal> 715</span>
<span class=normal> 716</span>
<span class=normal> 717</span>
<span class=normal> 718</span>
<span class=normal> 719</span>
<span class=normal> 720</span>
<span class=normal> 721</span>
<span class=normal> 722</span>
<span class=normal> 723</span>
<span class=normal> 724</span>
<span class=normal> 725</span>
<span class=normal> 726</span>
<span class=normal> 727</span>
<span class=normal> 728</span>
<span class=normal> 729</span>
<span class=normal> 730</span>
<span class=normal> 731</span>
<span class=normal> 732</span>
<span class=normal> 733</span>
<span class=normal> 734</span>
<span class=normal> 735</span>
<span class=normal> 736</span>
<span class=normal> 737</span>
<span class=normal> 738</span>
<span class=normal> 739</span>
<span class=normal> 740</span>
<span class=normal> 741</span>
<span class=normal> 742</span>
<span class=normal> 743</span>
<span class=normal> 744</span>
<span class=normal> 745</span>
<span class=normal> 746</span>
<span class=normal> 747</span>
<span class=normal> 748</span>
<span class=normal> 749</span>
<span class=normal> 750</span>
<span class=normal> 751</span>
<span class=normal> 752</span>
<span class=normal> 753</span>
<span class=normal> 754</span>
<span class=normal> 755</span>
<span class=normal> 756</span>
<span class=normal> 757</span>
<span class=normal> 758</span>
<span class=normal> 759</span>
<span class=normal> 760</span>
<span class=normal> 761</span>
<span class=normal> 762</span>
<span class=normal> 763</span>
<span class=normal> 764</span>
<span class=normal> 765</span>
<span class=normal> 766</span>
<span class=normal> 767</span>
<span class=normal> 768</span>
<span class=normal> 769</span>
<span class=normal> 770</span>
<span class=normal> 771</span>
<span class=normal> 772</span>
<span class=normal> 773</span>
<span class=normal> 774</span>
<span class=normal> 775</span>
<span class=normal> 776</span>
<span class=normal> 777</span>
<span class=normal> 778</span>
<span class=normal> 779</span>
<span class=normal> 780</span>
<span class=normal> 781</span>
<span class=normal> 782</span>
<span class=normal> 783</span>
<span class=normal> 784</span>
<span class=normal> 785</span>
<span class=normal> 786</span>
<span class=normal> 787</span>
<span class=normal> 788</span>
<span class=normal> 789</span>
<span class=normal> 790</span>
<span class=normal> 791</span>
<span class=normal> 792</span>
<span class=normal> 793</span>
<span class=normal> 794</span>
<span class=normal> 795</span>
<span class=normal> 796</span>
<span class=normal> 797</span>
<span class=normal> 798</span>
<span class=normal> 799</span>
<span class=normal> 800</span>
<span class=normal> 801</span>
<span class=normal> 802</span>
<span class=normal> 803</span>
<span class=normal> 804</span>
<span class=normal> 805</span>
<span class=normal> 806</span>
<span class=normal> 807</span>
<span class=normal> 808</span>
<span class=normal> 809</span>
<span class=normal> 810</span>
<span class=normal> 811</span>
<span class=normal> 812</span>
<span class=normal> 813</span>
<span class=normal> 814</span>
<span class=normal> 815</span>
<span class=normal> 816</span>
<span class=normal> 817</span>
<span class=normal> 818</span>
<span class=normal> 819</span>
<span class=normal> 820</span>
<span class=normal> 821</span>
<span class=normal> 822</span>
<span class=normal> 823</span>
<span class=normal> 824</span>
<span class=normal> 825</span>
<span class=normal> 826</span>
<span class=normal> 827</span>
<span class=normal> 828</span>
<span class=normal> 829</span>
<span class=normal> 830</span>
<span class=normal> 831</span>
<span class=normal> 832</span>
<span class=normal> 833</span>
<span class=normal> 834</span>
<span class=normal> 835</span>
<span class=normal> 836</span>
<span class=normal> 837</span>
<span class=normal> 838</span>
<span class=normal> 839</span>
<span class=normal> 840</span>
<span class=normal> 841</span>
<span class=normal> 842</span>
<span class=normal> 843</span>
<span class=normal> 844</span>
<span class=normal> 845</span>
<span class=normal> 846</span>
<span class=normal> 847</span>
<span class=normal> 848</span>
<span class=normal> 849</span>
<span class=normal> 850</span>
<span class=normal> 851</span>
<span class=normal> 852</span>
<span class=normal> 853</span>
<span class=normal> 854</span>
<span class=normal> 855</span>
<span class=normal> 856</span>
<span class=normal> 857</span>
<span class=normal> 858</span>
<span class=normal> 859</span>
<span class=normal> 860</span>
<span class=normal> 861</span>
<span class=normal> 862</span>
<span class=normal> 863</span>
<span class=normal> 864</span>
<span class=normal> 865</span>
<span class=normal> 866</span>
<span class=normal> 867</span>
<span class=normal> 868</span>
<span class=normal> 869</span>
<span class=normal> 870</span>
<span class=normal> 871</span>
<span class=normal> 872</span>
<span class=normal> 873</span>
<span class=normal> 874</span>
<span class=normal> 875</span>
<span class=normal> 876</span>
<span class=normal> 877</span>
<span class=normal> 878</span>
<span class=normal> 879</span>
<span class=normal> 880</span>
<span class=normal> 881</span>
<span class=normal> 882</span>
<span class=normal> 883</span>
<span class=normal> 884</span>
<span class=normal> 885</span>
<span class=normal> 886</span>
<span class=normal> 887</span>
<span class=normal> 888</span>
<span class=normal> 889</span>
<span class=normal> 890</span>
<span class=normal> 891</span>
<span class=normal> 892</span>
<span class=normal> 893</span>
<span class=normal> 894</span>
<span class=normal> 895</span>
<span class=normal> 896</span>
<span class=normal> 897</span>
<span class=normal> 898</span>
<span class=normal> 899</span>
<span class=normal> 900</span>
<span class=normal> 901</span>
<span class=normal> 902</span>
<span class=normal> 903</span>
<span class=normal> 904</span>
<span class=normal> 905</span>
<span class=normal> 906</span>
<span class=normal> 907</span>
<span class=normal> 908</span>
<span class=normal> 909</span>
<span class=normal> 910</span>
<span class=normal> 911</span>
<span class=normal> 912</span>
<span class=normal> 913</span>
<span class=normal> 914</span>
<span class=normal> 915</span>
<span class=normal> 916</span>
<span class=normal> 917</span>
<span class=normal> 918</span>
<span class=normal> 919</span>
<span class=normal> 920</span>
<span class=normal> 921</span>
<span class=normal> 922</span>
<span class=normal> 923</span>
<span class=normal> 924</span>
<span class=normal> 925</span>
<span class=normal> 926</span>
<span class=normal> 927</span>
<span class=normal> 928</span>
<span class=normal> 929</span>
<span class=normal> 930</span>
<span class=normal> 931</span>
<span class=normal> 932</span>
<span class=normal> 933</span>
<span class=normal> 934</span>
<span class=normal> 935</span>
<span class=normal> 936</span>
<span class=normal> 937</span>
<span class=normal> 938</span>
<span class=normal> 939</span>
<span class=normal> 940</span>
<span class=normal> 941</span>
<span class=normal> 942</span>
<span class=normal> 943</span>
<span class=normal> 944</span>
<span class=normal> 945</span>
<span class=normal> 946</span>
<span class=normal> 947</span>
<span class=normal> 948</span>
<span class=normal> 949</span>
<span class=normal> 950</span>
<span class=normal> 951</span>
<span class=normal> 952</span>
<span class=normal> 953</span>
<span class=normal> 954</span>
<span class=normal> 955</span>
<span class=normal> 956</span>
<span class=normal> 957</span>
<span class=normal> 958</span>
<span class=normal> 959</span>
<span class=normal> 960</span>
<span class=normal> 961</span>
<span class=normal> 962</span>
<span class=normal> 963</span>
<span class=normal> 964</span>
<span class=normal> 965</span>
<span class=normal> 966</span>
<span class=normal> 967</span>
<span class=normal> 968</span>
<span class=normal> 969</span>
<span class=normal> 970</span>
<span class=normal> 971</span>
<span class=normal> 972</span>
<span class=normal> 973</span>
<span class=normal> 974</span>
<span class=normal> 975</span>
<span class=normal> 976</span>
<span class=normal> 977</span>
<span class=normal> 978</span>
<span class=normal> 979</span>
<span class=normal> 980</span>
<span class=normal> 981</span>
<span class=normal> 982</span>
<span class=normal> 983</span>
<span class=normal> 984</span>
<span class=normal> 985</span>
<span class=normal> 986</span>
<span class=normal> 987</span>
<span class=normal> 988</span>
<span class=normal> 989</span>
<span class=normal> 990</span>
<span class=normal> 991</span>
<span class=normal> 992</span>
<span class=normal> 993</span>
<span class=normal> 994</span>
<span class=normal> 995</span>
<span class=normal> 996</span>
<span class=normal> 997</span>
<span class=normal> 998</span>
<span class=normal> 999</span>
<span class=normal>1000</span>
<span class=normal>1001</span>
<span class=normal>1002</span>
<span class=normal>1003</span>
<span class=normal>1004</span>
<span class=normal>1005</span>
<span class=normal>1006</span>
<span class=normal>1007</span>
<span class=normal>1008</span>
<span class=normal>1009</span>
<span class=normal>1010</span>
<span class=normal>1011</span>
<span class=normal>1012</span>
<span class=normal>1013</span>
<span class=normal>1014</span>
<span class=normal>1015</span>
<span class=normal>1016</span>
<span class=normal>1017</span>
<span class=normal>1018</span>
<span class=normal>1019</span>
<span class=normal>1020</span>
<span class=normal>1021</span>
<span class=normal>1022</span>
<span class=normal>1023</span>
<span class=normal>1024</span>
<span class=normal>1025</span>
<span class=normal>1026</span>
<span class=normal>1027</span>
<span class=normal>1028</span>
<span class=normal>1029</span>
<span class=normal>1030</span>
<span class=normal>1031</span>
<span class=normal>1032</span>
<span class=normal>1033</span>
<span class=normal>1034</span>
<span class=normal>1035</span>
<span class=normal>1036</span>
<span class=normal>1037</span>
<span class=normal>1038</span>
<span class=normal>1039</span>
<span class=normal>1040</span>
<span class=normal>1041</span>
<span class=normal>1042</span>
<span class=normal>1043</span>
<span class=normal>1044</span>
<span class=normal>1045</span>
<span class=normal>1046</span>
<span class=normal>1047</span>
<span class=normal>1048</span>
<span class=normal>1049</span>
<span class=normal>1050</span>
<span class=normal>1051</span>
<span class=normal>1052</span>
<span class=normal>1053</span>
<span class=normal>1054</span>
<span class=normal>1055</span>
<span class=normal>1056</span>
<span class=normal>1057</span>
<span class=normal>1058</span>
<span class=normal>1059</span>
<span class=normal>1060</span>
<span class=normal>1061</span>
<span class=normal>1062</span>
<span class=normal>1063</span>
<span class=normal>1064</span>
<span class=normal>1065</span>
<span class=normal>1066</span>
<span class=normal>1067</span>
<span class=normal>1068</span>
<span class=normal>1069</span>
<span class=normal>1070</span>
<span class=normal>1071</span>
<span class=normal>1072</span>
<span class=normal>1073</span>
<span class=normal>1074</span>
<span class=normal>1075</span>
<span class=normal>1076</span>
<span class=normal>1077</span>
<span class=normal>1078</span>
<span class=normal>1079</span>
<span class=normal>1080</span>
<span class=normal>1081</span>
<span class=normal>1082</span>
<span class=normal>1083</span>
<span class=normal>1084</span>
<span class=normal>1085</span>
<span class=normal>1086</span>
<span class=normal>1087</span>
<span class=normal>1088</span>
<span class=normal>1089</span>
<span class=normal>1090</span>
<span class=normal>1091</span>
<span class=normal>1092</span>
<span class=normal>1093</span>
<span class=normal>1094</span>
<span class=normal>1095</span>
<span class=normal>1096</span>
<span class=normal>1097</span>
<span class=normal>1098</span>
<span class=normal>1099</span>
<span class=normal>1100</span>
<span class=normal>1101</span>
<span class=normal>1102</span>
<span class=normal>1103</span>
<span class=normal>1104</span>
<span class=normal>1105</span>
<span class=normal>1106</span>
<span class=normal>1107</span>
<span class=normal>1108</span>
<span class=normal>1109</span>
<span class=normal>1110</span>
<span class=normal>1111</span>
<span class=normal>1112</span>
<span class=normal>1113</span>
<span class=normal>1114</span>
<span class=normal>1115</span>
<span class=normal>1116</span>
<span class=normal>1117</span>
<span class=normal>1118</span>
<span class=normal>1119</span>
<span class=normal>1120</span>
<span class=normal>1121</span>
<span class=normal>1122</span>
<span class=normal>1123</span>
<span class=normal>1124</span>
<span class=normal>1125</span>
<span class=normal>1126</span>
<span class=normal>1127</span>
<span class=normal>1128</span>
<span class=normal>1129</span>
<span class=normal>1130</span>
<span class=normal>1131</span>
<span class=normal>1132</span>
<span class=normal>1133</span>
<span class=normal>1134</span>
<span class=normal>1135</span>
<span class=normal>1136</span>
<span class=normal>1137</span>
<span class=normal>1138</span>
<span class=normal>1139</span>
<span class=normal>1140</span>
<span class=normal>1141</span>
<span class=normal>1142</span>
<span class=normal>1143</span>
<span class=normal>1144</span>
<span class=normal>1145</span>
<span class=normal>1146</span>
<span class=normal>1147</span>
<span class=normal>1148</span>
<span class=normal>1149</span>
<span class=normal>1150</span>
<span class=normal>1151</span>
<span class=normal>1152</span>
<span class=normal>1153</span>
<span class=normal>1154</span>
<span class=normal>1155</span>
<span class=normal>1156</span>
<span class=normal>1157</span>
<span class=normal>1158</span>
<span class=normal>1159</span>
<span class=normal>1160</span>
<span class=normal>1161</span>
<span class=normal>1162</span>
<span class=normal>1163</span>
<span class=normal>1164</span>
<span class=normal>1165</span>
<span class=normal>1166</span>
<span class=normal>1167</span>
<span class=normal>1168</span>
<span class=normal>1169</span>
<span class=normal>1170</span>
<span class=normal>1171</span>
<span class=normal>1172</span>
<span class=normal>1173</span>
<span class=normal>1174</span>
<span class=normal>1175</span>
<span class=normal>1176</span>
<span class=normal>1177</span>
<span class=normal>1178</span>
<span class=normal>1179</span>
<span class=normal>1180</span>
<span class=normal>1181</span>
<span class=normal>1182</span>
<span class=normal>1183</span>
<span class=normal>1184</span>
<span class=normal>1185</span>
<span class=normal>1186</span>
<span class=normal>1187</span>
<span class=normal>1188</span>
<span class=normal>1189</span>
<span class=normal>1190</span>
<span class=normal>1191</span>
<span class=normal>1192</span>
<span class=normal>1193</span>
<span class=normal>1194</span>
<span class=normal>1195</span>
<span class=normal>1196</span>
<span class=normal>1197</span>
<span class=normal>1198</span>
<span class=normal>1199</span>
<span class=normal>1200</span>
<span class=normal>1201</span>
<span class=normal>1202</span>
<span class=normal>1203</span>
<span class=normal>1204</span>
<span class=normal>1205</span>
<span class=normal>1206</span>
<span class=normal>1207</span>
<span class=normal>1208</span>
<span class=normal>1209</span>
<span class=normal>1210</span>
<span class=normal>1211</span>
<span class=normal>1212</span>
<span class=normal>1213</span>
<span class=normal>1214</span>
<span class=normal>1215</span>
<span class=normal>1216</span>
<span class=normal>1217</span>
<span class=normal>1218</span>
<span class=normal>1219</span>
<span class=normal>1220</span>
<span class=normal>1221</span>
<span class=normal>1222</span>
<span class=normal>1223</span>
<span class=normal>1224</span>
<span class=normal>1225</span>
<span class=normal>1226</span>
<span class=normal>1227</span>
<span class=normal>1228</span>
<span class=normal>1229</span>
<span class=normal>1230</span>
<span class=normal>1231</span>
<span class=normal>1232</span>
<span class=normal>1233</span>
<span class=normal>1234</span>
<span class=normal>1235</span>
<span class=normal>1236</span>
<span class=normal>1237</span>
<span class=normal>1238</span>
<span class=normal>1239</span>
<span class=normal>1240</span>
<span class=normal>1241</span>
<span class=normal>1242</span>
<span class=normal>1243</span>
<span class=normal>1244</span>
<span class=normal>1245</span>
<span class=normal>1246</span>
<span class=normal>1247</span>
<span class=normal>1248</span>
<span class=normal>1249</span>
<span class=normal>1250</span>
<span class=normal>1251</span>
<span class=normal>1252</span>
<span class=normal>1253</span>
<span class=normal>1254</span>
<span class=normal>1255</span>
<span class=normal>1256</span>
<span class=normal>1257</span>
<span class=normal>1258</span>
<span class=normal>1259</span>
<span class=normal>1260</span>
<span class=normal>1261</span>
<span class=normal>1262</span>
<span class=normal>1263</span>
<span class=normal>1264</span>
<span class=normal>1265</span>
<span class=normal>1266</span>
<span class=normal>1267</span>
<span class=normal>1268</span>
<span class=normal>1269</span>
<span class=normal>1270</span>
<span class=normal>1271</span>
<span class=normal>1272</span>
<span class=normal>1273</span>
<span class=normal>1274</span>
<span class=normal>1275</span>
<span class=normal>1276</span>
<span class=normal>1277</span>
<span class=normal>1278</span>
<span class=normal>1279</span>
<span class=normal>1280</span>
<span class=normal>1281</span>
<span class=normal>1282</span>
<span class=normal>1283</span>
<span class=normal>1284</span>
<span class=normal>1285</span>
<span class=normal>1286</span>
<span class=normal>1287</span>
<span class=normal>1288</span>
<span class=normal>1289</span>
<span class=normal>1290</span>
<span class=normal>1291</span>
<span class=normal>1292</span>
<span class=normal>1293</span>
<span class=normal>1294</span>
<span class=normal>1295</span>
<span class=normal>1296</span>
<span class=normal>1297</span>
<span class=normal>1298</span>
<span class=normal>1299</span>
<span class=normal>1300</span>
<span class=normal>1301</span>
<span class=normal>1302</span>
<span class=normal>1303</span>
<span class=normal>1304</span>
<span class=normal>1305</span>
<span class=normal>1306</span>
<span class=normal>1307</span>
<span class=normal>1308</span>
<span class=normal>1309</span>
<span class=normal>1310</span>
<span class=normal>1311</span>
<span class=normal>1312</span>
<span class=normal>1313</span>
<span class=normal>1314</span>
<span class=normal>1315</span>
<span class=normal>1316</span>
<span class=normal>1317</span>
<span class=normal>1318</span>
<span class=normal>1319</span>
<span class=normal>1320</span>
<span class=normal>1321</span>
<span class=normal>1322</span>
<span class=normal>1323</span>
<span class=normal>1324</span>
<span class=normal>1325</span>
<span class=normal>1326</span>
<span class=normal>1327</span>
<span class=normal>1328</span>
<span class=normal>1329</span>
<span class=normal>1330</span>
<span class=normal>1331</span>
<span class=normal>1332</span>
<span class=normal>1333</span>
<span class=normal>1334</span>
<span class=normal>1335</span>
<span class=normal>1336</span>
<span class=normal>1337</span>
<span class=normal>1338</span>
<span class=normal>1339</span>
<span class=normal>1340</span>
<span class=normal>1341</span>
<span class=normal>1342</span>
<span class=normal>1343</span>
<span class=normal>1344</span>
<span class=normal>1345</span>
<span class=normal>1346</span>
<span class=normal>1347</span>
<span class=normal>1348</span>
<span class=normal>1349</span>
<span class=normal>1350</span>
<span class=normal>1351</span>
<span class=normal>1352</span>
<span class=normal>1353</span>
<span class=normal>1354</span>
<span class=normal>1355</span>
<span class=normal>1356</span>
<span class=normal>1357</span>
<span class=normal>1358</span>
<span class=normal>1359</span>
<span class=normal>1360</span>
<span class=normal>1361</span>
<span class=normal>1362</span>
<span class=normal>1363</span>
<span class=normal>1364</span>
<span class=normal>1365</span>
<span class=normal>1366</span>
<span class=normal>1367</span>
<span class=normal>1368</span>
<span class=normal>1369</span>
<span class=normal>1370</span>
<span class=normal>1371</span>
<span class=normal>1372</span>
<span class=normal>1373</span>
<span class=normal>1374</span>
<span class=normal>1375</span>
<span class=normal>1376</span>
<span class=normal>1377</span>
<span class=normal>1378</span>
<span class=normal>1379</span>
<span class=normal>1380</span>
<span class=normal>1381</span>
<span class=normal>1382</span>
<span class=normal>1383</span>
<span class=normal>1384</span>
<span class=normal>1385</span>
<span class=normal>1386</span>
<span class=normal>1387</span>
<span class=normal>1388</span>
<span class=normal>1389</span>
<span class=normal>1390</span>
<span class=normal>1391</span>
<span class=normal>1392</span>
<span class=normal>1393</span>
<span class=normal>1394</span>
<span class=normal>1395</span>
<span class=normal>1396</span>
<span class=normal>1397</span>
<span class=normal>1398</span>
<span class=normal>1399</span>
<span class=normal>1400</span>
<span class=normal>1401</span>
<span class=normal>1402</span>
<span class=normal>1403</span>
<span class=normal>1404</span>
<span class=normal>1405</span>
<span class=normal>1406</span>
<span class=normal>1407</span>
<span class=normal>1408</span>
<span class=normal>1409</span>
<span class=normal>1410</span>
<span class=normal>1411</span>
<span class=normal>1412</span>
<span class=normal>1413</span>
<span class=normal>1414</span>
<span class=normal>1415</span>
<span class=normal>1416</span>
<span class=normal>1417</span>
<span class=normal>1418</span>
<span class=normal>1419</span>
<span class=normal>1420</span>
<span class=normal>1421</span>
<span class=normal>1422</span>
<span class=normal>1423</span>
<span class=normal>1424</span>
<span class=normal>1425</span>
<span class=normal>1426</span>
<span class=normal>1427</span>
<span class=normal>1428</span>
<span class=normal>1429</span>
<span class=normal>1430</span>
<span class=normal>1431</span>
<span class=normal>1432</span>
<span class=normal>1433</span>
<span class=normal>1434</span>
<span class=normal>1435</span>
<span class=normal>1436</span>
<span class=normal>1437</span>
<span class=normal>1438</span>
<span class=normal>1439</span>
<span class=normal>1440</span>
<span class=normal>1441</span>
<span class=normal>1442</span>
<span class=normal>1443</span>
<span class=normal>1444</span>
<span class=normal>1445</span>
<span class=normal>1446</span>
<span class=normal>1447</span>
<span class=normal>1448</span>
<span class=normal>1449</span>
<span class=normal>1450</span>
<span class=normal>1451</span>
<span class=normal>1452</span>
<span class=normal>1453</span>
<span class=normal>1454</span>
<span class=normal>1455</span>
<span class=normal>1456</span>
<span class=normal>1457</span>
<span class=normal>1458</span>
<span class=normal>1459</span>
<span class=normal>1460</span>
<span class=normal>1461</span>
<span class=normal>1462</span>
<span class=normal>1463</span>
<span class=normal>1464</span>
<span class=normal>1465</span>
<span class=normal>1466</span>
<span class=normal>1467</span>
<span class=normal>1468</span>
<span class=normal>1469</span>
<span class=normal>1470</span>
<span class=normal>1471</span>
<span class=normal>1472</span>
<span class=normal>1473</span>
<span class=normal>1474</span>
<span class=normal>1475</span>
<span class=normal>1476</span>
<span class=normal>1477</span>
<span class=normal>1478</span>
<span class=normal>1479</span>
<span class=normal>1480</span>
<span class=normal>1481</span>
<span class=normal>1482</span>
<span class=normal>1483</span>
<span class=normal>1484</span>
<span class=normal>1485</span>
<span class=normal>1486</span>
<span class=normal>1487</span>
<span class=normal>1488</span>
<span class=normal>1489</span>
<span class=normal>1490</span>
<span class=normal>1491</span>
<span class=normal>1492</span>
<span class=normal>1493</span>
<span class=normal>1494</span>
<span class=normal>1495</span>
<span class=normal>1496</span>
<span class=normal>1497</span>
<span class=normal>1498</span>
<span class=normal>1499</span>
<span class=normal>1500</span>
<span class=normal>1501</span>
<span class=normal>1502</span>
<span class=normal>1503</span>
<span class=normal>1504</span>
<span class=normal>1505</span>
<span class=normal>1506</span>
<span class=normal>1507</span>
<span class=normal>1508</span>
<span class=normal>1509</span>
<span class=normal>1510</span>
<span class=normal>1511</span>
<span class=normal>1512</span>
<span class=normal>1513</span>
<span class=normal>1514</span>
<span class=normal>1515</span>
<span class=normal>1516</span>
<span class=normal>1517</span>
<span class=normal>1518</span>
<span class=normal>1519</span>
<span class=normal>1520</span>
<span class=normal>1521</span>
<span class=normal>1522</span>
<span class=normal>1523</span>
<span class=normal>1524</span>
<span class=normal>1525</span>
<span class=normal>1526</span>
<span class=normal>1527</span>
<span class=normal>1528</span>
<span class=normal>1529</span>
<span class=normal>1530</span>
<span class=normal>1531</span>
<span class=normal>1532</span>
<span class=normal>1533</span>
<span class=normal>1534</span>
<span class=normal>1535</span>
<span class=normal>1536</span>
<span class=normal>1537</span>
<span class=normal>1538</span>
<span class=normal>1539</span>
<span class=normal>1540</span>
<span class=normal>1541</span>
<span class=normal>1542</span>
<span class=normal>1543</span>
<span class=normal>1544</span>
<span class=normal>1545</span>
<span class=normal>1546</span>
<span class=normal>1547</span>
<span class=normal>1548</span>
<span class=normal>1549</span>
<span class=normal>1550</span>
<span class=normal>1551</span>
<span class=normal>1552</span>
<span class=normal>1553</span>
<span class=normal>1554</span>
<span class=normal>1555</span>
<span class=normal>1556</span>
<span class=normal>1557</span>
<span class=normal>1558</span>
<span class=normal>1559</span>
<span class=normal>1560</span>
<span class=normal>1561</span>
<span class=normal>1562</span>
<span class=normal>1563</span>
<span class=normal>1564</span>
<span class=normal>1565</span>
<span class=normal>1566</span>
<span class=normal>1567</span>
<span class=normal>1568</span>
<span class=normal>1569</span>
<span class=normal>1570</span>
<span class=normal>1571</span>
<span class=normal>1572</span>
<span class=normal>1573</span>
<span class=normal>1574</span>
<span class=normal>1575</span>
<span class=normal>1576</span>
<span class=normal>1577</span>
<span class=normal>1578</span>
<span class=normal>1579</span>
<span class=normal>1580</span>
<span class=normal>1581</span>
<span class=normal>1582</span>
<span class=normal>1583</span>
<span class=normal>1584</span>
<span class=normal>1585</span>
<span class=normal>1586</span>
<span class=normal>1587</span>
<span class=normal>1588</span>
<span class=normal>1589</span>
<span class=normal>1590</span>
<span class=normal>1591</span>
<span class=normal>1592</span>
<span class=normal>1593</span>
<span class=normal>1594</span>
<span class=normal>1595</span>
<span class=normal>1596</span>
<span class=normal>1597</span>
<span class=normal>1598</span>
<span class=normal>1599</span>
<span class=normal>1600</span>
<span class=normal>1601</span>
<span class=normal>1602</span>
<span class=normal>1603</span>
<span class=normal>1604</span>
<span class=normal>1605</span>
<span class=normal>1606</span>
<span class=normal>1607</span>
<span class=normal>1608</span>
<span class=normal>1609</span>
<span class=normal>1610</span>
<span class=normal>1611</span>
<span class=normal>1612</span>
<span class=normal>1613</span>
<span class=normal>1614</span>
<span class=normal>1615</span>
<span class=normal>1616</span>
<span class=normal>1617</span>
<span class=normal>1618</span>
<span class=normal>1619</span>
<span class=normal>1620</span>
<span class=normal>1621</span>
<span class=normal>1622</span>
<span class=normal>1623</span>
<span class=normal>1624</span>
<span class=normal>1625</span>
<span class=normal>1626</span>
<span class=normal>1627</span>
<span class=normal>1628</span>
<span class=normal>1629</span>
<span class=normal>1630</span>
<span class=normal>1631</span>
<span class=normal>1632</span>
<span class=normal>1633</span>
<span class=normal>1634</span>
<span class=normal>1635</span>
<span class=normal>1636</span>
<span class=normal>1637</span>
<span class=normal>1638</span>
<span class=normal>1639</span>
<span class=normal>1640</span>
<span class=normal>1641</span>
<span class=normal>1642</span>
<span class=normal>1643</span>
<span class=normal>1644</span>
<span class=normal>1645</span>
<span class=normal>1646</span>
<span class=normal>1647</span>
<span class=normal>1648</span>
<span class=normal>1649</span>
<span class=normal>1650</span>
<span class=normal>1651</span>
<span class=normal>1652</span>
<span class=normal>1653</span>
<span class=normal>1654</span>
<span class=normal>1655</span>
<span class=normal>1656</span>
<span class=normal>1657</span>
<span class=normal>1658</span>
<span class=normal>1659</span>
<span class=normal>1660</span>
<span class=normal>1661</span>
<span class=normal>1662</span>
<span class=normal>1663</span>
<span class=normal>1664</span>
<span class=normal>1665</span>
<span class=normal>1666</span>
<span class=normal>1667</span>
<span class=normal>1668</span>
<span class=normal>1669</span>
<span class=normal>1670</span>
<span class=normal>1671</span>
<span class=normal>1672</span>
<span class=normal>1673</span>
<span class=normal>1674</span>
<span class=normal>1675</span>
<span class=normal>1676</span>
<span class=normal>1677</span>
<span class=normal>1678</span>
<span class=normal>1679</span>
<span class=normal>1680</span>
<span class=normal>1681</span>
<span class=normal>1682</span>
<span class=normal>1683</span>
<span class=normal>1684</span>
<span class=normal>1685</span>
<span class=normal>1686</span>
<span class=normal>1687</span>
<span class=normal>1688</span>
<span class=normal>1689</span>
<span class=normal>1690</span>
<span class=normal>1691</span>
<span class=normal>1692</span>
<span class=normal>1693</span>
<span class=normal>1694</span>
<span class=normal>1695</span>
<span class=normal>1696</span>
<span class=normal>1697</span>
<span class=normal>1698</span>
<span class=normal>1699</span>
<span class=normal>1700</span>
<span class=normal>1701</span>
<span class=normal>1702</span>
<span class=normal>1703</span>
<span class=normal>1704</span>
<span class=normal>1705</span>
<span class=normal>1706</span>
<span class=normal>1707</span>
<span class=normal>1708</span>
<span class=normal>1709</span>
<span class=normal>1710</span>
<span class=normal>1711</span>
<span class=normal>1712</span>
<span class=normal>1713</span>
<span class=normal>1714</span>
<span class=normal>1715</span>
<span class=normal>1716</span>
<span class=normal>1717</span>
<span class=normal>1718</span>
<span class=normal>1719</span>
<span class=normal>1720</span>
<span class=normal>1721</span>
<span class=normal>1722</span>
<span class=normal>1723</span>
<span class=normal>1724</span>
<span class=normal>1725</span>
<span class=normal>1726</span>
<span class=normal>1727</span>
<span class=normal>1728</span>
<span class=normal>1729</span>
<span class=normal>1730</span>
<span class=normal>1731</span>
<span class=normal>1732</span>
<span class=normal>1733</span>
<span class=normal>1734</span>
<span class=normal>1735</span>
<span class=normal>1736</span>
<span class=normal>1737</span>
<span class=normal>1738</span>
<span class=normal>1739</span>
<span class=normal>1740</span>
<span class=normal>1741</span>
<span class=normal>1742</span>
<span class=normal>1743</span>
<span class=normal>1744</span>
<span class=normal>1745</span>
<span class=normal>1746</span>
<span class=normal>1747</span>
<span class=normal>1748</span>
<span class=normal>1749</span>
<span class=normal>1750</span>
<span class=normal>1751</span>
<span class=normal>1752</span>
<span class=normal>1753</span>
<span class=normal>1754</span>
<span class=normal>1755</span>
<span class=normal>1756</span>
<span class=normal>1757</span>
<span class=normal>1758</span>
<span class=normal>1759</span>
<span class=normal>1760</span>
<span class=normal>1761</span>
<span class=normal>1762</span>
<span class=normal>1763</span>
<span class=normal>1764</span>
<span class=normal>1765</span>
<span class=normal>1766</span>
<span class=normal>1767</span>
<span class=normal>1768</span>
<span class=normal>1769</span>
<span class=normal>1770</span>
<span class=normal>1771</span>
<span class=normal>1772</span>
<span class=normal>1773</span>
<span class=normal>1774</span>
<span class=normal>1775</span>
<span class=normal>1776</span>
<span class=normal>1777</span>
<span class=normal>1778</span>
<span class=normal>1779</span>
<span class=normal>1780</span>
<span class=normal>1781</span>
<span class=normal>1782</span>
<span class=normal>1783</span>
<span class=normal>1784</span>
<span class=normal>1785</span>
<span class=normal>1786</span>
<span class=normal>1787</span>
<span class=normal>1788</span>
<span class=normal>1789</span>
<span class=normal>1790</span>
<span class=normal>1791</span>
<span class=normal>1792</span>
<span class=normal>1793</span>
<span class=normal>1794</span>
<span class=normal>1795</span>
<span class=normal>1796</span>
<span class=normal>1797</span>
<span class=normal>1798</span>
<span class=normal>1799</span>
<span class=normal>1800</span>
<span class=normal>1801</span>
<span class=normal>1802</span>
<span class=normal>1803</span>
<span class=normal>1804</span>
<span class=normal>1805</span>
<span class=normal>1806</span>
<span class=normal>1807</span>
<span class=normal>1808</span>
<span class=normal>1809</span>
<span class=normal>1810</span>
<span class=normal>1811</span>
<span class=normal>1812</span>
<span class=normal>1813</span>
<span class=normal>1814</span>
<span class=normal>1815</span>
<span class=normal>1816</span>
<span class=normal>1817</span>
<span class=normal>1818</span>
<span class=normal>1819</span>
<span class=normal>1820</span>
<span class=normal>1821</span>
<span class=normal>1822</span>
<span class=normal>1823</span>
<span class=normal>1824</span>
<span class=normal>1825</span>
<span class=normal>1826</span>
<span class=normal>1827</span>
<span class=normal>1828</span>
<span class=normal>1829</span>
<span class=normal>1830</span>
<span class=normal>1831</span>
<span class=normal>1832</span>
<span class=normal>1833</span>
<span class=normal>1834</span>
<span class=normal>1835</span>
<span class=normal>1836</span>
<span class=normal>1837</span>
<span class=normal>1838</span>
<span class=normal>1839</span>
<span class=normal>1840</span>
<span class=normal>1841</span>
<span class=normal>1842</span>
<span class=normal>1843</span>
<span class=normal>1844</span>
<span class=normal>1845</span>
<span class=normal>1846</span>
<span class=normal>1847</span>
<span class=normal>1848</span>
<span class=normal>1849</span>
<span class=normal>1850</span>
<span class=normal>1851</span>
<span class=normal>1852</span>
<span class=normal>1853</span>
<span class=normal>1854</span>
<span class=normal>1855</span>
<span class=normal>1856</span>
<span class=normal>1857</span>
<span class=normal>1858</span>
<span class=normal>1859</span>
<span class=normal>1860</span>
<span class=normal>1861</span>
<span class=normal>1862</span>
<span class=normal>1863</span>
<span class=normal>1864</span>
<span class=normal>1865</span>
<span class=normal>1866</span>
<span class=normal>1867</span>
<span class=normal>1868</span>
<span class=normal>1869</span>
<span class=normal>1870</span>
<span class=normal>1871</span>
<span class=normal>1872</span>
<span class=normal>1873</span>
<span class=normal>1874</span>
<span class=normal>1875</span>
<span class=normal>1876</span>
<span class=normal>1877</span>
<span class=normal>1878</span>
<span class=normal>1879</span>
<span class=normal>1880</span>
<span class=normal>1881</span>
<span class=normal>1882</span>
<span class=normal>1883</span>
<span class=normal>1884</span>
<span class=normal>1885</span>
<span class=normal>1886</span>
<span class=normal>1887</span>
<span class=normal>1888</span>
<span class=normal>1889</span>
<span class=normal>1890</span>
<span class=normal>1891</span>
<span class=normal>1892</span>
<span class=normal>1893</span>
<span class=normal>1894</span>
<span class=normal>1895</span>
<span class=normal>1896</span>
<span class=normal>1897</span>
<span class=normal>1898</span>
<span class=normal>1899</span>
<span class=normal>1900</span>
<span class=normal>1901</span>
<span class=normal>1902</span>
<span class=normal>1903</span>
<span class=normal>1904</span>
<span class=normal>1905</span>
<span class=normal>1906</span>
<span class=normal>1907</span>
<span class=normal>1908</span>
<span class=normal>1909</span>
<span class=normal>1910</span>
<span class=normal>1911</span>
<span class=normal>1912</span>
<span class=normal>1913</span>
<span class=normal>1914</span>
<span class=normal>1915</span>
<span class=normal>1916</span>
<span class=normal>1917</span>
<span class=normal>1918</span>
<span class=normal>1919</span>
<span class=normal>1920</span>
<span class=normal>1921</span>
<span class=normal>1922</span>
<span class=normal>1923</span>
<span class=normal>1924</span>
<span class=normal>1925</span>
<span class=normal>1926</span>
<span class=normal>1927</span>
<span class=normal>1928</span>
<span class=normal>1929</span>
<span class=normal>1930</span>
<span class=normal>1931</span>
<span class=normal>1932</span>
<span class=normal>1933</span>
<span class=normal>1934</span>
<span class=normal>1935</span>
<span class=normal>1936</span>
<span class=normal>1937</span>
<span class=normal>1938</span>
<span class=normal>1939</span>
<span class=normal>1940</span>
<span class=normal>1941</span>
<span class=normal>1942</span>
<span class=normal>1943</span>
<span class=normal>1944</span>
<span class=normal>1945</span>
<span class=normal>1946</span>
<span class=normal>1947</span>
<span class=normal>1948</span>
<span class=normal>1949</span>
<span class=normal>1950</span>
<span class=normal>1951</span>
<span class=normal>1952</span>
<span class=normal>1953</span>
<span class=normal>1954</span>
<span class=normal>1955</span>
<span class=normal>1956</span>
<span class=normal>1957</span>
<span class=normal>1958</span>
<span class=normal>1959</span>
<span class=normal>1960</span>
<span class=normal>1961</span>
<span class=normal>1962</span>
<span class=normal>1963</span>
<span class=normal>1964</span>
<span class=normal>1965</span>
<span class=normal>1966</span>
<span class=normal>1967</span>
<span class=normal>1968</span>
<span class=normal>1969</span>
<span class=normal>1970</span>
<span class=normal>1971</span>
<span class=normal>1972</span>
<span class=normal>1973</span>
<span class=normal>1974</span>
<span class=normal>1975</span>
<span class=normal>1976</span>
<span class=normal>1977</span>
<span class=normal>1978</span>
<span class=normal>1979</span>
<span class=normal>1980</span>
<span class=normal>1981</span>
<span class=normal>1982</span>
<span class=normal>1983</span>
<span class=normal>1984</span>
<span class=normal>1985</span>
<span class=normal>1986</span>
<span class=normal>1987</span>
<span class=normal>1988</span>
<span class=normal>1989</span>
<span class=normal>1990</span>
<span class=normal>1991</span>
<span class=normal>1992</span>
<span class=normal>1993</span>
<span class=normal>1994</span>
<span class=normal>1995</span>
<span class=normal>1996</span>
<span class=normal>1997</span>
<span class=normal>1998</span>
<span class=normal>1999</span>
<span class=normal>2000</span>
<span class=normal>2001</span>
<span class=normal>2002</span>
<span class=normal>2003</span>
<span class=normal>2004</span>
<span class=normal>2005</span>
<span class=normal>2006</span>
<span class=normal>2007</span>
<span class=normal>2008</span>
<span class=normal>2009</span>
<span class=normal>2010</span>
<span class=normal>2011</span>
<span class=normal>2012</span>
<span class=normal>2013</span>
<span class=normal>2014</span>
<span class=normal>2015</span>
<span class=normal>2016</span>
<span class=normal>2017</span>
<span class=normal>2018</span>
<span class=normal>2019</span>
<span class=normal>2020</span>
<span class=normal>2021</span>
<span class=normal>2022</span>
<span class=normal>2023</span>
<span class=normal>2024</span>
<span class=normal>2025</span>
<span class=normal>2026</span>
<span class=normal>2027</span>
<span class=normal>2028</span>
<span class=normal>2029</span>
<span class=normal>2030</span>
<span class=normal>2031</span>
<span class=normal>2032</span>
<span class=normal>2033</span>
<span class=normal>2034</span>
<span class=normal>2035</span>
<span class=normal>2036</span>
<span class=normal>2037</span>
<span class=normal>2038</span>
<span class=normal>2039</span>
<span class=normal>2040</span>
<span class=normal>2041</span>
<span class=normal>2042</span>
<span class=normal>2043</span>
<span class=normal>2044</span>
<span class=normal>2045</span>
<span class=normal>2046</span>
<span class=normal>2047</span>
<span class=normal>2048</span>
<span class=normal>2049</span>
<span class=normal>2050</span>
<span class=normal>2051</span>
<span class=normal>2052</span>
<span class=normal>2053</span>
<span class=normal>2054</span>
<span class=normal>2055</span>
<span class=normal>2056</span>
<span class=normal>2057</span>
<span class=normal>2058</span>
<span class=normal>2059</span>
<span class=normal>2060</span>
<span class=normal>2061</span>
<span class=normal>2062</span>
<span class=normal>2063</span>
<span class=normal>2064</span>
<span class=normal>2065</span>
<span class=normal>2066</span>
<span class=normal>2067</span>
<span class=normal>2068</span>
<span class=normal>2069</span>
<span class=normal>2070</span>
<span class=normal>2071</span>
<span class=normal>2072</span>
<span class=normal>2073</span>
<span class=normal>2074</span>
<span class=normal>2075</span>
<span class=normal>2076</span>
<span class=normal>2077</span>
<span class=normal>2078</span>
<span class=normal>2079</span>
<span class=normal>2080</span>
<span class=normal>2081</span>
<span class=normal>2082</span>
<span class=normal>2083</span>
<span class=normal>2084</span>
<span class=normal>2085</span>
<span class=normal>2086</span>
<span class=normal>2087</span>
<span class=normal>2088</span>
<span class=normal>2089</span>
<span class=normal>2090</span>
<span class=normal>2091</span>
<span class=normal>2092</span>
<span class=normal>2093</span>
<span class=normal>2094</span>
<span class=normal>2095</span>
<span class=normal>2096</span>
<span class=normal>2097</span>
<span class=normal>2098</span>
<span class=normal>2099</span>
<span class=normal>2100</span>
<span class=normal>2101</span>
<span class=normal>2102</span>
<span class=normal>2103</span>
<span class=normal>2104</span>
<span class=normal>2105</span>
<span class=normal>2106</span>
<span class=normal>2107</span>
<span class=normal>2108</span>
<span class=normal>2109</span>
<span class=normal>2110</span>
<span class=normal>2111</span>
<span class=normal>2112</span>
<span class=normal>2113</span>
<span class=normal>2114</span>
<span class=normal>2115</span>
<span class=normal>2116</span>
<span class=normal>2117</span>
<span class=normal>2118</span>
<span class=normal>2119</span>
<span class=normal>2120</span>
<span class=normal>2121</span>
<span class=normal>2122</span>
<span class=normal>2123</span>
<span class=normal>2124</span>
<span class=normal>2125</span>
<span class=normal>2126</span>
<span class=normal>2127</span>
<span class=normal>2128</span>
<span class=normal>2129</span>
<span class=normal>2130</span>
<span class=normal>2131</span>
<span class=normal>2132</span>
<span class=normal>2133</span>
<span class=normal>2134</span>
<span class=normal>2135</span>
<span class=normal>2136</span>
<span class=normal>2137</span>
<span class=normal>2138</span>
<span class=normal>2139</span>
<span class=normal>2140</span>
<span class=normal>2141</span>
<span class=normal>2142</span>
<span class=normal>2143</span>
<span class=normal>2144</span>
<span class=normal>2145</span>
<span class=normal>2146</span>
<span class=normal>2147</span>
<span class=normal>2148</span>
<span class=normal>2149</span>
<span class=normal>2150</span>
<span class=normal>2151</span>
<span class=normal>2152</span>
<span class=normal>2153</span>
<span class=normal>2154</span>
<span class=normal>2155</span>
<span class=normal>2156</span>
<span class=normal>2157</span>
<span class=normal>2158</span>
<span class=normal>2159</span>
<span class=normal>2160</span>
<span class=normal>2161</span>
<span class=normal>2162</span>
<span class=normal>2163</span>
<span class=normal>2164</span>
<span class=normal>2165</span>
<span class=normal>2166</span>
<span class=normal>2167</span>
<span class=normal>2168</span>
<span class=normal>2169</span>
<span class=normal>2170</span>
<span class=normal>2171</span>
<span class=normal>2172</span>
<span class=normal>2173</span>
<span class=normal>2174</span>
<span class=normal>2175</span>
<span class=normal>2176</span>
<span class=normal>2177</span>
<span class=normal>2178</span>
<span class=normal>2179</span>
<span class=normal>2180</span>
<span class=normal>2181</span>
<span class=normal>2182</span>
<span class=normal>2183</span>
<span class=normal>2184</span>
<span class=normal>2185</span>
<span class=normal>2186</span>
<span class=normal>2187</span>
<span class=normal>2188</span>
<span class=normal>2189</span>
<span class=normal>2190</span>
<span class=normal>2191</span>
<span class=normal>2192</span>
<span class=normal>2193</span>
<span class=normal>2194</span>
<span class=normal>2195</span>
<span class=normal>2196</span>
<span class=normal>2197</span>
<span class=normal>2198</span>
<span class=normal>2199</span>
<span class=normal>2200</span>
<span class=normal>2201</span>
<span class=normal>2202</span>
<span class=normal>2203</span>
<span class=normal>2204</span>
<span class=normal>2205</span>
<span class=normal>2206</span>
<span class=normal>2207</span>
<span class=normal>2208</span>
<span class=normal>2209</span>
<span class=normal>2210</span>
<span class=normal>2211</span>
<span class=normal>2212</span>
<span class=normal>2213</span>
<span class=normal>2214</span>
<span class=normal>2215</span>
<span class=normal>2216</span>
<span class=normal>2217</span>
<span class=normal>2218</span>
<span class=normal>2219</span>
<span class=normal>2220</span>
<span class=normal>2221</span>
<span class=normal>2222</span>
<span class=normal>2223</span>
<span class=normal>2224</span>
<span class=normal>2225</span>
<span class=normal>2226</span>
<span class=normal>2227</span>
<span class=normal>2228</span>
<span class=normal>2229</span>
<span class=normal>2230</span>
<span class=normal>2231</span>
<span class=normal>2232</span>
<span class=normal>2233</span>
<span class=normal>2234</span>
<span class=normal>2235</span>
<span class=normal>2236</span>
<span class=normal>2237</span>
<span class=normal>2238</span>
<span class=normal>2239</span>
<span class=normal>2240</span>
<span class=normal>2241</span>
<span class=normal>2242</span>
<span class=normal>2243</span>
<span class=normal>2244</span>
<span class=normal>2245</span>
<span class=normal>2246</span>
<span class=normal>2247</span>
<span class=normal>2248</span>
<span class=normal>2249</span>
<span class=normal>2250</span>
<span class=normal>2251</span>
<span class=normal>2252</span>
<span class=normal>2253</span>
<span class=normal>2254</span>
<span class=normal>2255</span>
<span class=normal>2256</span>
<span class=normal>2257</span>
<span class=normal>2258</span>
<span class=normal>2259</span>
<span class=normal>2260</span>
<span class=normal>2261</span>
<span class=normal>2262</span>
<span class=normal>2263</span>
<span class=normal>2264</span>
<span class=normal>2265</span>
<span class=normal>2266</span>
<span class=normal>2267</span>
<span class=normal>2268</span>
<span class=normal>2269</span>
<span class=normal>2270</span>
<span class=normal>2271</span>
<span class=normal>2272</span>
<span class=normal>2273</span>
<span class=normal>2274</span>
<span class=normal>2275</span>
<span class=normal>2276</span>
<span class=normal>2277</span>
<span class=normal>2278</span>
<span class=normal>2279</span>
<span class=normal>2280</span>
<span class=normal>2281</span>
<span class=normal>2282</span>
<span class=normal>2283</span>
<span class=normal>2284</span>
<span class=normal>2285</span>
<span class=normal>2286</span>
<span class=normal>2287</span>
<span class=normal>2288</span>
<span class=normal>2289</span>
<span class=normal>2290</span>
<span class=normal>2291</span>
<span class=normal>2292</span>
<span class=normal>2293</span>
<span class=normal>2294</span>
<span class=normal>2295</span>
<span class=normal>2296</span>
<span class=normal>2297</span>
<span class=normal>2298</span>
<span class=normal>2299</span>
<span class=normal>2300</span>
<span class=normal>2301</span>
<span class=normal>2302</span>
<span class=normal>2303</span>
<span class=normal>2304</span>
<span class=normal>2305</span>
<span class=normal>2306</span>
<span class=normal>2307</span>
<span class=normal>2308</span>
<span class=normal>2309</span>
<span class=normal>2310</span>
<span class=normal>2311</span>
<span class=normal>2312</span>
<span class=normal>2313</span>
<span class=normal>2314</span></pre></div></td><td class=code><div><pre><span></span><code><span class=c1># coding=utf-8</span>
<span class=c1># Copyright 2021 The Fairseq Authors and The HuggingFace Inc. team. All rights reserved.</span>
<span class=c1>#</span>
<span class=c1># Licensed under the Apache License, Version 2.0 (the "License");</span>
<span class=c1># you may not use this file except in compliance with the License.</span>
<span class=c1># You may obtain a copy of the License at</span>
<span class=c1>#</span>
<span class=c1>#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class=c1>#</span>
<span class=c1># Unless required by applicable law or agreed to in writing, software</span>
<span class=c1># distributed under the License is distributed on an "AS IS" BASIS,</span>
<span class=c1># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class=c1># See the License for the specific language governing permissions and</span>
<span class=c1># limitations under the License.</span>
<span class=sd>""" PyTorch BART model."""</span>
<span class=kn>import</span> <span class=nn>copy</span>
<span class=kn>import</span> <span class=nn>math</span>
<span class=kn>import</span> <span class=nn>warnings</span>
<span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>List</span><span class=p>,</span> <span class=n>Optional</span><span class=p>,</span> <span class=n>Tuple</span><span class=p>,</span> <span class=n>Union</span>

<span class=kn>import</span> <span class=nn>torch</span>
<span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
<span class=kn>import</span> <span class=nn>torch.utils.checkpoint</span>
<span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span>
<span class=kn>from</span> <span class=nn>torch.nn</span> <span class=kn>import</span> <span class=n>BCEWithLogitsLoss</span><span class=p>,</span> <span class=n>CrossEntropyLoss</span><span class=p>,</span> <span class=n>MSELoss</span>

<span class=kn>from</span> <span class=nn>...activations</span> <span class=kn>import</span> <span class=n>ACT2FN</span>
<span class=kn>from</span> <span class=nn>...modeling_attn_mask_utils</span> <span class=kn>import</span> <span class=p>(</span>
    <span class=n>_prepare_4d_attention_mask</span><span class=p>,</span>
    <span class=n>_prepare_4d_attention_mask_for_sdpa</span><span class=p>,</span>
    <span class=n>_prepare_4d_causal_attention_mask</span><span class=p>,</span>
    <span class=n>_prepare_4d_causal_attention_mask_for_sdpa</span><span class=p>,</span>
<span class=p>)</span>
<span class=kn>from</span> <span class=nn>...modeling_outputs</span> <span class=kn>import</span> <span class=p>(</span>
    <span class=n>BaseModelOutput</span><span class=p>,</span>
    <span class=n>BaseModelOutputWithPastAndCrossAttentions</span><span class=p>,</span>
    <span class=n>CausalLMOutputWithCrossAttentions</span><span class=p>,</span>
    <span class=n>Seq2SeqLMOutput</span><span class=p>,</span>
    <span class=n>Seq2SeqModelOutput</span><span class=p>,</span>
    <span class=n>Seq2SeqQuestionAnsweringModelOutput</span><span class=p>,</span>
    <span class=n>Seq2SeqSequenceClassifierOutput</span><span class=p>,</span>
<span class=p>)</span>
<span class=kn>from</span> <span class=nn>...modeling_utils</span> <span class=kn>import</span> <span class=n>PreTrainedModel</span>
<span class=kn>from</span> <span class=nn>...utils</span> <span class=kn>import</span> <span class=p>(</span>
    <span class=n>add_code_sample_docstrings</span><span class=p>,</span>
    <span class=n>add_end_docstrings</span><span class=p>,</span>
    <span class=n>add_start_docstrings</span><span class=p>,</span>
    <span class=n>add_start_docstrings_to_model_forward</span><span class=p>,</span>
    <span class=n>is_flash_attn_2_available</span><span class=p>,</span>
    <span class=n>is_flash_attn_greater_or_equal_2_10</span><span class=p>,</span>
    <span class=n>logging</span><span class=p>,</span>
    <span class=n>replace_return_docstrings</span><span class=p>,</span>
<span class=p>)</span>
<span class=kn>from</span> <span class=nn>.configuration_bart</span> <span class=kn>import</span> <span class=n>BartConfig</span>


<span class=k>if</span> <span class=n>is_flash_attn_2_available</span><span class=p>():</span>
    <span class=kn>from</span> <span class=nn>flash_attn</span> <span class=kn>import</span> <span class=n>flash_attn_func</span><span class=p>,</span> <span class=n>flash_attn_varlen_func</span>
    <span class=kn>from</span> <span class=nn>flash_attn.bert_padding</span> <span class=kn>import</span> <span class=n>index_first_axis</span><span class=p>,</span> <span class=n>pad_input</span><span class=p>,</span> <span class=n>unpad_input</span>  <span class=c1># noqa</span>


<span class=n>logger</span> <span class=o>=</span> <span class=n>logging</span><span class=o>.</span><span class=n>get_logger</span><span class=p>(</span><span class=vm>__name__</span><span class=p>)</span>

<span class=n>_CHECKPOINT_FOR_DOC</span> <span class=o>=</span> <span class=s2>"facebook/bart-base"</span>
<span class=n>_CONFIG_FOR_DOC</span> <span class=o>=</span> <span class=s2>"BartConfig"</span>

<span class=c1># Base model docstring</span>
<span class=n>_EXPECTED_OUTPUT_SHAPE</span> <span class=o>=</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>768</span><span class=p>]</span>

<span class=c1># SequenceClassification docstring</span>
<span class=n>_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION</span> <span class=o>=</span> <span class=s2>"valhalla/bart-large-sst2"</span>
<span class=n>_SEQ_CLASS_EXPECTED_LOSS</span> <span class=o>=</span> <span class=mf>0.0</span>
<span class=n>_SEQ_CLASS_EXPECTED_OUTPUT</span> <span class=o>=</span> <span class=s2>"'POSITIVE'"</span>

<span class=c1># QuestionAsnwering docstring</span>
<span class=n>_CHECKPOINT_FOR_QA</span> <span class=o>=</span> <span class=s2>"valhalla/bart-large-finetuned-squadv1"</span>
<span class=n>_QA_EXPECTED_LOSS</span> <span class=o>=</span> <span class=mf>0.59</span>
<span class=n>_QA_EXPECTED_OUTPUT</span> <span class=o>=</span> <span class=s2>"' nice puppet'"</span>


<span class=n>BART_PRETRAINED_MODEL_ARCHIVE_LIST</span> <span class=o>=</span> <span class=p>[</span>
    <span class=s2>"facebook/bart-large"</span><span class=p>,</span>
    <span class=c1># see all BART models at https://huggingface.co/models?filter=bart</span>
<span class=p>]</span>


<span class=c1># Copied from transformers.models.llama.modeling_llama._get_unpad_data</span>
<span class=k>def</span> <span class=nf>_get_unpad_data</span><span class=p>(</span><span class=n>attention_mask</span><span class=p>):</span>
    <span class=n>seqlens_in_batch</span> <span class=o>=</span> <span class=n>attention_mask</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
    <span class=n>indices</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nonzero</span><span class=p>(</span><span class=n>attention_mask</span><span class=o>.</span><span class=n>flatten</span><span class=p>(),</span> <span class=n>as_tuple</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span><span class=o>.</span><span class=n>flatten</span><span class=p>()</span>
    <span class=n>max_seqlen_in_batch</span> <span class=o>=</span> <span class=n>seqlens_in_batch</span><span class=o>.</span><span class=n>max</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
    <span class=n>cu_seqlens</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>pad</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>cumsum</span><span class=p>(</span><span class=n>seqlens_in_batch</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>torch</span><span class=o>.</span><span class=n>int32</span><span class=p>),</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>))</span>
    <span class=k>return</span> <span class=p>(</span>
        <span class=n>indices</span><span class=p>,</span>
        <span class=n>cu_seqlens</span><span class=p>,</span>
        <span class=n>max_seqlen_in_batch</span><span class=p>,</span>
    <span class=p>)</span>


<span class=k>def</span> <span class=nf>shift_tokens_right</span><span class=p>(</span><span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>pad_token_id</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>decoder_start_token_id</span><span class=p>:</span> <span class=nb>int</span><span class=p>):</span>
<span class=w>    </span><span class=sd>"""</span>
<span class=sd>    Shift input ids one token to the right.</span>
<span class=sd>    """</span>
    <span class=n>shifted_input_ids</span> <span class=o>=</span> <span class=n>input_ids</span><span class=o>.</span><span class=n>new_zeros</span><span class=p>(</span><span class=n>input_ids</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
    <span class=n>shifted_input_ids</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>:]</span> <span class=o>=</span> <span class=n>input_ids</span><span class=p>[:,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>clone</span><span class=p>()</span>
    <span class=n>shifted_input_ids</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=n>decoder_start_token_id</span>

    <span class=k>if</span> <span class=n>pad_token_id</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
        <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=s2>"self.model.config.pad_token_id has to be defined."</span><span class=p>)</span>
    <span class=c1># replace possible -100 values in labels by `pad_token_id`</span>
    <span class=n>shifted_input_ids</span><span class=o>.</span><span class=n>masked_fill_</span><span class=p>(</span><span class=n>shifted_input_ids</span> <span class=o>==</span> <span class=o>-</span><span class=mi>100</span><span class=p>,</span> <span class=n>pad_token_id</span><span class=p>)</span>

    <span class=k>return</span> <span class=n>shifted_input_ids</span>


<span class=k>class</span> <span class=nc>BartLearnedPositionalEmbedding</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>):</span>
<span class=w>    </span><span class=sd>"""</span>
<span class=sd>    This module learns positional embeddings up to a fixed maximum size.</span>
<span class=sd>    """</span>

    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>num_embeddings</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>):</span>
        <span class=c1># Bart is set up so that if padding_idx is specified then offset the embedding ids by 2</span>
        <span class=c1># and adjust num_embeddings appropriately. Other models don't have this hack</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>offset</span> <span class=o>=</span> <span class=mi>2</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>num_embeddings</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>offset</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>)</span>

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>past_key_values_length</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>0</span><span class=p>):</span>
<span class=w>        </span><span class=sd>"""`input_ids' shape is expected to be [bsz x seqlen]."""</span>

        <span class=n>bsz</span><span class=p>,</span> <span class=n>seq_len</span> <span class=o>=</span> <span class=n>input_ids</span><span class=o>.</span><span class=n>shape</span><span class=p>[:</span><span class=mi>2</span><span class=p>]</span>
        <span class=n>positions</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span>
            <span class=n>past_key_values_length</span><span class=p>,</span> <span class=n>past_key_values_length</span> <span class=o>+</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>long</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>device</span>
        <span class=p>)</span><span class=o>.</span><span class=n>expand</span><span class=p>(</span><span class=n>bsz</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>

        <span class=k>return</span> <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>positions</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>offset</span><span class=p>)</span>


<span class=k>class</span> <span class=nc>BartAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>"""Multi-headed attention from 'Attention Is All You Need' paper"""</span>

    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span>
        <span class=n>embed_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
        <span class=n>num_heads</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
        <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.0</span><span class=p>,</span>
        <span class=n>is_decoder</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
        <span class=n>bias</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
        <span class=n>is_causal</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
        <span class=n>config</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>BartConfig</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
    <span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span> <span class=o>=</span> <span class=n>embed_dim</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>=</span> <span class=n>num_heads</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>dropout</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span> <span class=o>=</span> <span class=n>embed_dim</span> <span class=o>//</span> <span class=n>num_heads</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>config</span> <span class=o>=</span> <span class=n>config</span>

        <span class=k>if</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span> <span class=o>*</span> <span class=n>num_heads</span><span class=p>)</span> <span class=o>!=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span><span class=p>:</span>
            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span>
                <span class=sa>f</span><span class=s2>"embed_dim must be divisible by num_heads (got `embed_dim`: </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span><span class=si>}</span><span class=s2>"</span>
                <span class=sa>f</span><span class=s2>" and `num_heads`: </span><span class=si>{</span><span class=n>num_heads</span><span class=si>}</span><span class=s2>)."</span>
            <span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>scaling</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=o>**-</span><span class=mf>0.5</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>is_decoder</span> <span class=o>=</span> <span class=n>is_decoder</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>is_causal</span> <span class=o>=</span> <span class=n>is_causal</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>bias</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>bias</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>q_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>bias</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>out_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>bias</span><span class=p>)</span>

    <span class=k>def</span> <span class=nf>_shape</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>tensor</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>bsz</span><span class=p>:</span> <span class=nb>int</span><span class=p>):</span>
        <span class=k>return</span> <span class=n>tensor</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bsz</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span>

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span>
        <span class=n>hidden_states</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
        <span class=n>key_value_states</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>past_key_value</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>layer_head_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>output_attentions</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
    <span class=p>)</span> <span class=o>-></span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>],</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]]]:</span>
<span class=w>        </span><span class=sd>"""Input shape: Batch x Time x Channel"""</span>

        <span class=c1># if key_value_states are provided this layer is used as a cross-attention layer</span>
        <span class=c1># for the decoder</span>
        <span class=n>is_cross_attention</span> <span class=o>=</span> <span class=n>key_value_states</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span>

        <span class=n>bsz</span><span class=p>,</span> <span class=n>tgt_len</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>hidden_states</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>

        <span class=c1># get query proj</span>
        <span class=n>query_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>q_proj</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>scaling</span>
        <span class=c1># get key, value proj</span>
        <span class=c1># `past_key_value[0].shape[2] == key_value_states.shape[1]`</span>
        <span class=c1># is checking that the `sequence_length` of the `past_key_value` is the same as</span>
        <span class=c1># the provided `key_value_states` to support prefix tuning</span>
        <span class=k>if</span> <span class=p>(</span>
            <span class=n>is_cross_attention</span>
            <span class=ow>and</span> <span class=n>past_key_value</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span>
            <span class=ow>and</span> <span class=n>past_key_value</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span> <span class=o>==</span> <span class=n>key_value_states</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
        <span class=p>):</span>
            <span class=c1># reuse k,v, cross_attentions</span>
            <span class=n>key_states</span> <span class=o>=</span> <span class=n>past_key_value</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
            <span class=n>value_states</span> <span class=o>=</span> <span class=n>past_key_value</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
        <span class=k>elif</span> <span class=n>is_cross_attention</span><span class=p>:</span>
            <span class=c1># cross_attentions</span>
            <span class=n>key_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_shape</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span><span class=p>(</span><span class=n>key_value_states</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>bsz</span><span class=p>)</span>
            <span class=n>value_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_shape</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span><span class=p>(</span><span class=n>key_value_states</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>bsz</span><span class=p>)</span>
        <span class=k>elif</span> <span class=n>past_key_value</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=c1># reuse k, v, self_attention</span>
            <span class=n>key_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_shape</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>bsz</span><span class=p>)</span>
            <span class=n>value_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_shape</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>bsz</span><span class=p>)</span>
            <span class=n>key_states</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>past_key_value</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>key_states</span><span class=p>],</span> <span class=n>dim</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
            <span class=n>value_states</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>past_key_value</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>value_states</span><span class=p>],</span> <span class=n>dim</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=c1># self_attention</span>
            <span class=n>key_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_shape</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>bsz</span><span class=p>)</span>
            <span class=n>value_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_shape</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>bsz</span><span class=p>)</span>

        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_decoder</span><span class=p>:</span>
            <span class=c1># if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.</span>
            <span class=c1># Further calls to cross_attention layer can then reuse all cross-attention</span>
            <span class=c1># key/value_states (first "if" case)</span>
            <span class=c1># if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of</span>
            <span class=c1># all previous decoder key/value_states. Further calls to uni-directional self-attention</span>
            <span class=c1># can concat previous decoder key/value_states to current projected key/value_states (third "elif" case)</span>
            <span class=c1># if encoder bi-directional self-attention `past_key_value` is always `None`</span>
            <span class=n>past_key_value</span> <span class=o>=</span> <span class=p>(</span><span class=n>key_states</span><span class=p>,</span> <span class=n>value_states</span><span class=p>)</span>

        <span class=n>proj_shape</span> <span class=o>=</span> <span class=p>(</span><span class=n>bsz</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
        <span class=n>query_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_shape</span><span class=p>(</span><span class=n>query_states</span><span class=p>,</span> <span class=n>tgt_len</span><span class=p>,</span> <span class=n>bsz</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>*</span><span class=n>proj_shape</span><span class=p>)</span>
        <span class=n>key_states</span> <span class=o>=</span> <span class=n>key_states</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>*</span><span class=n>proj_shape</span><span class=p>)</span>
        <span class=n>value_states</span> <span class=o>=</span> <span class=n>value_states</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>*</span><span class=n>proj_shape</span><span class=p>)</span>

        <span class=n>src_len</span> <span class=o>=</span> <span class=n>key_states</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
        <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>bmm</span><span class=p>(</span><span class=n>query_states</span><span class=p>,</span> <span class=n>key_states</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>

        <span class=k>if</span> <span class=n>attn_weights</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>!=</span> <span class=p>(</span><span class=n>bsz</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=n>tgt_len</span><span class=p>,</span> <span class=n>src_len</span><span class=p>):</span>
            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span>
                <span class=sa>f</span><span class=s2>"Attention weights should be of size </span><span class=si>{</span><span class=p>(</span><span class=n>bsz</span><span class=w> </span><span class=o>*</span><span class=w> </span><span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span><span class=w> </span><span class=n>tgt_len</span><span class=p>,</span><span class=w> </span><span class=n>src_len</span><span class=p>)</span><span class=si>}</span><span class=s2>, but is"</span>
                <span class=sa>f</span><span class=s2>" </span><span class=si>{</span><span class=n>attn_weights</span><span class=o>.</span><span class=n>size</span><span class=p>()</span><span class=si>}</span><span class=s2>"</span>
            <span class=p>)</span>

        <span class=k>if</span> <span class=n>attention_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=k>if</span> <span class=n>attention_mask</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>!=</span> <span class=p>(</span><span class=n>bsz</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>tgt_len</span><span class=p>,</span> <span class=n>src_len</span><span class=p>):</span>
                <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span>
                    <span class=sa>f</span><span class=s2>"Attention mask should be of size </span><span class=si>{</span><span class=p>(</span><span class=n>bsz</span><span class=p>,</span><span class=w> </span><span class=mi>1</span><span class=p>,</span><span class=w> </span><span class=n>tgt_len</span><span class=p>,</span><span class=w> </span><span class=n>src_len</span><span class=p>)</span><span class=si>}</span><span class=s2>, but is </span><span class=si>{</span><span class=n>attention_mask</span><span class=o>.</span><span class=n>size</span><span class=p>()</span><span class=si>}</span><span class=s2>"</span>
                <span class=p>)</span>
            <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>attn_weights</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bsz</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=n>tgt_len</span><span class=p>,</span> <span class=n>src_len</span><span class=p>)</span> <span class=o>+</span> <span class=n>attention_mask</span>
            <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>attn_weights</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bsz</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=n>tgt_len</span><span class=p>,</span> <span class=n>src_len</span><span class=p>)</span>

        <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>attn_weights</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>

        <span class=k>if</span> <span class=n>layer_head_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=k>if</span> <span class=n>layer_head_mask</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>!=</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,):</span>
                <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span>
                    <span class=sa>f</span><span class=s2>"Head mask for a single layer should be of size </span><span class=si>{</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,)</span><span class=si>}</span><span class=s2>, but is"</span>
                    <span class=sa>f</span><span class=s2>" </span><span class=si>{</span><span class=n>layer_head_mask</span><span class=o>.</span><span class=n>size</span><span class=p>()</span><span class=si>}</span><span class=s2>"</span>
                <span class=p>)</span>
            <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>layer_head_mask</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span> <span class=o>*</span> <span class=n>attn_weights</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bsz</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=n>tgt_len</span><span class=p>,</span> <span class=n>src_len</span><span class=p>)</span>
            <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>attn_weights</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bsz</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=n>tgt_len</span><span class=p>,</span> <span class=n>src_len</span><span class=p>)</span>

        <span class=k>if</span> <span class=n>output_attentions</span><span class=p>:</span>
            <span class=c1># this operation is a bit awkward, but it's required to</span>
            <span class=c1># make sure that attn_weights keeps its gradient.</span>
            <span class=c1># In order to do so, attn_weights have to be reshaped</span>
            <span class=c1># twice and have to be reused in the following</span>
            <span class=n>attn_weights_reshaped</span> <span class=o>=</span> <span class=n>attn_weights</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bsz</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=n>tgt_len</span><span class=p>,</span> <span class=n>src_len</span><span class=p>)</span>
            <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>attn_weights_reshaped</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bsz</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=n>tgt_len</span><span class=p>,</span> <span class=n>src_len</span><span class=p>)</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=n>attn_weights_reshaped</span> <span class=o>=</span> <span class=kc>None</span>

        <span class=n>attn_probs</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>attn_weights</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>,</span> <span class=n>training</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>training</span><span class=p>)</span>

        <span class=n>attn_output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>bmm</span><span class=p>(</span><span class=n>attn_probs</span><span class=p>,</span> <span class=n>value_states</span><span class=p>)</span>

        <span class=k>if</span> <span class=n>attn_output</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>!=</span> <span class=p>(</span><span class=n>bsz</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=n>tgt_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>):</span>
            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span>
                <span class=sa>f</span><span class=s2>"`attn_output` should be of size </span><span class=si>{</span><span class=p>(</span><span class=n>bsz</span><span class=w> </span><span class=o>*</span><span class=w> </span><span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span><span class=w> </span><span class=n>tgt_len</span><span class=p>,</span><span class=w> </span><span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=si>}</span><span class=s2>, but is"</span>
                <span class=sa>f</span><span class=s2>" </span><span class=si>{</span><span class=n>attn_output</span><span class=o>.</span><span class=n>size</span><span class=p>()</span><span class=si>}</span><span class=s2>"</span>
            <span class=p>)</span>

        <span class=n>attn_output</span> <span class=o>=</span> <span class=n>attn_output</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bsz</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=n>tgt_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
        <span class=n>attn_output</span> <span class=o>=</span> <span class=n>attn_output</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>

        <span class=c1># Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be</span>
        <span class=c1># partitioned across GPUs when using tensor-parallelism.</span>
        <span class=n>attn_output</span> <span class=o>=</span> <span class=n>attn_output</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>bsz</span><span class=p>,</span> <span class=n>tgt_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span><span class=p>)</span>

        <span class=n>attn_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>out_proj</span><span class=p>(</span><span class=n>attn_output</span><span class=p>)</span>

        <span class=k>return</span> <span class=n>attn_output</span><span class=p>,</span> <span class=n>attn_weights_reshaped</span><span class=p>,</span> <span class=n>past_key_value</span>


<span class=k>class</span> <span class=nc>BartFlashAttention2</span><span class=p>(</span><span class=n>BartAttention</span><span class=p>):</span>
<span class=w>    </span><span class=sd>"""</span>
<span class=sd>    Bart flash attention module. This module inherits from `BartAttention` as the weights of the module stays</span>
<span class=sd>    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of</span>
<span class=sd>    flash attention and deal with padding tokens in case the input contains any of them.</span>
<span class=sd>    """</span>

    <span class=c1># Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>

        <span class=c1># TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.</span>
        <span class=c1># flash_attn&LT2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.</span>
        <span class=c1># Beware that with flash_attn&LT2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>_flash_attn_uses_top_left_mask</span> <span class=o>=</span> <span class=ow>not</span> <span class=n>is_flash_attn_greater_or_equal_2_10</span><span class=p>()</span>

    <span class=k>def</span> <span class=nf>_reshape</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>tensor</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>bsz</span><span class=p>:</span> <span class=nb>int</span><span class=p>):</span>
        <span class=k>return</span> <span class=n>tensor</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bsz</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span>
        <span class=n>hidden_states</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
        <span class=n>key_value_states</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>past_key_value</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>layer_head_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>output_attentions</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
    <span class=p>)</span> <span class=o>-></span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>],</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]]]:</span>
        <span class=c1># BartFlashAttention2 attention does not support output_attentions</span>
        <span class=k>if</span> <span class=n>output_attentions</span><span class=p>:</span>
            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=s2>"BartFlashAttention2 attention does not support output_attentions"</span><span class=p>)</span>

        <span class=c1># if key_value_states are provided this layer is used as a cross-attention layer</span>
        <span class=c1># for the decoder</span>
        <span class=n>is_cross_attention</span> <span class=o>=</span> <span class=n>key_value_states</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span>

        <span class=n>bsz</span><span class=p>,</span> <span class=n>q_len</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>hidden_states</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>

        <span class=c1># get query proj</span>
        <span class=n>query_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_reshape</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>q_proj</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>bsz</span><span class=p>)</span>
        <span class=c1># get key, value proj</span>
        <span class=c1># `past_key_value[0].shape[2] == key_value_states.shape[1]`</span>
        <span class=c1># is checking that the `sequence_length` of the `past_key_value` is the same as</span>
        <span class=c1># the provided `key_value_states` to support prefix tuning</span>
        <span class=k>if</span> <span class=p>(</span>
            <span class=n>is_cross_attention</span>
            <span class=ow>and</span> <span class=n>past_key_value</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span>
            <span class=ow>and</span> <span class=n>past_key_value</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span> <span class=o>==</span> <span class=n>key_value_states</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
        <span class=p>):</span>
            <span class=c1># reuse k,v, cross_attentions</span>
            <span class=n>key_states</span> <span class=o>=</span> <span class=n>past_key_value</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
            <span class=n>value_states</span> <span class=o>=</span> <span class=n>past_key_value</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
        <span class=k>elif</span> <span class=n>is_cross_attention</span><span class=p>:</span>
            <span class=c1># cross_attentions</span>
            <span class=n>key_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_reshape</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span><span class=p>(</span><span class=n>key_value_states</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>bsz</span><span class=p>)</span>
            <span class=n>value_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_reshape</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span><span class=p>(</span><span class=n>key_value_states</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>bsz</span><span class=p>)</span>
        <span class=k>elif</span> <span class=n>past_key_value</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=c1># reuse k, v, self_attention</span>
            <span class=n>key_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_reshape</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>bsz</span><span class=p>)</span>
            <span class=n>value_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_reshape</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>bsz</span><span class=p>)</span>
            <span class=n>key_states</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>past_key_value</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span> <span class=n>key_states</span><span class=p>],</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
            <span class=n>value_states</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>past_key_value</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span> <span class=n>value_states</span><span class=p>],</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=c1># self_attention</span>
            <span class=n>key_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_reshape</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>bsz</span><span class=p>)</span>
            <span class=n>value_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_reshape</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>bsz</span><span class=p>)</span>

        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_decoder</span><span class=p>:</span>
            <span class=c1># if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.</span>
            <span class=c1># Further calls to cross_attention layer can then reuse all cross-attention</span>
            <span class=c1># key/value_states (first "if" case)</span>
            <span class=c1># if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of</span>
            <span class=c1># all previous decoder key/value_states. Further calls to uni-directional self-attention</span>
            <span class=c1># can concat previous decoder key/value_states to current projected key/value_states (third "elif" case)</span>
            <span class=c1># if encoder bi-directional self-attention `past_key_value` is always `None`</span>
            <span class=n>past_key_value</span> <span class=o>=</span> <span class=p>(</span><span class=n>key_states</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span> <span class=n>value_states</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>

        <span class=n>kv_seq_len</span> <span class=o>=</span> <span class=n>key_states</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span>
        <span class=k>if</span> <span class=n>past_key_value</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=n>kv_seq_len</span> <span class=o>+=</span> <span class=n>past_key_value</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span>

        <span class=c1># In PEFT, usually we cast the layer norms in float32 for training stability reasons</span>
        <span class=c1># therefore the input hidden states gets silently casted in float32. Hence, we need</span>
        <span class=c1># cast them back in the correct dtype just to be sure everything works as expected.</span>
        <span class=c1># This might slowdown training & inference so it is recommended to not cast the LayerNorms</span>
        <span class=c1># in fp32. (LlamaRMSNorm handles it correctly)</span>

        <span class=n>input_dtype</span> <span class=o>=</span> <span class=n>query_states</span><span class=o>.</span><span class=n>dtype</span>
        <span class=k>if</span> <span class=n>input_dtype</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>:</span>
            <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>is_autocast_enabled</span><span class=p>():</span>
                <span class=n>target_dtype</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>get_autocast_gpu_dtype</span><span class=p>()</span>
            <span class=c1># Handle the case where the model is quantized</span>
            <span class=k>elif</span> <span class=nb>hasattr</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=p>,</span> <span class=s2>"_pre_quantization_dtype"</span><span class=p>):</span>
                <span class=n>target_dtype</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>_pre_quantization_dtype</span>
            <span class=k>else</span><span class=p>:</span>
                <span class=n>target_dtype</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>q_proj</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>dtype</span>

            <span class=n>logger</span><span class=o>.</span><span class=n>warning_once</span><span class=p>(</span>
                <span class=sa>f</span><span class=s2>"The input hidden states seems to be silently casted in float32, this might be related to"</span>
                <span class=sa>f</span><span class=s2>" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in"</span>
                <span class=sa>f</span><span class=s2>" </span><span class=si>{</span><span class=n>target_dtype</span><span class=si>}</span><span class=s2>."</span>
            <span class=p>)</span>

            <span class=n>query_states</span> <span class=o>=</span> <span class=n>query_states</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>target_dtype</span><span class=p>)</span>
            <span class=n>key_states</span> <span class=o>=</span> <span class=n>key_states</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>target_dtype</span><span class=p>)</span>
            <span class=n>value_states</span> <span class=o>=</span> <span class=n>value_states</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>target_dtype</span><span class=p>)</span>

        <span class=n>attn_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_flash_attention_forward</span><span class=p>(</span>
            <span class=n>query_states</span><span class=p>,</span> <span class=n>key_states</span><span class=p>,</span> <span class=n>value_states</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>,</span> <span class=n>q_len</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>dropout</span>
        <span class=p>)</span>

        <span class=n>attn_output</span> <span class=o>=</span> <span class=n>attn_output</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>bsz</span><span class=p>,</span> <span class=n>q_len</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
        <span class=n>attn_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>out_proj</span><span class=p>(</span><span class=n>attn_output</span><span class=p>)</span>

        <span class=k>if</span> <span class=ow>not</span> <span class=n>output_attentions</span><span class=p>:</span>
            <span class=n>attn_weights</span> <span class=o>=</span> <span class=kc>None</span>

        <span class=k>return</span> <span class=n>attn_output</span><span class=p>,</span> <span class=n>attn_weights</span><span class=p>,</span> <span class=n>past_key_value</span>

    <span class=c1># Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2._flash_attention_forward</span>
    <span class=k>def</span> <span class=nf>_flash_attention_forward</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span> <span class=n>query_states</span><span class=p>,</span> <span class=n>key_states</span><span class=p>,</span> <span class=n>value_states</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>,</span> <span class=n>query_length</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=mf>0.0</span><span class=p>,</span> <span class=n>softmax_scale</span><span class=o>=</span><span class=kc>None</span>
    <span class=p>):</span>
<span class=w>        </span><span class=sd>"""</span>
<span class=sd>        Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token</span>
<span class=sd>        first unpad the input, then computes the attention scores and pad the final attention scores.</span>

<span class=sd>        Args:</span>
<span class=sd>            query_states (`torch.Tensor`):</span>
<span class=sd>                Input query states to be passed to Flash Attention API</span>
<span class=sd>            key_states (`torch.Tensor`):</span>
<span class=sd>                Input key states to be passed to Flash Attention API</span>
<span class=sd>            value_states (`torch.Tensor`):</span>
<span class=sd>                Input value states to be passed to Flash Attention API</span>
<span class=sd>            attention_mask (`torch.Tensor`):</span>
<span class=sd>                The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the</span>
<span class=sd>                position of padding tokens and 1 for the position of non-padding tokens.</span>
<span class=sd>            dropout (`int`, *optional*):</span>
<span class=sd>                Attention dropout</span>
<span class=sd>            softmax_scale (`float`, *optional*):</span>
<span class=sd>                The scaling of QK^T before applying softmax. Default to 1 / sqrt(head_dim)</span>
<span class=sd>        """</span>
        <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>_flash_attn_uses_top_left_mask</span><span class=p>:</span>
            <span class=n>causal</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_causal</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=c1># TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.</span>
            <span class=n>causal</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_causal</span> <span class=ow>and</span> <span class=n>query_length</span> <span class=o>!=</span> <span class=mi>1</span>

        <span class=c1># Contains at least one padding token in the sequence</span>
        <span class=k>if</span> <span class=n>attention_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=n>batch_size</span> <span class=o>=</span> <span class=n>query_states</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
            <span class=n>query_states</span><span class=p>,</span> <span class=n>key_states</span><span class=p>,</span> <span class=n>value_states</span><span class=p>,</span> <span class=n>indices_q</span><span class=p>,</span> <span class=n>cu_seq_lens</span><span class=p>,</span> <span class=n>max_seq_lens</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_upad_input</span><span class=p>(</span>
                <span class=n>query_states</span><span class=p>,</span> <span class=n>key_states</span><span class=p>,</span> <span class=n>value_states</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>,</span> <span class=n>query_length</span>
            <span class=p>)</span>

            <span class=n>cu_seqlens_q</span><span class=p>,</span> <span class=n>cu_seqlens_k</span> <span class=o>=</span> <span class=n>cu_seq_lens</span>
            <span class=n>max_seqlen_in_batch_q</span><span class=p>,</span> <span class=n>max_seqlen_in_batch_k</span> <span class=o>=</span> <span class=n>max_seq_lens</span>

            <span class=n>attn_output_unpad</span> <span class=o>=</span> <span class=n>flash_attn_varlen_func</span><span class=p>(</span>
                <span class=n>query_states</span><span class=p>,</span>
                <span class=n>key_states</span><span class=p>,</span>
                <span class=n>value_states</span><span class=p>,</span>
                <span class=n>cu_seqlens_q</span><span class=o>=</span><span class=n>cu_seqlens_q</span><span class=p>,</span>
                <span class=n>cu_seqlens_k</span><span class=o>=</span><span class=n>cu_seqlens_k</span><span class=p>,</span>
                <span class=n>max_seqlen_q</span><span class=o>=</span><span class=n>max_seqlen_in_batch_q</span><span class=p>,</span>
                <span class=n>max_seqlen_k</span><span class=o>=</span><span class=n>max_seqlen_in_batch_k</span><span class=p>,</span>
                <span class=n>dropout_p</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span>
                <span class=n>softmax_scale</span><span class=o>=</span><span class=n>softmax_scale</span><span class=p>,</span>
                <span class=n>causal</span><span class=o>=</span><span class=n>causal</span><span class=p>,</span>
            <span class=p>)</span>

            <span class=n>attn_output</span> <span class=o>=</span> <span class=n>pad_input</span><span class=p>(</span><span class=n>attn_output_unpad</span><span class=p>,</span> <span class=n>indices_q</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>,</span> <span class=n>query_length</span><span class=p>)</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=n>attn_output</span> <span class=o>=</span> <span class=n>flash_attn_func</span><span class=p>(</span>
                <span class=n>query_states</span><span class=p>,</span> <span class=n>key_states</span><span class=p>,</span> <span class=n>value_states</span><span class=p>,</span> <span class=n>dropout</span><span class=p>,</span> <span class=n>softmax_scale</span><span class=o>=</span><span class=n>softmax_scale</span><span class=p>,</span> <span class=n>causal</span><span class=o>=</span><span class=n>causal</span>
            <span class=p>)</span>

        <span class=k>return</span> <span class=n>attn_output</span>

    <span class=c1># Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2._upad_input</span>
    <span class=k>def</span> <span class=nf>_upad_input</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>query_layer</span><span class=p>,</span> <span class=n>key_layer</span><span class=p>,</span> <span class=n>value_layer</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>,</span> <span class=n>query_length</span><span class=p>):</span>
        <span class=n>indices_k</span><span class=p>,</span> <span class=n>cu_seqlens_k</span><span class=p>,</span> <span class=n>max_seqlen_in_batch_k</span> <span class=o>=</span> <span class=n>_get_unpad_data</span><span class=p>(</span><span class=n>attention_mask</span><span class=p>)</span>
        <span class=n>batch_size</span><span class=p>,</span> <span class=n>kv_seq_len</span><span class=p>,</span> <span class=n>num_key_value_heads</span><span class=p>,</span> <span class=n>head_dim</span> <span class=o>=</span> <span class=n>key_layer</span><span class=o>.</span><span class=n>shape</span>

        <span class=n>key_layer</span> <span class=o>=</span> <span class=n>index_first_axis</span><span class=p>(</span>
            <span class=n>key_layer</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>batch_size</span> <span class=o>*</span> <span class=n>kv_seq_len</span><span class=p>,</span> <span class=n>num_key_value_heads</span><span class=p>,</span> <span class=n>head_dim</span><span class=p>),</span> <span class=n>indices_k</span>
        <span class=p>)</span>
        <span class=n>value_layer</span> <span class=o>=</span> <span class=n>index_first_axis</span><span class=p>(</span>
            <span class=n>value_layer</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>batch_size</span> <span class=o>*</span> <span class=n>kv_seq_len</span><span class=p>,</span> <span class=n>num_key_value_heads</span><span class=p>,</span> <span class=n>head_dim</span><span class=p>),</span> <span class=n>indices_k</span>
        <span class=p>)</span>
        <span class=k>if</span> <span class=n>query_length</span> <span class=o>==</span> <span class=n>kv_seq_len</span><span class=p>:</span>
            <span class=n>query_layer</span> <span class=o>=</span> <span class=n>index_first_axis</span><span class=p>(</span>
                <span class=n>query_layer</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>batch_size</span> <span class=o>*</span> <span class=n>kv_seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=n>head_dim</span><span class=p>),</span> <span class=n>indices_k</span>
            <span class=p>)</span>
            <span class=n>cu_seqlens_q</span> <span class=o>=</span> <span class=n>cu_seqlens_k</span>
            <span class=n>max_seqlen_in_batch_q</span> <span class=o>=</span> <span class=n>max_seqlen_in_batch_k</span>
            <span class=n>indices_q</span> <span class=o>=</span> <span class=n>indices_k</span>
        <span class=k>elif</span> <span class=n>query_length</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
            <span class=n>max_seqlen_in_batch_q</span> <span class=o>=</span> <span class=mi>1</span>
            <span class=n>cu_seqlens_q</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span>
                <span class=n>batch_size</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>int32</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>query_layer</span><span class=o>.</span><span class=n>device</span>
            <span class=p>)</span>  <span class=c1># There is a memcpy here, that is very bad.</span>
            <span class=n>indices_q</span> <span class=o>=</span> <span class=n>cu_seqlens_q</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
            <span class=n>query_layer</span> <span class=o>=</span> <span class=n>query_layer</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=c1># The -q_len: slice assumes left padding.</span>
            <span class=n>attention_mask</span> <span class=o>=</span> <span class=n>attention_mask</span><span class=p>[:,</span> <span class=o>-</span><span class=n>query_length</span><span class=p>:]</span>
            <span class=n>query_layer</span><span class=p>,</span> <span class=n>indices_q</span><span class=p>,</span> <span class=n>cu_seqlens_q</span><span class=p>,</span> <span class=n>max_seqlen_in_batch_q</span> <span class=o>=</span> <span class=n>unpad_input</span><span class=p>(</span><span class=n>query_layer</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>)</span>

        <span class=k>return</span> <span class=p>(</span>
            <span class=n>query_layer</span><span class=p>,</span>
            <span class=n>key_layer</span><span class=p>,</span>
            <span class=n>value_layer</span><span class=p>,</span>
            <span class=n>indices_q</span><span class=p>,</span>
            <span class=p>(</span><span class=n>cu_seqlens_q</span><span class=p>,</span> <span class=n>cu_seqlens_k</span><span class=p>),</span>
            <span class=p>(</span><span class=n>max_seqlen_in_batch_q</span><span class=p>,</span> <span class=n>max_seqlen_in_batch_k</span><span class=p>),</span>
        <span class=p>)</span>


<span class=k>class</span> <span class=nc>BartSdpaAttention</span><span class=p>(</span><span class=n>BartAttention</span><span class=p>):</span>
    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span>
        <span class=n>hidden_states</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
        <span class=n>key_value_states</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>past_key_value</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>layer_head_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>output_attentions</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
    <span class=p>)</span> <span class=o>-></span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>],</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]]]:</span>
<span class=w>        </span><span class=sd>"""Input shape: Batch x Time x Channel"""</span>
        <span class=k>if</span> <span class=n>output_attentions</span> <span class=ow>or</span> <span class=n>layer_head_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=c1># TODO: Improve this warning with e.g. `model.config._attn_implementation = "manual"` once this is implemented.</span>
            <span class=n>logger</span><span class=o>.</span><span class=n>warning_once</span><span class=p>(</span>
                <span class=s2>"BartModel is using BartSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention"</span>
                <span class=s1>' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.'</span>
            <span class=p>)</span>
            <span class=k>return</span> <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span>
                <span class=n>hidden_states</span><span class=p>,</span>
                <span class=n>key_value_states</span><span class=o>=</span><span class=n>key_value_states</span><span class=p>,</span>
                <span class=n>past_key_value</span><span class=o>=</span><span class=n>past_key_value</span><span class=p>,</span>
                <span class=n>attention_mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=p>,</span>
                <span class=n>layer_head_mask</span><span class=o>=</span><span class=n>layer_head_mask</span><span class=p>,</span>
                <span class=n>output_attentions</span><span class=o>=</span><span class=n>output_attentions</span><span class=p>,</span>
            <span class=p>)</span>

        <span class=c1># if key_value_states are provided this layer is used as a cross-attention layer</span>
        <span class=c1># for the decoder</span>
        <span class=n>is_cross_attention</span> <span class=o>=</span> <span class=n>key_value_states</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span>

        <span class=n>bsz</span><span class=p>,</span> <span class=n>tgt_len</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>hidden_states</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>

        <span class=c1># get query proj</span>
        <span class=n>query_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>q_proj</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
        <span class=c1># get key, value proj</span>
        <span class=c1># `past_key_value[0].shape[2] == key_value_states.shape[1]`</span>
        <span class=c1># is checking that the `sequence_length` of the `past_key_value` is the same as</span>
        <span class=c1># the provided `key_value_states` to support prefix tuning</span>
        <span class=k>if</span> <span class=p>(</span>
            <span class=n>is_cross_attention</span>
            <span class=ow>and</span> <span class=n>past_key_value</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span>
            <span class=ow>and</span> <span class=n>past_key_value</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span> <span class=o>==</span> <span class=n>key_value_states</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
        <span class=p>):</span>
            <span class=c1># reuse k,v, cross_attentions</span>
            <span class=n>key_states</span> <span class=o>=</span> <span class=n>past_key_value</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
            <span class=n>value_states</span> <span class=o>=</span> <span class=n>past_key_value</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
        <span class=k>elif</span> <span class=n>is_cross_attention</span><span class=p>:</span>
            <span class=c1># cross_attentions</span>
            <span class=n>key_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_shape</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span><span class=p>(</span><span class=n>key_value_states</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>bsz</span><span class=p>)</span>
            <span class=n>value_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_shape</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span><span class=p>(</span><span class=n>key_value_states</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>bsz</span><span class=p>)</span>
        <span class=k>elif</span> <span class=n>past_key_value</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=c1># reuse k, v, self_attention</span>
            <span class=n>key_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_shape</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>bsz</span><span class=p>)</span>
            <span class=n>value_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_shape</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>bsz</span><span class=p>)</span>
            <span class=n>key_states</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>past_key_value</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>key_states</span><span class=p>],</span> <span class=n>dim</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
            <span class=n>value_states</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>past_key_value</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>value_states</span><span class=p>],</span> <span class=n>dim</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=c1># self_attention</span>
            <span class=n>key_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_shape</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>bsz</span><span class=p>)</span>
            <span class=n>value_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_shape</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>bsz</span><span class=p>)</span>

        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_decoder</span><span class=p>:</span>
            <span class=c1># if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.</span>
            <span class=c1># Further calls to cross_attention layer can then reuse all cross-attention</span>
            <span class=c1># key/value_states (first "if" case)</span>
            <span class=c1># if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of</span>
            <span class=c1># all previous decoder key/value_states. Further calls to uni-directional self-attention</span>
            <span class=c1># can concat previous decoder key/value_states to current projected key/value_states (third "elif" case)</span>
            <span class=c1># if encoder bi-directional self-attention `past_key_value` is always `None`</span>
            <span class=n>past_key_value</span> <span class=o>=</span> <span class=p>(</span><span class=n>key_states</span><span class=p>,</span> <span class=n>value_states</span><span class=p>)</span>

        <span class=n>query_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_shape</span><span class=p>(</span><span class=n>query_states</span><span class=p>,</span> <span class=n>tgt_len</span><span class=p>,</span> <span class=n>bsz</span><span class=p>)</span>

        <span class=c1># NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,</span>
        <span class=c1># but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577</span>
        <span class=n>attn_output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>scaled_dot_product_attention</span><span class=p>(</span>
            <span class=n>query_states</span><span class=p>,</span>
            <span class=n>key_states</span><span class=p>,</span>
            <span class=n>value_states</span><span class=p>,</span>
            <span class=n>attn_mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=p>,</span>
            <span class=n>dropout_p</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>training</span> <span class=k>else</span> <span class=mf>0.0</span><span class=p>,</span>
            <span class=c1># The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.</span>
            <span class=n>is_causal</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>is_causal</span> <span class=ow>and</span> <span class=n>attention_mask</span> <span class=ow>is</span> <span class=kc>None</span> <span class=ow>and</span> <span class=n>tgt_len</span> <span class=o>></span> <span class=mi>1</span><span class=p>,</span>
        <span class=p>)</span>

        <span class=k>if</span> <span class=n>attn_output</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>!=</span> <span class=p>(</span><span class=n>bsz</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=n>tgt_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>):</span>
            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span>
                <span class=sa>f</span><span class=s2>"`attn_output` should be of size </span><span class=si>{</span><span class=p>(</span><span class=n>bsz</span><span class=p>,</span><span class=w> </span><span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span><span class=w> </span><span class=n>tgt_len</span><span class=p>,</span><span class=w> </span><span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=si>}</span><span class=s2>, but is"</span>
                <span class=sa>f</span><span class=s2>" </span><span class=si>{</span><span class=n>attn_output</span><span class=o>.</span><span class=n>size</span><span class=p>()</span><span class=si>}</span><span class=s2>"</span>
            <span class=p>)</span>

        <span class=n>attn_output</span> <span class=o>=</span> <span class=n>attn_output</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>

        <span class=c1># Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be</span>
        <span class=c1># partitioned across GPUs when using tensor-parallelism.</span>
        <span class=n>attn_output</span> <span class=o>=</span> <span class=n>attn_output</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>bsz</span><span class=p>,</span> <span class=n>tgt_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span><span class=p>)</span>

        <span class=n>attn_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>out_proj</span><span class=p>(</span><span class=n>attn_output</span><span class=p>)</span>

        <span class=k>return</span> <span class=n>attn_output</span><span class=p>,</span> <span class=kc>None</span><span class=p>,</span> <span class=n>past_key_value</span>


<span class=n>BART_ATTENTION_CLASSES</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s2>"eager"</span><span class=p>:</span> <span class=n>BartAttention</span><span class=p>,</span>
    <span class=s2>"sdpa"</span><span class=p>:</span> <span class=n>BartSdpaAttention</span><span class=p>,</span>
    <span class=s2>"flash_attention_2"</span><span class=p>:</span> <span class=n>BartFlashAttention2</span><span class=p>,</span>
<span class=p>}</span>


<span class=k>class</span> <span class=nc>BartEncoderLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=n>BartConfig</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>d_model</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span> <span class=o>=</span> <span class=n>BART_ATTENTION_CLASSES</span><span class=p>[</span><span class=n>config</span><span class=o>.</span><span class=n>_attn_implementation</span><span class=p>](</span>
            <span class=n>embed_dim</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span><span class=p>,</span>
            <span class=n>num_heads</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>encoder_attention_heads</span><span class=p>,</span>
            <span class=n>dropout</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>attention_dropout</span><span class=p>,</span>
            <span class=n>config</span><span class=o>=</span><span class=n>config</span><span class=p>,</span>
        <span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>self_attn_layer_norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>dropout</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>activation_fn</span> <span class=o>=</span> <span class=n>ACT2FN</span><span class=p>[</span><span class=n>config</span><span class=o>.</span><span class=n>activation_function</span><span class=p>]</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>activation_dropout</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>activation_dropout</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>encoder_ffn_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>encoder_ffn_dim</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>final_layer_norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span><span class=p>)</span>

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span>
        <span class=n>hidden_states</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span>
        <span class=n>attention_mask</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span>
        <span class=n>layer_head_mask</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span>
        <span class=n>output_attentions</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
    <span class=p>)</span> <span class=o>-></span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]]:</span>
<span class=w>        </span><span class=sd>"""</span>
<span class=sd>        Args:</span>
<span class=sd>            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`</span>
<span class=sd>            attention_mask (`torch.FloatTensor`): attention mask of size</span>
<span class=sd>                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.</span>
<span class=sd>            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size</span>
<span class=sd>                `(encoder_attention_heads,)`.</span>
<span class=sd>            output_attentions (`bool`, *optional*):</span>
<span class=sd>                Whether or not to return the attentions tensors of all attention layers. See `attentions` under</span>
<span class=sd>                returned tensors for more detail.</span>
<span class=sd>        """</span>
        <span class=n>residual</span> <span class=o>=</span> <span class=n>hidden_states</span>
        <span class=n>hidden_states</span><span class=p>,</span> <span class=n>attn_weights</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span><span class=p>(</span>
            <span class=n>hidden_states</span><span class=o>=</span><span class=n>hidden_states</span><span class=p>,</span>
            <span class=n>attention_mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=p>,</span>
            <span class=n>layer_head_mask</span><span class=o>=</span><span class=n>layer_head_mask</span><span class=p>,</span>
            <span class=n>output_attentions</span><span class=o>=</span><span class=n>output_attentions</span><span class=p>,</span>
        <span class=p>)</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>,</span> <span class=n>training</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>training</span><span class=p>)</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>residual</span> <span class=o>+</span> <span class=n>hidden_states</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>self_attn_layer_norm</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>

        <span class=n>residual</span> <span class=o>=</span> <span class=n>hidden_states</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>activation_fn</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>))</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>activation_dropout</span><span class=p>,</span> <span class=n>training</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>training</span><span class=p>)</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>,</span> <span class=n>training</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>training</span><span class=p>)</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>residual</span> <span class=o>+</span> <span class=n>hidden_states</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>final_layer_norm</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>

        <span class=k>if</span> <span class=n>hidden_states</span><span class=o>.</span><span class=n>dtype</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>float16</span> <span class=ow>and</span> <span class=p>(</span>
            <span class=n>torch</span><span class=o>.</span><span class=n>isinf</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span><span class=o>.</span><span class=n>any</span><span class=p>()</span> <span class=ow>or</span> <span class=n>torch</span><span class=o>.</span><span class=n>isnan</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span><span class=o>.</span><span class=n>any</span><span class=p>()</span>
        <span class=p>):</span>
            <span class=n>clamp_value</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>finfo</span><span class=p>(</span><span class=n>hidden_states</span><span class=o>.</span><span class=n>dtype</span><span class=p>)</span><span class=o>.</span><span class=n>max</span> <span class=o>-</span> <span class=mi>1000</span>
            <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>clamp</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>,</span> <span class=nb>min</span><span class=o>=-</span><span class=n>clamp_value</span><span class=p>,</span> <span class=nb>max</span><span class=o>=</span><span class=n>clamp_value</span><span class=p>)</span>

        <span class=n>outputs</span> <span class=o>=</span> <span class=p>(</span><span class=n>hidden_states</span><span class=p>,)</span>

        <span class=k>if</span> <span class=n>output_attentions</span><span class=p>:</span>
            <span class=n>outputs</span> <span class=o>+=</span> <span class=p>(</span><span class=n>attn_weights</span><span class=p>,)</span>

        <span class=k>return</span> <span class=n>outputs</span>


<span class=k>class</span> <span class=nc>BartDecoderLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=n>BartConfig</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>d_model</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span> <span class=o>=</span> <span class=n>BART_ATTENTION_CLASSES</span><span class=p>[</span><span class=n>config</span><span class=o>.</span><span class=n>_attn_implementation</span><span class=p>](</span>
            <span class=n>embed_dim</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span><span class=p>,</span>
            <span class=n>num_heads</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>decoder_attention_heads</span><span class=p>,</span>
            <span class=n>dropout</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>attention_dropout</span><span class=p>,</span>
            <span class=n>is_decoder</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
            <span class=n>is_causal</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
            <span class=n>config</span><span class=o>=</span><span class=n>config</span><span class=p>,</span>
        <span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>dropout</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>activation_fn</span> <span class=o>=</span> <span class=n>ACT2FN</span><span class=p>[</span><span class=n>config</span><span class=o>.</span><span class=n>activation_function</span><span class=p>]</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>activation_dropout</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>activation_dropout</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>self_attn_layer_norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>encoder_attn</span> <span class=o>=</span> <span class=n>BART_ATTENTION_CLASSES</span><span class=p>[</span><span class=n>config</span><span class=o>.</span><span class=n>_attn_implementation</span><span class=p>](</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span><span class=p>,</span>
            <span class=n>config</span><span class=o>.</span><span class=n>decoder_attention_heads</span><span class=p>,</span>
            <span class=n>dropout</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>attention_dropout</span><span class=p>,</span>
            <span class=n>is_decoder</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
            <span class=n>config</span><span class=o>=</span><span class=n>config</span><span class=p>,</span>
        <span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>encoder_attn_layer_norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>decoder_ffn_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>decoder_ffn_dim</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>final_layer_norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span><span class=p>)</span>

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span>
        <span class=n>hidden_states</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
        <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>encoder_hidden_states</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>encoder_attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>layer_head_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>cross_attn_layer_head_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>past_key_value</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>output_attentions</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
        <span class=n>use_cache</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
    <span class=p>)</span> <span class=o>-></span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]]]:</span>
<span class=w>        </span><span class=sd>"""</span>
<span class=sd>        Args:</span>
<span class=sd>            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`</span>
<span class=sd>            attention_mask (`torch.FloatTensor`): attention mask of size</span>
<span class=sd>                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.</span>
<span class=sd>            encoder_hidden_states (`torch.FloatTensor`):</span>
<span class=sd>                cross attention input to the layer of shape `(batch, seq_len, embed_dim)`</span>
<span class=sd>            encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size</span>
<span class=sd>                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.</span>
<span class=sd>            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size</span>
<span class=sd>                `(encoder_attention_heads,)`.</span>
<span class=sd>            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of</span>
<span class=sd>                size `(decoder_attention_heads,)`.</span>
<span class=sd>            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states</span>
<span class=sd>            output_attentions (`bool`, *optional*):</span>
<span class=sd>                Whether or not to return the attentions tensors of all attention layers. See `attentions` under</span>
<span class=sd>                returned tensors for more detail.</span>
<span class=sd>        """</span>
        <span class=n>residual</span> <span class=o>=</span> <span class=n>hidden_states</span>

        <span class=c1># Self Attention</span>
        <span class=c1># decoder uni-directional self-attention cached key/values tuple is at positions 1,2</span>
        <span class=n>self_attn_past_key_value</span> <span class=o>=</span> <span class=n>past_key_value</span><span class=p>[:</span><span class=mi>2</span><span class=p>]</span> <span class=k>if</span> <span class=n>past_key_value</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=kc>None</span>
        <span class=c1># add present self-attn cache to positions 1,2 of present_key_value tuple</span>
        <span class=n>hidden_states</span><span class=p>,</span> <span class=n>self_attn_weights</span><span class=p>,</span> <span class=n>present_key_value</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span><span class=p>(</span>
            <span class=n>hidden_states</span><span class=o>=</span><span class=n>hidden_states</span><span class=p>,</span>
            <span class=n>past_key_value</span><span class=o>=</span><span class=n>self_attn_past_key_value</span><span class=p>,</span>
            <span class=n>attention_mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=p>,</span>
            <span class=n>layer_head_mask</span><span class=o>=</span><span class=n>layer_head_mask</span><span class=p>,</span>
            <span class=n>output_attentions</span><span class=o>=</span><span class=n>output_attentions</span><span class=p>,</span>
        <span class=p>)</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>,</span> <span class=n>training</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>training</span><span class=p>)</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>residual</span> <span class=o>+</span> <span class=n>hidden_states</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>self_attn_layer_norm</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>

        <span class=c1># Cross-Attention Block</span>
        <span class=n>cross_attn_present_key_value</span> <span class=o>=</span> <span class=kc>None</span>
        <span class=n>cross_attn_weights</span> <span class=o>=</span> <span class=kc>None</span>
        <span class=k>if</span> <span class=n>encoder_hidden_states</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=n>residual</span> <span class=o>=</span> <span class=n>hidden_states</span>

            <span class=c1># cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple</span>
            <span class=n>cross_attn_past_key_value</span> <span class=o>=</span> <span class=n>past_key_value</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>:]</span> <span class=k>if</span> <span class=n>past_key_value</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=kc>None</span>
            <span class=n>hidden_states</span><span class=p>,</span> <span class=n>cross_attn_weights</span><span class=p>,</span> <span class=n>cross_attn_present_key_value</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder_attn</span><span class=p>(</span>
                <span class=n>hidden_states</span><span class=o>=</span><span class=n>hidden_states</span><span class=p>,</span>
                <span class=n>key_value_states</span><span class=o>=</span><span class=n>encoder_hidden_states</span><span class=p>,</span>
                <span class=n>attention_mask</span><span class=o>=</span><span class=n>encoder_attention_mask</span><span class=p>,</span>
                <span class=n>layer_head_mask</span><span class=o>=</span><span class=n>cross_attn_layer_head_mask</span><span class=p>,</span>
                <span class=n>past_key_value</span><span class=o>=</span><span class=n>cross_attn_past_key_value</span><span class=p>,</span>
                <span class=n>output_attentions</span><span class=o>=</span><span class=n>output_attentions</span><span class=p>,</span>
            <span class=p>)</span>
            <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>,</span> <span class=n>training</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>training</span><span class=p>)</span>
            <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>residual</span> <span class=o>+</span> <span class=n>hidden_states</span>
            <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder_attn_layer_norm</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>

            <span class=c1># add cross-attn to positions 3,4 of present_key_value tuple</span>
            <span class=n>present_key_value</span> <span class=o>=</span> <span class=n>present_key_value</span> <span class=o>+</span> <span class=n>cross_attn_present_key_value</span>

        <span class=c1># Fully Connected</span>
        <span class=n>residual</span> <span class=o>=</span> <span class=n>hidden_states</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>activation_fn</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>))</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>activation_dropout</span><span class=p>,</span> <span class=n>training</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>training</span><span class=p>)</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>,</span> <span class=n>training</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>training</span><span class=p>)</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>residual</span> <span class=o>+</span> <span class=n>hidden_states</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>final_layer_norm</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>

        <span class=n>outputs</span> <span class=o>=</span> <span class=p>(</span><span class=n>hidden_states</span><span class=p>,)</span>

        <span class=k>if</span> <span class=n>output_attentions</span><span class=p>:</span>
            <span class=n>outputs</span> <span class=o>+=</span> <span class=p>(</span><span class=n>self_attn_weights</span><span class=p>,</span> <span class=n>cross_attn_weights</span><span class=p>)</span>

        <span class=k>if</span> <span class=n>use_cache</span><span class=p>:</span>
            <span class=n>outputs</span> <span class=o>+=</span> <span class=p>(</span><span class=n>present_key_value</span><span class=p>,)</span>

        <span class=k>return</span> <span class=n>outputs</span>


<span class=k>class</span> <span class=nc>BartClassificationHead</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>"""Head for sentence-level classification tasks."""</span>

    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span>
        <span class=n>input_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
        <span class=n>inner_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
        <span class=n>num_classes</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
        <span class=n>pooler_dropout</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span>
    <span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>dense</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>inner_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=n>pooler_dropout</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>out_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>inner_dim</span><span class=p>,</span> <span class=n>num_classes</span><span class=p>)</span>

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>hidden_states</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span> <span class=o>-></span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dense</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>out_proj</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>hidden_states</span>


<span class=k>class</span> <span class=nc>BartPreTrainedModel</span><span class=p>(</span><span class=n>PreTrainedModel</span><span class=p>):</span>
    <span class=n>config_class</span> <span class=o>=</span> <span class=n>BartConfig</span>
    <span class=n>base_model_prefix</span> <span class=o>=</span> <span class=s2>"model"</span>
    <span class=n>supports_gradient_checkpointing</span> <span class=o>=</span> <span class=kc>True</span>
    <span class=n>_keys_to_ignore_on_load_unexpected</span> <span class=o>=</span> <span class=p>[</span><span class=s2>"encoder.version"</span><span class=p>,</span> <span class=s2>"decoder.version"</span><span class=p>]</span>
    <span class=n>_no_split_modules</span> <span class=o>=</span> <span class=p>[</span><span class=sa>r</span><span class=s2>"BartEncoderLayer"</span><span class=p>,</span> <span class=sa>r</span><span class=s2>"BartDecoderLayer"</span><span class=p>]</span>
    <span class=n>_skip_keys_device_placement</span> <span class=o>=</span> <span class=s2>"past_key_values"</span>
    <span class=n>_supports_flash_attn_2</span> <span class=o>=</span> <span class=kc>True</span>
    <span class=n>_supports_sdpa</span> <span class=o>=</span> <span class=kc>True</span>

    <span class=k>def</span> <span class=nf>_init_weights</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>module</span><span class=p>):</span>
        <span class=n>std</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>init_std</span>
        <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>module</span><span class=p>,</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>):</span>
            <span class=n>module</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>normal_</span><span class=p>(</span><span class=n>mean</span><span class=o>=</span><span class=mf>0.0</span><span class=p>,</span> <span class=n>std</span><span class=o>=</span><span class=n>std</span><span class=p>)</span>
            <span class=k>if</span> <span class=n>module</span><span class=o>.</span><span class=n>bias</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
                <span class=n>module</span><span class=o>.</span><span class=n>bias</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>zero_</span><span class=p>()</span>
        <span class=k>elif</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>module</span><span class=p>,</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>):</span>
            <span class=n>module</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>normal_</span><span class=p>(</span><span class=n>mean</span><span class=o>=</span><span class=mf>0.0</span><span class=p>,</span> <span class=n>std</span><span class=o>=</span><span class=n>std</span><span class=p>)</span>
            <span class=k>if</span> <span class=n>module</span><span class=o>.</span><span class=n>padding_idx</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
                <span class=n>module</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>data</span><span class=p>[</span><span class=n>module</span><span class=o>.</span><span class=n>padding_idx</span><span class=p>]</span><span class=o>.</span><span class=n>zero_</span><span class=p>()</span>

    <span class=nd>@property</span>
    <span class=k>def</span> <span class=nf>dummy_inputs</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=n>pad_token</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>pad_token_id</span>
        <span class=n>input_ids</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>6</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>2</span><span class=p>],</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>12</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>pad_token</span><span class=p>]],</span> <span class=n>device</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
        <span class=n>dummy_inputs</span> <span class=o>=</span> <span class=p>{</span>
            <span class=s2>"attention_mask"</span><span class=p>:</span> <span class=n>input_ids</span><span class=o>.</span><span class=n>ne</span><span class=p>(</span><span class=n>pad_token</span><span class=p>),</span>
            <span class=s2>"input_ids"</span><span class=p>:</span> <span class=n>input_ids</span><span class=p>,</span>
        <span class=p>}</span>
        <span class=k>return</span> <span class=n>dummy_inputs</span>


<span class=k>class</span> <span class=nc>PretrainedBartModel</span><span class=p>(</span><span class=n>BartPreTrainedModel</span><span class=p>):</span>
    <span class=k>def</span> <span class=nf>__init_subclass__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=n>warnings</span><span class=o>.</span><span class=n>warn</span><span class=p>(</span>
            <span class=s2>"The class `PretrainedBartModel` has been depreciated, please use `BartPreTrainedModel` instead."</span><span class=p>,</span>
            <span class=ne>FutureWarning</span><span class=p>,</span>
        <span class=p>)</span>


<span class=k>class</span> <span class=nc>BartPretrainedModel</span><span class=p>(</span><span class=n>BartPreTrainedModel</span><span class=p>):</span>
    <span class=k>def</span> <span class=nf>__init_subclass__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=n>warnings</span><span class=o>.</span><span class=n>warn</span><span class=p>(</span>
            <span class=s2>"The class `PretrainedBartModel` has been depreciated, please use `BartPreTrainedModel` instead."</span><span class=p>,</span>
            <span class=ne>FutureWarning</span><span class=p>,</span>
        <span class=p>)</span>


<span class=n>BART_START_DOCSTRING</span> <span class=o>=</span> <span class=sa>r</span><span class=s2>"""</span>
<span class=s2>    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the</span>
<span class=s2>    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads</span>
<span class=s2>    etc.)</span>

<span class=s2>    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.</span>
<span class=s2>    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage</span>
<span class=s2>    and behavior.</span>

<span class=s2>    Parameters:</span>
<span class=s2>        config ([`BartConfig`]):</span>
<span class=s2>            Model configuration class with all the parameters of the model. Initializing with a config file does not</span>
<span class=s2>            load the weights associated with the model, only the configuration. Check out the</span>
<span class=s2>            [`~PreTrainedModel.from_pretrained`] method to load the model weights.</span>
<span class=s2>"""</span>

<span class=n>BART_GENERATION_EXAMPLE</span> <span class=o>=</span> <span class=sa>r</span><span class=s2>"""</span>
<span class=s2>    Summarization example:</span>

<span class=s2>    ```python</span>
<span class=s2>    >>> from transformers import AutoTokenizer, BartForConditionalGeneration</span>

<span class=s2>    >>> model = BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn")</span>
<span class=s2>    >>> tokenizer = AutoTokenizer.from_pretrained("facebook/bart-large-cnn")</span>

<span class=s2>    >>> ARTICLE_TO_SUMMARIZE = (</span>
<span class=s2>    ...     "PG&E stated it scheduled the blackouts in response to forecasts for high winds "</span>
<span class=s2>    ...     "amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were "</span>
<span class=s2>    ...     "scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow."</span>
<span class=s2>    ... )</span>
<span class=s2>    >>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors="pt")</span>

<span class=s2>    >>> # Generate Summary</span>
<span class=s2>    >>> summary_ids = model.generate(inputs["input_ids"], num_beams=2, min_length=0, max_length=20)</span>
<span class=s2>    >>> tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]</span>
<span class=s2>    'PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions'</span>
<span class=s2>    ```</span>

<span class=s2>    Mask filling example:</span>

<span class=s2>    ```python</span>
<span class=s2>    >>> from transformers import AutoTokenizer, BartForConditionalGeneration</span>

<span class=s2>    >>> tokenizer = AutoTokenizer.from_pretrained("facebook/bart-base")</span>
<span class=s2>    >>> model = BartForConditionalGeneration.from_pretrained("facebook/bart-base")</span>

<span class=s2>    >>> TXT = "My friends are &LTmask> but they eat too many carbs."</span>
<span class=s2>    >>> input_ids = tokenizer([TXT], return_tensors="pt")["input_ids"]</span>
<span class=s2>    >>> logits = model(input_ids).logits</span>

<span class=s2>    >>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()</span>
<span class=s2>    >>> probs = logits[0, masked_index].softmax(dim=0)</span>
<span class=s2>    >>> values, predictions = probs.topk(5)</span>

<span class=s2>    >>> tokenizer.decode(predictions).split()</span>
<span class=s2>    ['not', 'good', 'healthy', 'great', 'very']</span>
<span class=s2>    ```</span>
<span class=s2>"""</span>

<span class=n>BART_INPUTS_DOCSTRING</span> <span class=o>=</span> <span class=sa>r</span><span class=s2>"""</span>
<span class=s2>    Args:</span>
<span class=s2>        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):</span>
<span class=s2>            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide</span>
<span class=s2>            it.</span>

<span class=s2>            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and</span>
<span class=s2>            [`PreTrainedTokenizer.__call__`] for details.</span>

<span class=s2>            [What are input IDs?](../glossary#input-ids)</span>
<span class=s2>        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class=s2>            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:</span>

<span class=s2>            - 1 for tokens that are **not masked**,</span>
<span class=s2>            - 0 for tokens that are **masked**.</span>

<span class=s2>            [What are attention masks?](../glossary#attention-mask)</span>
<span class=s2>        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):</span>
<span class=s2>            Indices of decoder input sequence tokens in the vocabulary.</span>

<span class=s2>            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and</span>
<span class=s2>            [`PreTrainedTokenizer.__call__`] for details.</span>

<span class=s2>            [What are decoder input IDs?](../glossary#decoder-input-ids)</span>

<span class=s2>            Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`</span>
<span class=s2>            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).</span>

<span class=s2>            For translation and summarization training, `decoder_input_ids` should be provided. If no</span>
<span class=s2>            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right</span>
<span class=s2>            for denoising pre-training following the paper.</span>
<span class=s2>        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):</span>
<span class=s2>            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also</span>
<span class=s2>            be used by default.</span>

<span class=s2>            If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]</span>
<span class=s2>            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more</span>
<span class=s2>            information on the default strategy.</span>
<span class=s2>        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):</span>
<span class=s2>            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:</span>

<span class=s2>            - 1 indicates the head is **not masked**,</span>
<span class=s2>            - 0 indicates the head is **masked**.</span>

<span class=s2>        decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):</span>
<span class=s2>            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:</span>

<span class=s2>            - 1 indicates the head is **not masked**,</span>
<span class=s2>            - 0 indicates the head is **masked**.</span>

<span class=s2>        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):</span>
<span class=s2>            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,</span>
<span class=s2>            1]`:</span>

<span class=s2>            - 1 indicates the head is **not masked**,</span>
<span class=s2>            - 0 indicates the head is **masked**.</span>

<span class=s2>        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):</span>
<span class=s2>            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)</span>
<span class=s2>            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of</span>
<span class=s2>            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.</span>
<span class=s2>        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):</span>
<span class=s2>            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape</span>
<span class=s2>            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape</span>
<span class=s2>            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.</span>

<span class=s2>            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention</span>
<span class=s2>            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.</span>

<span class=s2>            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that</span>
<span class=s2>            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all</span>
<span class=s2>            `decoder_input_ids` of shape `(batch_size, sequence_length)`.</span>
<span class=s2>        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):</span>
<span class=s2>            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.</span>
<span class=s2>            This is useful if you want more control over how to convert `input_ids` indices into associated vectors</span>
<span class=s2>            than the model's internal embedding lookup matrix.</span>
<span class=s2>        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):</span>
<span class=s2>            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded</span>
<span class=s2>            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be</span>
<span class=s2>            input (see `past_key_values`). This is useful if you want more control over how to convert</span>
<span class=s2>            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.</span>

<span class=s2>            If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value</span>
<span class=s2>            of `inputs_embeds`.</span>
<span class=s2>        use_cache (`bool`, *optional*):</span>
<span class=s2>            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see</span>
<span class=s2>            `past_key_values`).</span>
<span class=s2>        output_attentions (`bool`, *optional*):</span>
<span class=s2>            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned</span>
<span class=s2>            tensors for more detail.</span>
<span class=s2>        output_hidden_states (`bool`, *optional*):</span>
<span class=s2>            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for</span>
<span class=s2>            more detail.</span>
<span class=s2>        return_dict (`bool`, *optional*):</span>
<span class=s2>            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.</span>
<span class=s2>"""</span>


<span class=k>class</span> <span class=nc>BartEncoder</span><span class=p>(</span><span class=n>BartPreTrainedModel</span><span class=p>):</span>
<span class=w>    </span><span class=sd>"""</span>
<span class=sd>    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a</span>
<span class=sd>    [`BartEncoderLayer`].</span>

<span class=sd>    Args:</span>
<span class=sd>        config: BartConfig</span>
<span class=sd>        embed_tokens (nn.Embedding): output embedding</span>
<span class=sd>    """</span>

    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=n>BartConfig</span><span class=p>,</span> <span class=n>embed_tokens</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>dropout</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>layerdrop</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>encoder_layerdrop</span>

        <span class=n>embed_dim</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>d_model</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>padding_idx</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>pad_token_id</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>max_source_positions</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>max_position_embeddings</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>embed_scale</span> <span class=o>=</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>)</span> <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>scale_embedding</span> <span class=k>else</span> <span class=mf>1.0</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>embed_tokens</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>padding_idx</span><span class=p>)</span>

        <span class=k>if</span> <span class=n>embed_tokens</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>embed_tokens</span><span class=o>.</span><span class=n>weight</span> <span class=o>=</span> <span class=n>embed_tokens</span><span class=o>.</span><span class=n>weight</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>embed_positions</span> <span class=o>=</span> <span class=n>BartLearnedPositionalEmbedding</span><span class=p>(</span>
            <span class=n>config</span><span class=o>.</span><span class=n>max_position_embeddings</span><span class=p>,</span>
            <span class=n>embed_dim</span><span class=p>,</span>
        <span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span><span class=n>BartEncoderLayer</span><span class=p>(</span><span class=n>config</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>encoder_layers</span><span class=p>)])</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>_use_flash_attention_2</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>_attn_implementation</span> <span class=o>==</span> <span class=s2>"flash_attention_2"</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>_use_sdpa</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>_attn_implementation</span> <span class=o>==</span> <span class=s2>"sdpa"</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>layernorm_embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>)</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>gradient_checkpointing</span> <span class=o>=</span> <span class=kc>False</span>
        <span class=c1># Initialize weights and apply final processing</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>post_init</span><span class=p>()</span>

    <span class=k>def</span> <span class=nf>get_input_embeddings</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>embed_tokens</span>

    <span class=k>def</span> <span class=nf>set_input_embeddings</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>value</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>embed_tokens</span> <span class=o>=</span> <span class=n>value</span>

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span>
        <span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>head_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>inputs_embeds</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>output_attentions</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>output_hidden_states</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>return_dict</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
    <span class=p>)</span> <span class=o>-></span> <span class=n>Union</span><span class=p>[</span><span class=n>Tuple</span><span class=p>,</span> <span class=n>BaseModelOutput</span><span class=p>]:</span>
<span class=w>        </span><span class=sa>r</span><span class=sd>"""</span>
<span class=sd>        Args:</span>
<span class=sd>            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):</span>
<span class=sd>                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you</span>
<span class=sd>                provide it.</span>

<span class=sd>                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and</span>
<span class=sd>                [`PreTrainedTokenizer.__call__`] for details.</span>

<span class=sd>                [What are input IDs?](../glossary#input-ids)</span>
<span class=sd>            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class=sd>                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:</span>

<span class=sd>                - 1 for tokens that are **not masked**,</span>
<span class=sd>                - 0 for tokens that are **masked**.</span>

<span class=sd>                [What are attention masks?](../glossary#attention-mask)</span>
<span class=sd>            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):</span>
<span class=sd>                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:</span>

<span class=sd>                - 1 indicates the head is **not masked**,</span>
<span class=sd>                - 0 indicates the head is **masked**.</span>

<span class=sd>            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):</span>
<span class=sd>                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.</span>
<span class=sd>                This is useful if you want more control over how to convert `input_ids` indices into associated vectors</span>
<span class=sd>                than the model's internal embedding lookup matrix.</span>
<span class=sd>            output_attentions (`bool`, *optional*):</span>
<span class=sd>                Whether or not to return the attentions tensors of all attention layers. See `attentions` under</span>
<span class=sd>                returned tensors for more detail.</span>
<span class=sd>            output_hidden_states (`bool`, *optional*):</span>
<span class=sd>                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors</span>
<span class=sd>                for more detail.</span>
<span class=sd>            return_dict (`bool`, *optional*):</span>
<span class=sd>                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.</span>
<span class=sd>        """</span>
        <span class=n>output_attentions</span> <span class=o>=</span> <span class=n>output_attentions</span> <span class=k>if</span> <span class=n>output_attentions</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>output_attentions</span>
        <span class=n>output_hidden_states</span> <span class=o>=</span> <span class=p>(</span>
            <span class=n>output_hidden_states</span> <span class=k>if</span> <span class=n>output_hidden_states</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>output_hidden_states</span>
        <span class=p>)</span>
        <span class=n>return_dict</span> <span class=o>=</span> <span class=n>return_dict</span> <span class=k>if</span> <span class=n>return_dict</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>use_return_dict</span>

        <span class=c1># retrieve input_ids and inputs_embeds</span>
        <span class=k>if</span> <span class=n>input_ids</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=ow>and</span> <span class=n>inputs_embeds</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=s2>"You cannot specify both input_ids and inputs_embeds at the same time"</span><span class=p>)</span>
        <span class=k>elif</span> <span class=n>input_ids</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=nb>input</span> <span class=o>=</span> <span class=n>input_ids</span>
            <span class=n>input_ids</span> <span class=o>=</span> <span class=n>input_ids</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>input_ids</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span>
        <span class=k>elif</span> <span class=n>inputs_embeds</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=nb>input</span> <span class=o>=</span> <span class=n>inputs_embeds</span><span class=p>[:,</span> <span class=p>:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=s2>"You have to specify either input_ids or inputs_embeds"</span><span class=p>)</span>

        <span class=k>if</span> <span class=n>inputs_embeds</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
            <span class=n>inputs_embeds</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embed_tokens</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>embed_scale</span>

        <span class=n>embed_pos</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embed_positions</span><span class=p>(</span><span class=nb>input</span><span class=p>)</span>
        <span class=n>embed_pos</span> <span class=o>=</span> <span class=n>embed_pos</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>inputs_embeds</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>

        <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>inputs_embeds</span> <span class=o>+</span> <span class=n>embed_pos</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layernorm_embedding</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>,</span> <span class=n>training</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>training</span><span class=p>)</span>

        <span class=c1># expand attention_mask</span>
        <span class=k>if</span> <span class=n>attention_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_use_flash_attention_2</span><span class=p>:</span>
                <span class=n>attention_mask</span> <span class=o>=</span> <span class=n>attention_mask</span> <span class=k>if</span> <span class=mi>0</span> <span class=ow>in</span> <span class=n>attention_mask</span> <span class=k>else</span> <span class=kc>None</span>
            <span class=k>elif</span> <span class=bp>self</span><span class=o>.</span><span class=n>_use_sdpa</span> <span class=ow>and</span> <span class=n>head_mask</span> <span class=ow>is</span> <span class=kc>None</span> <span class=ow>and</span> <span class=ow>not</span> <span class=n>output_attentions</span><span class=p>:</span>
                <span class=c1># output_attentions=True & head_mask can not be supported when using SDPA, fall back to</span>
                <span class=c1># the manual implementation that requires a 4D causal mask in all cases.</span>
                <span class=c1># [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]</span>
                <span class=n>attention_mask</span> <span class=o>=</span> <span class=n>_prepare_4d_attention_mask_for_sdpa</span><span class=p>(</span><span class=n>attention_mask</span><span class=p>,</span> <span class=n>inputs_embeds</span><span class=o>.</span><span class=n>dtype</span><span class=p>)</span>
            <span class=k>else</span><span class=p>:</span>
                <span class=c1># [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]</span>
                <span class=n>attention_mask</span> <span class=o>=</span> <span class=n>_prepare_4d_attention_mask</span><span class=p>(</span><span class=n>attention_mask</span><span class=p>,</span> <span class=n>inputs_embeds</span><span class=o>.</span><span class=n>dtype</span><span class=p>)</span>

        <span class=n>encoder_states</span> <span class=o>=</span> <span class=p>()</span> <span class=k>if</span> <span class=n>output_hidden_states</span> <span class=k>else</span> <span class=kc>None</span>
        <span class=n>all_attentions</span> <span class=o>=</span> <span class=p>()</span> <span class=k>if</span> <span class=n>output_attentions</span> <span class=k>else</span> <span class=kc>None</span>

        <span class=c1># check if head_mask has a correct number of layers specified if desired</span>
        <span class=k>if</span> <span class=n>head_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=k>if</span> <span class=n>head_mask</span><span class=o>.</span><span class=n>size</span><span class=p>()[</span><span class=mi>0</span><span class=p>]</span> <span class=o>!=</span> <span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>)):</span>
                <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span>
                    <span class=sa>f</span><span class=s2>"The head_mask should be specified for </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>)</span><span class=si>}</span><span class=s2> layers, but it is for"</span>
                    <span class=sa>f</span><span class=s2>" </span><span class=si>{</span><span class=n>head_mask</span><span class=o>.</span><span class=n>size</span><span class=p>()[</span><span class=mi>0</span><span class=p>]</span><span class=si>}</span><span class=s2>."</span>
                <span class=p>)</span>

        <span class=k>for</span> <span class=n>idx</span><span class=p>,</span> <span class=n>encoder_layer</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>):</span>
            <span class=k>if</span> <span class=n>output_hidden_states</span><span class=p>:</span>
                <span class=n>encoder_states</span> <span class=o>=</span> <span class=n>encoder_states</span> <span class=o>+</span> <span class=p>(</span><span class=n>hidden_states</span><span class=p>,)</span>
            <span class=c1># add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)</span>
            <span class=n>to_drop</span> <span class=o>=</span> <span class=kc>False</span>
            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>training</span><span class=p>:</span>
                <span class=n>dropout_probability</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>([])</span>
                <span class=k>if</span> <span class=n>dropout_probability</span> <span class=o><</span> <span class=bp>self</span><span class=o>.</span><span class=n>layerdrop</span><span class=p>:</span>  <span class=c1># skip the layer</span>
                    <span class=n>to_drop</span> <span class=o>=</span> <span class=kc>True</span>

            <span class=k>if</span> <span class=n>to_drop</span><span class=p>:</span>
                <span class=n>layer_outputs</span> <span class=o>=</span> <span class=p>(</span><span class=kc>None</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span>
            <span class=k>else</span><span class=p>:</span>
                <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>gradient_checkpointing</span> <span class=ow>and</span> <span class=bp>self</span><span class=o>.</span><span class=n>training</span><span class=p>:</span>
                    <span class=n>layer_outputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_gradient_checkpointing_func</span><span class=p>(</span>
                        <span class=n>encoder_layer</span><span class=o>.</span><span class=fm>__call__</span><span class=p>,</span>
                        <span class=n>hidden_states</span><span class=p>,</span>
                        <span class=n>attention_mask</span><span class=p>,</span>
                        <span class=p>(</span><span class=n>head_mask</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=k>if</span> <span class=n>head_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=kc>None</span><span class=p>),</span>
                        <span class=n>output_attentions</span><span class=p>,</span>
                    <span class=p>)</span>
                <span class=k>else</span><span class=p>:</span>
                    <span class=n>layer_outputs</span> <span class=o>=</span> <span class=n>encoder_layer</span><span class=p>(</span>
                        <span class=n>hidden_states</span><span class=p>,</span>
                        <span class=n>attention_mask</span><span class=p>,</span>
                        <span class=n>layer_head_mask</span><span class=o>=</span><span class=p>(</span><span class=n>head_mask</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=k>if</span> <span class=n>head_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=kc>None</span><span class=p>),</span>
                        <span class=n>output_attentions</span><span class=o>=</span><span class=n>output_attentions</span><span class=p>,</span>
                    <span class=p>)</span>

                <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>layer_outputs</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>

            <span class=k>if</span> <span class=n>output_attentions</span><span class=p>:</span>
                <span class=n>all_attentions</span> <span class=o>=</span> <span class=n>all_attentions</span> <span class=o>+</span> <span class=p>(</span><span class=n>layer_outputs</span><span class=p>[</span><span class=mi>1</span><span class=p>],)</span>

        <span class=k>if</span> <span class=n>output_hidden_states</span><span class=p>:</span>
            <span class=n>encoder_states</span> <span class=o>=</span> <span class=n>encoder_states</span> <span class=o>+</span> <span class=p>(</span><span class=n>hidden_states</span><span class=p>,)</span>

        <span class=k>if</span> <span class=ow>not</span> <span class=n>return_dict</span><span class=p>:</span>
            <span class=k>return</span> <span class=nb>tuple</span><span class=p>(</span><span class=n>v</span> <span class=k>for</span> <span class=n>v</span> <span class=ow>in</span> <span class=p>[</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>encoder_states</span><span class=p>,</span> <span class=n>all_attentions</span><span class=p>]</span> <span class=k>if</span> <span class=n>v</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>BaseModelOutput</span><span class=p>(</span>
            <span class=n>last_hidden_state</span><span class=o>=</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>hidden_states</span><span class=o>=</span><span class=n>encoder_states</span><span class=p>,</span> <span class=n>attentions</span><span class=o>=</span><span class=n>all_attentions</span>
        <span class=p>)</span>


<span class=k>class</span> <span class=nc>BartDecoder</span><span class=p>(</span><span class=n>BartPreTrainedModel</span><span class=p>):</span>
<span class=w>    </span><span class=sd>"""</span>
<span class=sd>    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a [`BartDecoderLayer`]</span>

<span class=sd>    Args:</span>
<span class=sd>        config: BartConfig</span>
<span class=sd>        embed_tokens (nn.Embedding): output embedding</span>
<span class=sd>    """</span>

    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=n>BartConfig</span><span class=p>,</span> <span class=n>embed_tokens</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>dropout</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>layerdrop</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>decoder_layerdrop</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>padding_idx</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>pad_token_id</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>max_target_positions</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>max_position_embeddings</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>embed_scale</span> <span class=o>=</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>d_model</span><span class=p>)</span> <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>scale_embedding</span> <span class=k>else</span> <span class=mf>1.0</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>embed_tokens</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>d_model</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>padding_idx</span><span class=p>)</span>

        <span class=k>if</span> <span class=n>embed_tokens</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>embed_tokens</span><span class=o>.</span><span class=n>weight</span> <span class=o>=</span> <span class=n>embed_tokens</span><span class=o>.</span><span class=n>weight</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>embed_positions</span> <span class=o>=</span> <span class=n>BartLearnedPositionalEmbedding</span><span class=p>(</span>
            <span class=n>config</span><span class=o>.</span><span class=n>max_position_embeddings</span><span class=p>,</span>
            <span class=n>config</span><span class=o>.</span><span class=n>d_model</span><span class=p>,</span>
        <span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span><span class=n>BartDecoderLayer</span><span class=p>(</span><span class=n>config</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>decoder_layers</span><span class=p>)])</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>_use_flash_attention_2</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>_attn_implementation</span> <span class=o>==</span> <span class=s2>"flash_attention_2"</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>_use_sdpa</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>_attn_implementation</span> <span class=o>==</span> <span class=s2>"sdpa"</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>layernorm_embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>d_model</span><span class=p>)</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>gradient_checkpointing</span> <span class=o>=</span> <span class=kc>False</span>
        <span class=c1># Initialize weights and apply final processing</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>post_init</span><span class=p>()</span>

    <span class=k>def</span> <span class=nf>get_input_embeddings</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>embed_tokens</span>

    <span class=k>def</span> <span class=nf>set_input_embeddings</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>value</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>embed_tokens</span> <span class=o>=</span> <span class=n>value</span>

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span>
        <span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>encoder_hidden_states</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>encoder_attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>head_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>cross_attn_head_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>past_key_values</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>List</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>inputs_embeds</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>use_cache</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>output_attentions</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>output_hidden_states</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>return_dict</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
    <span class=p>)</span> <span class=o>-></span> <span class=n>Union</span><span class=p>[</span><span class=n>Tuple</span><span class=p>,</span> <span class=n>BaseModelOutputWithPastAndCrossAttentions</span><span class=p>]:</span>
<span class=w>        </span><span class=sa>r</span><span class=sd>"""</span>
<span class=sd>        Args:</span>
<span class=sd>            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):</span>
<span class=sd>                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you</span>
<span class=sd>                provide it.</span>

<span class=sd>                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and</span>
<span class=sd>                [`PreTrainedTokenizer.__call__`] for details.</span>

<span class=sd>                [What are input IDs?](../glossary#input-ids)</span>
<span class=sd>            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class=sd>                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:</span>

<span class=sd>                - 1 for tokens that are **not masked**,</span>
<span class=sd>                - 0 for tokens that are **masked**.</span>

<span class=sd>                [What are attention masks?](../glossary#attention-mask)</span>
<span class=sd>            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):</span>
<span class=sd>                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention</span>
<span class=sd>                of the decoder.</span>
<span class=sd>            encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):</span>
<span class=sd>                Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values</span>
<span class=sd>                selected in `[0, 1]`:</span>

<span class=sd>                - 1 for tokens that are **not masked**,</span>
<span class=sd>                - 0 for tokens that are **masked**.</span>

<span class=sd>                [What are attention masks?](../glossary#attention-mask)</span>
<span class=sd>            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):</span>
<span class=sd>                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:</span>

<span class=sd>                - 1 indicates the head is **not masked**,</span>
<span class=sd>                - 0 indicates the head is **masked**.</span>

<span class=sd>            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):</span>
<span class=sd>                Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performing</span>
<span class=sd>                cross-attention on hidden heads. Mask values selected in `[0, 1]`:</span>

<span class=sd>                - 1 indicates the head is **not masked**,</span>
<span class=sd>                - 0 indicates the head is **masked**.</span>

<span class=sd>            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):</span>
<span class=sd>                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of</span>
<span class=sd>                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of</span>
<span class=sd>                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.</span>

<span class=sd>                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the</span>
<span class=sd>                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.</span>

<span class=sd>                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those</span>
<span class=sd>                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of</span>
<span class=sd>                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.</span>
<span class=sd>            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):</span>
<span class=sd>                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.</span>
<span class=sd>                This is useful if you want more control over how to convert `input_ids` indices into associated vectors</span>
<span class=sd>                than the model's internal embedding lookup matrix.</span>
<span class=sd>            output_attentions (`bool`, *optional*):</span>
<span class=sd>                Whether or not to return the attentions tensors of all attention layers. See `attentions` under</span>
<span class=sd>                returned tensors for more detail.</span>
<span class=sd>            output_hidden_states (`bool`, *optional*):</span>
<span class=sd>                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors</span>
<span class=sd>                for more detail.</span>
<span class=sd>            return_dict (`bool`, *optional*):</span>
<span class=sd>                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.</span>
<span class=sd>        """</span>
        <span class=n>output_attentions</span> <span class=o>=</span> <span class=n>output_attentions</span> <span class=k>if</span> <span class=n>output_attentions</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>output_attentions</span>
        <span class=n>output_hidden_states</span> <span class=o>=</span> <span class=p>(</span>
            <span class=n>output_hidden_states</span> <span class=k>if</span> <span class=n>output_hidden_states</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>output_hidden_states</span>
        <span class=p>)</span>
        <span class=n>use_cache</span> <span class=o>=</span> <span class=n>use_cache</span> <span class=k>if</span> <span class=n>use_cache</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>use_cache</span>
        <span class=n>return_dict</span> <span class=o>=</span> <span class=n>return_dict</span> <span class=k>if</span> <span class=n>return_dict</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>use_return_dict</span>

        <span class=c1># retrieve input_ids and inputs_embeds</span>
        <span class=k>if</span> <span class=n>input_ids</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=ow>and</span> <span class=n>inputs_embeds</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=s2>"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time"</span><span class=p>)</span>
        <span class=k>elif</span> <span class=n>input_ids</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=nb>input</span> <span class=o>=</span> <span class=n>input_ids</span>
            <span class=n>input_shape</span> <span class=o>=</span> <span class=nb>input</span><span class=o>.</span><span class=n>shape</span>
            <span class=n>input_ids</span> <span class=o>=</span> <span class=n>input_ids</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>input_shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span>
        <span class=k>elif</span> <span class=n>inputs_embeds</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=n>input_shape</span> <span class=o>=</span> <span class=n>inputs_embeds</span><span class=o>.</span><span class=n>size</span><span class=p>()[:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
            <span class=nb>input</span> <span class=o>=</span> <span class=n>inputs_embeds</span><span class=p>[:,</span> <span class=p>:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=s2>"You have to specify either decoder_input_ids or decoder_inputs_embeds"</span><span class=p>)</span>

        <span class=c1># past_key_values_length</span>
        <span class=n>past_key_values_length</span> <span class=o>=</span> <span class=n>past_key_values</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span> <span class=k>if</span> <span class=n>past_key_values</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=mi>0</span>

        <span class=k>if</span> <span class=n>inputs_embeds</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
            <span class=n>inputs_embeds</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embed_tokens</span><span class=p>(</span><span class=nb>input</span><span class=p>)</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>embed_scale</span>

        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_use_flash_attention_2</span><span class=p>:</span>
            <span class=c1># 2d mask is passed through the layers</span>
            <span class=n>attention_mask</span> <span class=o>=</span> <span class=n>attention_mask</span> <span class=k>if</span> <span class=p>(</span><span class=n>attention_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=ow>and</span> <span class=mi>0</span> <span class=ow>in</span> <span class=n>attention_mask</span><span class=p>)</span> <span class=k>else</span> <span class=kc>None</span>
        <span class=k>elif</span> <span class=bp>self</span><span class=o>.</span><span class=n>_use_sdpa</span> <span class=ow>and</span> <span class=ow>not</span> <span class=n>output_attentions</span> <span class=ow>and</span> <span class=n>cross_attn_head_mask</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
            <span class=c1># output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on</span>
            <span class=c1># the manual implementation that requires a 4D causal mask in all cases.</span>
            <span class=n>attention_mask</span> <span class=o>=</span> <span class=n>_prepare_4d_causal_attention_mask_for_sdpa</span><span class=p>(</span>
                <span class=n>attention_mask</span><span class=p>,</span>
                <span class=n>input_shape</span><span class=p>,</span>
                <span class=n>inputs_embeds</span><span class=p>,</span>
                <span class=n>past_key_values_length</span><span class=p>,</span>
            <span class=p>)</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=c1># 4d mask is passed through the layers</span>
            <span class=n>attention_mask</span> <span class=o>=</span> <span class=n>_prepare_4d_causal_attention_mask</span><span class=p>(</span>
                <span class=n>attention_mask</span><span class=p>,</span> <span class=n>input_shape</span><span class=p>,</span> <span class=n>inputs_embeds</span><span class=p>,</span> <span class=n>past_key_values_length</span>
            <span class=p>)</span>

        <span class=c1># expand encoder attention mask</span>
        <span class=k>if</span> <span class=n>encoder_hidden_states</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=ow>and</span> <span class=n>encoder_attention_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_use_flash_attention_2</span><span class=p>:</span>
                <span class=n>encoder_attention_mask</span> <span class=o>=</span> <span class=n>encoder_attention_mask</span> <span class=k>if</span> <span class=mi>0</span> <span class=ow>in</span> <span class=n>encoder_attention_mask</span> <span class=k>else</span> <span class=kc>None</span>
            <span class=k>elif</span> <span class=bp>self</span><span class=o>.</span><span class=n>_use_sdpa</span> <span class=ow>and</span> <span class=n>cross_attn_head_mask</span> <span class=ow>is</span> <span class=kc>None</span> <span class=ow>and</span> <span class=ow>not</span> <span class=n>output_attentions</span><span class=p>:</span>
                <span class=c1># output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on</span>
                <span class=c1># the manual implementation that requires a 4D causal mask in all cases.</span>
                <span class=c1># [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]</span>
                <span class=n>encoder_attention_mask</span> <span class=o>=</span> <span class=n>_prepare_4d_attention_mask_for_sdpa</span><span class=p>(</span>
                    <span class=n>encoder_attention_mask</span><span class=p>,</span>
                    <span class=n>inputs_embeds</span><span class=o>.</span><span class=n>dtype</span><span class=p>,</span>
                    <span class=n>tgt_len</span><span class=o>=</span><span class=n>input_shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span>
                <span class=p>)</span>
            <span class=k>else</span><span class=p>:</span>
                <span class=c1># [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]</span>
                <span class=n>encoder_attention_mask</span> <span class=o>=</span> <span class=n>_prepare_4d_attention_mask</span><span class=p>(</span>
                    <span class=n>encoder_attention_mask</span><span class=p>,</span> <span class=n>inputs_embeds</span><span class=o>.</span><span class=n>dtype</span><span class=p>,</span> <span class=n>tgt_len</span><span class=o>=</span><span class=n>input_shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
                <span class=p>)</span>

        <span class=c1># embed positions</span>
        <span class=n>positions</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embed_positions</span><span class=p>(</span><span class=nb>input</span><span class=p>,</span> <span class=n>past_key_values_length</span><span class=p>)</span>
        <span class=n>positions</span> <span class=o>=</span> <span class=n>positions</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>inputs_embeds</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>

        <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>inputs_embeds</span> <span class=o>+</span> <span class=n>positions</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layernorm_embedding</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>

        <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>,</span> <span class=n>training</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>training</span><span class=p>)</span>

        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>gradient_checkpointing</span> <span class=ow>and</span> <span class=bp>self</span><span class=o>.</span><span class=n>training</span><span class=p>:</span>
            <span class=k>if</span> <span class=n>use_cache</span><span class=p>:</span>
                <span class=n>logger</span><span class=o>.</span><span class=n>warning_once</span><span class=p>(</span>
                    <span class=s2>"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."</span>
                <span class=p>)</span>
                <span class=n>use_cache</span> <span class=o>=</span> <span class=kc>False</span>

        <span class=c1># decoder layers</span>
        <span class=n>all_hidden_states</span> <span class=o>=</span> <span class=p>()</span> <span class=k>if</span> <span class=n>output_hidden_states</span> <span class=k>else</span> <span class=kc>None</span>
        <span class=n>all_self_attns</span> <span class=o>=</span> <span class=p>()</span> <span class=k>if</span> <span class=n>output_attentions</span> <span class=k>else</span> <span class=kc>None</span>
        <span class=n>all_cross_attentions</span> <span class=o>=</span> <span class=p>()</span> <span class=k>if</span> <span class=p>(</span><span class=n>output_attentions</span> <span class=ow>and</span> <span class=n>encoder_hidden_states</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>)</span> <span class=k>else</span> <span class=kc>None</span>
        <span class=n>next_decoder_cache</span> <span class=o>=</span> <span class=p>()</span> <span class=k>if</span> <span class=n>use_cache</span> <span class=k>else</span> <span class=kc>None</span>

        <span class=c1># check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired</span>
        <span class=k>for</span> <span class=n>attn_mask</span><span class=p>,</span> <span class=n>mask_name</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>([</span><span class=n>head_mask</span><span class=p>,</span> <span class=n>cross_attn_head_mask</span><span class=p>],</span> <span class=p>[</span><span class=s2>"head_mask"</span><span class=p>,</span> <span class=s2>"cross_attn_head_mask"</span><span class=p>]):</span>
            <span class=k>if</span> <span class=n>attn_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
                <span class=k>if</span> <span class=n>attn_mask</span><span class=o>.</span><span class=n>size</span><span class=p>()[</span><span class=mi>0</span><span class=p>]</span> <span class=o>!=</span> <span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>)):</span>
                    <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span>
                        <span class=sa>f</span><span class=s2>"The `</span><span class=si>{</span><span class=n>mask_name</span><span class=si>}</span><span class=s2>` should be specified for </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>)</span><span class=si>}</span><span class=s2> layers, but it is for"</span>
                        <span class=sa>f</span><span class=s2>" </span><span class=si>{</span><span class=n>head_mask</span><span class=o>.</span><span class=n>size</span><span class=p>()[</span><span class=mi>0</span><span class=p>]</span><span class=si>}</span><span class=s2>."</span>
                    <span class=p>)</span>

        <span class=k>for</span> <span class=n>idx</span><span class=p>,</span> <span class=n>decoder_layer</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>):</span>
            <span class=c1># add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)</span>
            <span class=k>if</span> <span class=n>output_hidden_states</span><span class=p>:</span>
                <span class=n>all_hidden_states</span> <span class=o>+=</span> <span class=p>(</span><span class=n>hidden_states</span><span class=p>,)</span>
            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>training</span><span class=p>:</span>
                <span class=n>dropout_probability</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>([])</span>
                <span class=k>if</span> <span class=n>dropout_probability</span> <span class=o><</span> <span class=bp>self</span><span class=o>.</span><span class=n>layerdrop</span><span class=p>:</span>
                    <span class=k>continue</span>

            <span class=n>past_key_value</span> <span class=o>=</span> <span class=n>past_key_values</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=k>if</span> <span class=n>past_key_values</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=kc>None</span>

            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>gradient_checkpointing</span> <span class=ow>and</span> <span class=bp>self</span><span class=o>.</span><span class=n>training</span><span class=p>:</span>
                <span class=n>layer_outputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_gradient_checkpointing_func</span><span class=p>(</span>
                    <span class=n>decoder_layer</span><span class=o>.</span><span class=fm>__call__</span><span class=p>,</span>
                    <span class=n>hidden_states</span><span class=p>,</span>
                    <span class=n>attention_mask</span><span class=p>,</span>
                    <span class=n>encoder_hidden_states</span><span class=p>,</span>
                    <span class=n>encoder_attention_mask</span><span class=p>,</span>
                    <span class=n>head_mask</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=k>if</span> <span class=n>head_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=kc>None</span><span class=p>,</span>
                    <span class=n>cross_attn_head_mask</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=k>if</span> <span class=n>cross_attn_head_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=kc>None</span><span class=p>,</span>
                    <span class=kc>None</span><span class=p>,</span>
                    <span class=n>output_attentions</span><span class=p>,</span>
                    <span class=n>use_cache</span><span class=p>,</span>
                <span class=p>)</span>
            <span class=k>else</span><span class=p>:</span>
                <span class=n>layer_outputs</span> <span class=o>=</span> <span class=n>decoder_layer</span><span class=p>(</span>
                    <span class=n>hidden_states</span><span class=p>,</span>
                    <span class=n>attention_mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=p>,</span>
                    <span class=n>encoder_hidden_states</span><span class=o>=</span><span class=n>encoder_hidden_states</span><span class=p>,</span>
                    <span class=n>encoder_attention_mask</span><span class=o>=</span><span class=n>encoder_attention_mask</span><span class=p>,</span>
                    <span class=n>layer_head_mask</span><span class=o>=</span><span class=p>(</span><span class=n>head_mask</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=k>if</span> <span class=n>head_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=kc>None</span><span class=p>),</span>
                    <span class=n>cross_attn_layer_head_mask</span><span class=o>=</span><span class=p>(</span>
                        <span class=n>cross_attn_head_mask</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=k>if</span> <span class=n>cross_attn_head_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=kc>None</span>
                    <span class=p>),</span>
                    <span class=n>past_key_value</span><span class=o>=</span><span class=n>past_key_value</span><span class=p>,</span>
                    <span class=n>output_attentions</span><span class=o>=</span><span class=n>output_attentions</span><span class=p>,</span>
                    <span class=n>use_cache</span><span class=o>=</span><span class=n>use_cache</span><span class=p>,</span>
                <span class=p>)</span>
            <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>layer_outputs</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>

            <span class=k>if</span> <span class=n>use_cache</span><span class=p>:</span>
                <span class=n>next_decoder_cache</span> <span class=o>+=</span> <span class=p>(</span><span class=n>layer_outputs</span><span class=p>[</span><span class=mi>3</span> <span class=k>if</span> <span class=n>output_attentions</span> <span class=k>else</span> <span class=mi>1</span><span class=p>],)</span>

            <span class=k>if</span> <span class=n>output_attentions</span><span class=p>:</span>
                <span class=n>all_self_attns</span> <span class=o>+=</span> <span class=p>(</span><span class=n>layer_outputs</span><span class=p>[</span><span class=mi>1</span><span class=p>],)</span>

                <span class=k>if</span> <span class=n>encoder_hidden_states</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
                    <span class=n>all_cross_attentions</span> <span class=o>+=</span> <span class=p>(</span><span class=n>layer_outputs</span><span class=p>[</span><span class=mi>2</span><span class=p>],)</span>

        <span class=c1># add hidden states from the last decoder layer</span>
        <span class=k>if</span> <span class=n>output_hidden_states</span><span class=p>:</span>
            <span class=n>all_hidden_states</span> <span class=o>+=</span> <span class=p>(</span><span class=n>hidden_states</span><span class=p>,)</span>

        <span class=n>next_cache</span> <span class=o>=</span> <span class=n>next_decoder_cache</span> <span class=k>if</span> <span class=n>use_cache</span> <span class=k>else</span> <span class=kc>None</span>
        <span class=k>if</span> <span class=ow>not</span> <span class=n>return_dict</span><span class=p>:</span>
            <span class=k>return</span> <span class=nb>tuple</span><span class=p>(</span>
                <span class=n>v</span>
                <span class=k>for</span> <span class=n>v</span> <span class=ow>in</span> <span class=p>[</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>next_cache</span><span class=p>,</span> <span class=n>all_hidden_states</span><span class=p>,</span> <span class=n>all_self_attns</span><span class=p>,</span> <span class=n>all_cross_attentions</span><span class=p>]</span>
                <span class=k>if</span> <span class=n>v</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span>
            <span class=p>)</span>
        <span class=k>return</span> <span class=n>BaseModelOutputWithPastAndCrossAttentions</span><span class=p>(</span>
            <span class=n>last_hidden_state</span><span class=o>=</span><span class=n>hidden_states</span><span class=p>,</span>
            <span class=n>past_key_values</span><span class=o>=</span><span class=n>next_cache</span><span class=p>,</span>
            <span class=n>hidden_states</span><span class=o>=</span><span class=n>all_hidden_states</span><span class=p>,</span>
            <span class=n>attentions</span><span class=o>=</span><span class=n>all_self_attns</span><span class=p>,</span>
            <span class=n>cross_attentions</span><span class=o>=</span><span class=n>all_cross_attentions</span><span class=p>,</span>
        <span class=p>)</span>


<span class=nd>@add_start_docstrings</span><span class=p>(</span>
    <span class=s2>"The bare BART Model outputting raw hidden-states without any specific head on top."</span><span class=p>,</span>
    <span class=n>BART_START_DOCSTRING</span><span class=p>,</span>
<span class=p>)</span>
<span class=k>class</span> <span class=nc>BartModel</span><span class=p>(</span><span class=n>BartPreTrainedModel</span><span class=p>):</span>
    <span class=n>_tied_weights_keys</span> <span class=o>=</span> <span class=p>[</span><span class=s2>"encoder.embed_tokens.weight"</span><span class=p>,</span> <span class=s2>"decoder.embed_tokens.weight"</span><span class=p>]</span>

    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=n>BartConfig</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>

        <span class=n>padding_idx</span><span class=p>,</span> <span class=n>vocab_size</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>pad_token_id</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>vocab_size</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>shared</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>d_model</span><span class=p>,</span> <span class=n>padding_idx</span><span class=p>)</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span> <span class=o>=</span> <span class=n>BartEncoder</span><span class=p>(</span><span class=n>config</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>shared</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span> <span class=o>=</span> <span class=n>BartDecoder</span><span class=p>(</span><span class=n>config</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>shared</span><span class=p>)</span>

        <span class=c1># Initialize weights and apply final processing</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>post_init</span><span class=p>()</span>

    <span class=k>def</span> <span class=nf>_tie_weights</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>tie_word_embeddings</span><span class=p>:</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>_tie_or_clone_weights</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=o>.</span><span class=n>embed_tokens</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>shared</span><span class=p>)</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>_tie_or_clone_weights</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>decoder</span><span class=o>.</span><span class=n>embed_tokens</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>shared</span><span class=p>)</span>

    <span class=k>def</span> <span class=nf>get_input_embeddings</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>shared</span>

    <span class=k>def</span> <span class=nf>set_input_embeddings</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>value</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>shared</span> <span class=o>=</span> <span class=n>value</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=o>.</span><span class=n>embed_tokens</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>shared</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span><span class=o>.</span><span class=n>embed_tokens</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>shared</span>

    <span class=k>def</span> <span class=nf>get_encoder</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span>

    <span class=k>def</span> <span class=nf>get_decoder</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span>

    <span class=nd>@add_start_docstrings_to_model_forward</span><span class=p>(</span><span class=n>BART_INPUTS_DOCSTRING</span><span class=p>)</span>
    <span class=nd>@add_code_sample_docstrings</span><span class=p>(</span>
        <span class=n>checkpoint</span><span class=o>=</span><span class=n>_CHECKPOINT_FOR_DOC</span><span class=p>,</span>
        <span class=n>output_type</span><span class=o>=</span><span class=n>Seq2SeqModelOutput</span><span class=p>,</span>
        <span class=n>config_class</span><span class=o>=</span><span class=n>_CONFIG_FOR_DOC</span><span class=p>,</span>
        <span class=n>expected_output</span><span class=o>=</span><span class=n>_EXPECTED_OUTPUT_SHAPE</span><span class=p>,</span>
    <span class=p>)</span>
    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span>
        <span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>decoder_input_ids</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>decoder_attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>head_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>decoder_head_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>cross_attn_head_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>encoder_outputs</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>List</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>past_key_values</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>List</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>inputs_embeds</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>decoder_inputs_embeds</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>use_cache</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>output_attentions</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>output_hidden_states</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>return_dict</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
    <span class=p>)</span> <span class=o>-></span> <span class=n>Union</span><span class=p>[</span><span class=n>Tuple</span><span class=p>,</span> <span class=n>Seq2SeqModelOutput</span><span class=p>]:</span>
        <span class=c1># different to other models, Bart automatically creates decoder_input_ids from</span>
        <span class=c1># input_ids if no decoder_input_ids are provided</span>
        <span class=k>if</span> <span class=n>decoder_input_ids</span> <span class=ow>is</span> <span class=kc>None</span> <span class=ow>and</span> <span class=n>decoder_inputs_embeds</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
            <span class=k>if</span> <span class=n>input_ids</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
                <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span>
                    <span class=s2>"If no `decoder_input_ids` or `decoder_inputs_embeds` are "</span>
                    <span class=s2>"passed, `input_ids` cannot be `None`. Please pass either "</span>
                    <span class=s2>"`input_ids` or `decoder_input_ids` or `decoder_inputs_embeds`."</span>
                <span class=p>)</span>

            <span class=n>decoder_input_ids</span> <span class=o>=</span> <span class=n>shift_tokens_right</span><span class=p>(</span>
                <span class=n>input_ids</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>pad_token_id</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>decoder_start_token_id</span>
            <span class=p>)</span>

        <span class=n>output_attentions</span> <span class=o>=</span> <span class=n>output_attentions</span> <span class=k>if</span> <span class=n>output_attentions</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>output_attentions</span>
        <span class=n>output_hidden_states</span> <span class=o>=</span> <span class=p>(</span>
            <span class=n>output_hidden_states</span> <span class=k>if</span> <span class=n>output_hidden_states</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>output_hidden_states</span>
        <span class=p>)</span>
        <span class=n>use_cache</span> <span class=o>=</span> <span class=n>use_cache</span> <span class=k>if</span> <span class=n>use_cache</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>use_cache</span>
        <span class=n>return_dict</span> <span class=o>=</span> <span class=n>return_dict</span> <span class=k>if</span> <span class=n>return_dict</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>use_return_dict</span>

        <span class=k>if</span> <span class=n>encoder_outputs</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
            <span class=n>encoder_outputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=p>(</span>
                <span class=n>input_ids</span><span class=o>=</span><span class=n>input_ids</span><span class=p>,</span>
                <span class=n>attention_mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=p>,</span>
                <span class=n>head_mask</span><span class=o>=</span><span class=n>head_mask</span><span class=p>,</span>
                <span class=n>inputs_embeds</span><span class=o>=</span><span class=n>inputs_embeds</span><span class=p>,</span>
                <span class=n>output_attentions</span><span class=o>=</span><span class=n>output_attentions</span><span class=p>,</span>
                <span class=n>output_hidden_states</span><span class=o>=</span><span class=n>output_hidden_states</span><span class=p>,</span>
                <span class=n>return_dict</span><span class=o>=</span><span class=n>return_dict</span><span class=p>,</span>
            <span class=p>)</span>
        <span class=c1># If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True</span>
        <span class=k>elif</span> <span class=n>return_dict</span> <span class=ow>and</span> <span class=ow>not</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>encoder_outputs</span><span class=p>,</span> <span class=n>BaseModelOutput</span><span class=p>):</span>
            <span class=n>encoder_outputs</span> <span class=o>=</span> <span class=n>BaseModelOutput</span><span class=p>(</span>
                <span class=n>last_hidden_state</span><span class=o>=</span><span class=n>encoder_outputs</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span>
                <span class=n>hidden_states</span><span class=o>=</span><span class=n>encoder_outputs</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>encoder_outputs</span><span class=p>)</span> <span class=o>></span> <span class=mi>1</span> <span class=k>else</span> <span class=kc>None</span><span class=p>,</span>
                <span class=n>attentions</span><span class=o>=</span><span class=n>encoder_outputs</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span> <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>encoder_outputs</span><span class=p>)</span> <span class=o>></span> <span class=mi>2</span> <span class=k>else</span> <span class=kc>None</span><span class=p>,</span>
            <span class=p>)</span>

        <span class=c1># decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)</span>
        <span class=n>decoder_outputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span><span class=p>(</span>
            <span class=n>input_ids</span><span class=o>=</span><span class=n>decoder_input_ids</span><span class=p>,</span>
            <span class=n>attention_mask</span><span class=o>=</span><span class=n>decoder_attention_mask</span><span class=p>,</span>
            <span class=n>encoder_hidden_states</span><span class=o>=</span><span class=n>encoder_outputs</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span>
            <span class=n>encoder_attention_mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=p>,</span>
            <span class=n>head_mask</span><span class=o>=</span><span class=n>decoder_head_mask</span><span class=p>,</span>
            <span class=n>cross_attn_head_mask</span><span class=o>=</span><span class=n>cross_attn_head_mask</span><span class=p>,</span>
            <span class=n>past_key_values</span><span class=o>=</span><span class=n>past_key_values</span><span class=p>,</span>
            <span class=n>inputs_embeds</span><span class=o>=</span><span class=n>decoder_inputs_embeds</span><span class=p>,</span>
            <span class=n>use_cache</span><span class=o>=</span><span class=n>use_cache</span><span class=p>,</span>
            <span class=n>output_attentions</span><span class=o>=</span><span class=n>output_attentions</span><span class=p>,</span>
            <span class=n>output_hidden_states</span><span class=o>=</span><span class=n>output_hidden_states</span><span class=p>,</span>
            <span class=n>return_dict</span><span class=o>=</span><span class=n>return_dict</span><span class=p>,</span>
        <span class=p>)</span>

        <span class=k>if</span> <span class=ow>not</span> <span class=n>return_dict</span><span class=p>:</span>
            <span class=k>return</span> <span class=n>decoder_outputs</span> <span class=o>+</span> <span class=n>encoder_outputs</span>

        <span class=k>return</span> <span class=n>Seq2SeqModelOutput</span><span class=p>(</span>
            <span class=n>last_hidden_state</span><span class=o>=</span><span class=n>decoder_outputs</span><span class=o>.</span><span class=n>last_hidden_state</span><span class=p>,</span>
            <span class=n>past_key_values</span><span class=o>=</span><span class=n>decoder_outputs</span><span class=o>.</span><span class=n>past_key_values</span><span class=p>,</span>
            <span class=n>decoder_hidden_states</span><span class=o>=</span><span class=n>decoder_outputs</span><span class=o>.</span><span class=n>hidden_states</span><span class=p>,</span>
            <span class=n>decoder_attentions</span><span class=o>=</span><span class=n>decoder_outputs</span><span class=o>.</span><span class=n>attentions</span><span class=p>,</span>
            <span class=n>cross_attentions</span><span class=o>=</span><span class=n>decoder_outputs</span><span class=o>.</span><span class=n>cross_attentions</span><span class=p>,</span>
            <span class=n>encoder_last_hidden_state</span><span class=o>=</span><span class=n>encoder_outputs</span><span class=o>.</span><span class=n>last_hidden_state</span><span class=p>,</span>
            <span class=n>encoder_hidden_states</span><span class=o>=</span><span class=n>encoder_outputs</span><span class=o>.</span><span class=n>hidden_states</span><span class=p>,</span>
            <span class=n>encoder_attentions</span><span class=o>=</span><span class=n>encoder_outputs</span><span class=o>.</span><span class=n>attentions</span><span class=p>,</span>
        <span class=p>)</span>


<span class=nd>@add_start_docstrings</span><span class=p>(</span>
    <span class=s2>"The BART Model with a language modeling head. Can be used for summarization."</span><span class=p>,</span> <span class=n>BART_START_DOCSTRING</span>
<span class=p>)</span>
<span class=k>class</span> <span class=nc>BartForConditionalGeneration</span><span class=p>(</span><span class=n>BartPreTrainedModel</span><span class=p>):</span>
    <span class=n>base_model_prefix</span> <span class=o>=</span> <span class=s2>"model"</span>
    <span class=n>_tied_weights_keys</span> <span class=o>=</span> <span class=p>[</span><span class=s2>"encoder.embed_tokens.weight"</span><span class=p>,</span> <span class=s2>"decoder.embed_tokens.weight"</span><span class=p>,</span> <span class=s2>"lm_head.weight"</span><span class=p>]</span>
    <span class=n>_keys_to_ignore_on_load_missing</span> <span class=o>=</span> <span class=p>[</span><span class=s2>"final_logits_bias"</span><span class=p>]</span>

    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=n>BartConfig</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>BartModel</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span><span class=s2>"final_logits_bias"</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>shared</span><span class=o>.</span><span class=n>num_embeddings</span><span class=p>)))</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>lm_head</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>d_model</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>shared</span><span class=o>.</span><span class=n>num_embeddings</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

        <span class=c1># Initialize weights and apply final processing</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>post_init</span><span class=p>()</span>

    <span class=k>def</span> <span class=nf>get_encoder</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>get_encoder</span><span class=p>()</span>

    <span class=k>def</span> <span class=nf>get_decoder</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>get_decoder</span><span class=p>()</span>

    <span class=k>def</span> <span class=nf>resize_token_embeddings</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>new_num_tokens</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>pad_to_multiple_of</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>int</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>)</span> <span class=o>-></span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>:</span>
        <span class=n>new_embeddings</span> <span class=o>=</span> <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=n>resize_token_embeddings</span><span class=p>(</span><span class=n>new_num_tokens</span><span class=p>,</span> <span class=n>pad_to_multiple_of</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>_resize_final_logits_bias</span><span class=p>(</span><span class=n>new_embeddings</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
        <span class=k>return</span> <span class=n>new_embeddings</span>

    <span class=k>def</span> <span class=nf>_resize_final_logits_bias</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>new_num_tokens</span><span class=p>:</span> <span class=nb>int</span><span class=p>)</span> <span class=o>-></span> <span class=kc>None</span><span class=p>:</span>
        <span class=n>old_num_tokens</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>final_logits_bias</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
        <span class=k>if</span> <span class=n>new_num_tokens</span> <span class=o><=</span> <span class=n>old_num_tokens</span><span class=p>:</span>
            <span class=n>new_bias</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>final_logits_bias</span><span class=p>[:,</span> <span class=p>:</span><span class=n>new_num_tokens</span><span class=p>]</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=n>extra_bias</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=n>new_num_tokens</span> <span class=o>-</span> <span class=n>old_num_tokens</span><span class=p>),</span> <span class=n>device</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>final_logits_bias</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
            <span class=n>new_bias</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=bp>self</span><span class=o>.</span><span class=n>final_logits_bias</span><span class=p>,</span> <span class=n>extra_bias</span><span class=p>],</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span><span class=s2>"final_logits_bias"</span><span class=p>,</span> <span class=n>new_bias</span><span class=p>)</span>

    <span class=k>def</span> <span class=nf>get_output_embeddings</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>lm_head</span>

    <span class=k>def</span> <span class=nf>set_output_embeddings</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>new_embeddings</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>lm_head</span> <span class=o>=</span> <span class=n>new_embeddings</span>

    <span class=nd>@add_start_docstrings_to_model_forward</span><span class=p>(</span><span class=n>BART_INPUTS_DOCSTRING</span><span class=p>)</span>
    <span class=nd>@replace_return_docstrings</span><span class=p>(</span><span class=n>output_type</span><span class=o>=</span><span class=n>Seq2SeqLMOutput</span><span class=p>,</span> <span class=n>config_class</span><span class=o>=</span><span class=n>_CONFIG_FOR_DOC</span><span class=p>)</span>
    <span class=nd>@add_end_docstrings</span><span class=p>(</span><span class=n>BART_GENERATION_EXAMPLE</span><span class=p>)</span>
    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span>
        <span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>decoder_input_ids</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>decoder_attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>head_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>decoder_head_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>cross_attn_head_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>encoder_outputs</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>List</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>past_key_values</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>List</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>inputs_embeds</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>decoder_inputs_embeds</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>labels</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>use_cache</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>output_attentions</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>output_hidden_states</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>return_dict</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
    <span class=p>)</span> <span class=o>-></span> <span class=n>Union</span><span class=p>[</span><span class=n>Tuple</span><span class=p>,</span> <span class=n>Seq2SeqLMOutput</span><span class=p>]:</span>
<span class=w>        </span><span class=sa>r</span><span class=sd>"""</span>
<span class=sd>        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class=sd>            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,</span>
<span class=sd>            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored</span>
<span class=sd>            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.</span>

<span class=sd>        Returns:</span>
<span class=sd>        """</span>
        <span class=n>return_dict</span> <span class=o>=</span> <span class=n>return_dict</span> <span class=k>if</span> <span class=n>return_dict</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>use_return_dict</span>

        <span class=k>if</span> <span class=n>labels</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=k>if</span> <span class=n>use_cache</span><span class=p>:</span>
                <span class=n>logger</span><span class=o>.</span><span class=n>warning</span><span class=p>(</span><span class=s2>"The `use_cache` argument is changed to `False` since `labels` is provided."</span><span class=p>)</span>
            <span class=n>use_cache</span> <span class=o>=</span> <span class=kc>False</span>
            <span class=k>if</span> <span class=n>decoder_input_ids</span> <span class=ow>is</span> <span class=kc>None</span> <span class=ow>and</span> <span class=n>decoder_inputs_embeds</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
                <span class=n>decoder_input_ids</span> <span class=o>=</span> <span class=n>shift_tokens_right</span><span class=p>(</span>
                    <span class=n>labels</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>pad_token_id</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>decoder_start_token_id</span>
                <span class=p>)</span>

        <span class=n>outputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span>
            <span class=n>input_ids</span><span class=p>,</span>
            <span class=n>attention_mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=p>,</span>
            <span class=n>decoder_input_ids</span><span class=o>=</span><span class=n>decoder_input_ids</span><span class=p>,</span>
            <span class=n>encoder_outputs</span><span class=o>=</span><span class=n>encoder_outputs</span><span class=p>,</span>
            <span class=n>decoder_attention_mask</span><span class=o>=</span><span class=n>decoder_attention_mask</span><span class=p>,</span>
            <span class=n>head_mask</span><span class=o>=</span><span class=n>head_mask</span><span class=p>,</span>
            <span class=n>decoder_head_mask</span><span class=o>=</span><span class=n>decoder_head_mask</span><span class=p>,</span>
            <span class=n>cross_attn_head_mask</span><span class=o>=</span><span class=n>cross_attn_head_mask</span><span class=p>,</span>
            <span class=n>past_key_values</span><span class=o>=</span><span class=n>past_key_values</span><span class=p>,</span>
            <span class=n>inputs_embeds</span><span class=o>=</span><span class=n>inputs_embeds</span><span class=p>,</span>
            <span class=n>decoder_inputs_embeds</span><span class=o>=</span><span class=n>decoder_inputs_embeds</span><span class=p>,</span>
            <span class=n>use_cache</span><span class=o>=</span><span class=n>use_cache</span><span class=p>,</span>
            <span class=n>output_attentions</span><span class=o>=</span><span class=n>output_attentions</span><span class=p>,</span>
            <span class=n>output_hidden_states</span><span class=o>=</span><span class=n>output_hidden_states</span><span class=p>,</span>
            <span class=n>return_dict</span><span class=o>=</span><span class=n>return_dict</span><span class=p>,</span>
        <span class=p>)</span>

        <span class=n>lm_logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>lm_head</span><span class=p>(</span><span class=n>outputs</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
        <span class=n>lm_logits</span> <span class=o>=</span> <span class=n>lm_logits</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>final_logits_bias</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>lm_logits</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>

        <span class=n>masked_lm_loss</span> <span class=o>=</span> <span class=kc>None</span>
        <span class=k>if</span> <span class=n>labels</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=n>labels</span> <span class=o>=</span> <span class=n>labels</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>lm_logits</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
            <span class=n>loss_fct</span> <span class=o>=</span> <span class=n>CrossEntropyLoss</span><span class=p>()</span>
            <span class=n>masked_lm_loss</span> <span class=o>=</span> <span class=n>loss_fct</span><span class=p>(</span><span class=n>lm_logits</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>vocab_size</span><span class=p>),</span> <span class=n>labels</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>))</span>

        <span class=k>if</span> <span class=ow>not</span> <span class=n>return_dict</span><span class=p>:</span>
            <span class=n>output</span> <span class=o>=</span> <span class=p>(</span><span class=n>lm_logits</span><span class=p>,)</span> <span class=o>+</span> <span class=n>outputs</span><span class=p>[</span><span class=mi>1</span><span class=p>:]</span>
            <span class=k>return</span> <span class=p>((</span><span class=n>masked_lm_loss</span><span class=p>,)</span> <span class=o>+</span> <span class=n>output</span><span class=p>)</span> <span class=k>if</span> <span class=n>masked_lm_loss</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=n>output</span>

        <span class=k>return</span> <span class=n>Seq2SeqLMOutput</span><span class=p>(</span>
            <span class=n>loss</span><span class=o>=</span><span class=n>masked_lm_loss</span><span class=p>,</span>
            <span class=n>logits</span><span class=o>=</span><span class=n>lm_logits</span><span class=p>,</span>
            <span class=n>past_key_values</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>past_key_values</span><span class=p>,</span>
            <span class=n>decoder_hidden_states</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>decoder_hidden_states</span><span class=p>,</span>
            <span class=n>decoder_attentions</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>decoder_attentions</span><span class=p>,</span>
            <span class=n>cross_attentions</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>cross_attentions</span><span class=p>,</span>
            <span class=n>encoder_last_hidden_state</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>encoder_last_hidden_state</span><span class=p>,</span>
            <span class=n>encoder_hidden_states</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>encoder_hidden_states</span><span class=p>,</span>
            <span class=n>encoder_attentions</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>encoder_attentions</span><span class=p>,</span>
        <span class=p>)</span>

    <span class=k>def</span> <span class=nf>prepare_inputs_for_generation</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span>
        <span class=n>decoder_input_ids</span><span class=p>,</span>
        <span class=n>past_key_values</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
        <span class=n>attention_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
        <span class=n>decoder_attention_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
        <span class=n>head_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
        <span class=n>decoder_head_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
        <span class=n>cross_attn_head_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
        <span class=n>use_cache</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
        <span class=n>encoder_outputs</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
        <span class=o>**</span><span class=n>kwargs</span><span class=p>,</span>
    <span class=p>):</span>
        <span class=c1># cut decoder_input_ids if past_key_values is used</span>
        <span class=k>if</span> <span class=n>past_key_values</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=n>past_length</span> <span class=o>=</span> <span class=n>past_key_values</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span>

            <span class=c1># Some generation methods already pass only the last input ID</span>
            <span class=k>if</span> <span class=n>decoder_input_ids</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>></span> <span class=n>past_length</span><span class=p>:</span>
                <span class=n>remove_prefix_length</span> <span class=o>=</span> <span class=n>past_length</span>
            <span class=k>else</span><span class=p>:</span>
                <span class=c1># Default to old behavior: keep only final ID</span>
                <span class=n>remove_prefix_length</span> <span class=o>=</span> <span class=n>decoder_input_ids</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=mi>1</span>

            <span class=n>decoder_input_ids</span> <span class=o>=</span> <span class=n>decoder_input_ids</span><span class=p>[:,</span> <span class=n>remove_prefix_length</span><span class=p>:]</span>

        <span class=k>return</span> <span class=p>{</span>
            <span class=s2>"input_ids"</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>  <span class=c1># encoder_outputs is defined. input_ids not needed</span>
            <span class=s2>"encoder_outputs"</span><span class=p>:</span> <span class=n>encoder_outputs</span><span class=p>,</span>
            <span class=s2>"past_key_values"</span><span class=p>:</span> <span class=n>past_key_values</span><span class=p>,</span>
            <span class=s2>"decoder_input_ids"</span><span class=p>:</span> <span class=n>decoder_input_ids</span><span class=p>,</span>
            <span class=s2>"attention_mask"</span><span class=p>:</span> <span class=n>attention_mask</span><span class=p>,</span>
            <span class=s2>"decoder_attention_mask"</span><span class=p>:</span> <span class=n>decoder_attention_mask</span><span class=p>,</span>
            <span class=s2>"head_mask"</span><span class=p>:</span> <span class=n>head_mask</span><span class=p>,</span>
            <span class=s2>"decoder_head_mask"</span><span class=p>:</span> <span class=n>decoder_head_mask</span><span class=p>,</span>
            <span class=s2>"cross_attn_head_mask"</span><span class=p>:</span> <span class=n>cross_attn_head_mask</span><span class=p>,</span>
            <span class=s2>"use_cache"</span><span class=p>:</span> <span class=n>use_cache</span><span class=p>,</span>  <span class=c1># change this to avoid caching (presumably for debugging)</span>
        <span class=p>}</span>

    <span class=k>def</span> <span class=nf>prepare_decoder_input_ids_from_labels</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>labels</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>):</span>
        <span class=k>return</span> <span class=n>shift_tokens_right</span><span class=p>(</span><span class=n>labels</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>pad_token_id</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>decoder_start_token_id</span><span class=p>)</span>

    <span class=nd>@staticmethod</span>
    <span class=k>def</span> <span class=nf>_reorder_cache</span><span class=p>(</span><span class=n>past_key_values</span><span class=p>,</span> <span class=n>beam_idx</span><span class=p>):</span>
        <span class=n>reordered_past</span> <span class=o>=</span> <span class=p>()</span>
        <span class=k>for</span> <span class=n>layer_past</span> <span class=ow>in</span> <span class=n>past_key_values</span><span class=p>:</span>
            <span class=c1># cached cross_attention states don't have to be reordered -> they are always the same</span>
            <span class=n>reordered_past</span> <span class=o>+=</span> <span class=p>(</span>
                <span class=nb>tuple</span><span class=p>(</span><span class=n>past_state</span><span class=o>.</span><span class=n>index_select</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>beam_idx</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>past_state</span><span class=o>.</span><span class=n>device</span><span class=p>))</span> <span class=k>for</span> <span class=n>past_state</span> <span class=ow>in</span> <span class=n>layer_past</span><span class=p>[:</span><span class=mi>2</span><span class=p>])</span>
                <span class=o>+</span> <span class=n>layer_past</span><span class=p>[</span><span class=mi>2</span><span class=p>:],</span>
            <span class=p>)</span>
        <span class=k>return</span> <span class=n>reordered_past</span>


<span class=nd>@add_start_docstrings</span><span class=p>(</span>
<span class=w>    </span><span class=sd>"""</span>
<span class=sd>    Bart model with a sequence classification/head on top (a linear layer on top of the pooled output) e.g. for GLUE</span>
<span class=sd>    tasks.</span>
<span class=sd>    """</span><span class=p>,</span>
    <span class=n>BART_START_DOCSTRING</span><span class=p>,</span>
<span class=p>)</span>
<span class=k>class</span> <span class=nc>BartForSequenceClassification</span><span class=p>(</span><span class=n>BartPreTrainedModel</span><span class=p>):</span>
    <span class=n>_tied_weights_keys</span> <span class=o>=</span> <span class=p>[</span><span class=s2>"encoder.embed_tokens.weight"</span><span class=p>,</span> <span class=s2>"decoder.embed_tokens.weight"</span><span class=p>]</span>

    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=n>BartConfig</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>config</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>BartModel</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>classification_head</span> <span class=o>=</span> <span class=n>BartClassificationHead</span><span class=p>(</span>
            <span class=n>config</span><span class=o>.</span><span class=n>d_model</span><span class=p>,</span>
            <span class=n>config</span><span class=o>.</span><span class=n>d_model</span><span class=p>,</span>
            <span class=n>config</span><span class=o>.</span><span class=n>num_labels</span><span class=p>,</span>
            <span class=n>config</span><span class=o>.</span><span class=n>classifier_dropout</span><span class=p>,</span>
        <span class=p>)</span>

        <span class=c1># Initialize weights and apply final processing</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>post_init</span><span class=p>()</span>

    <span class=nd>@add_start_docstrings_to_model_forward</span><span class=p>(</span><span class=n>BART_INPUTS_DOCSTRING</span><span class=p>)</span>
    <span class=nd>@add_code_sample_docstrings</span><span class=p>(</span>
        <span class=n>checkpoint</span><span class=o>=</span><span class=n>_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION</span><span class=p>,</span>
        <span class=n>output_type</span><span class=o>=</span><span class=n>Seq2SeqSequenceClassifierOutput</span><span class=p>,</span>
        <span class=n>config_class</span><span class=o>=</span><span class=n>_CONFIG_FOR_DOC</span><span class=p>,</span>
        <span class=n>expected_output</span><span class=o>=</span><span class=n>_SEQ_CLASS_EXPECTED_OUTPUT</span><span class=p>,</span>
        <span class=n>expected_loss</span><span class=o>=</span><span class=n>_SEQ_CLASS_EXPECTED_LOSS</span><span class=p>,</span>
    <span class=p>)</span>
    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span>
        <span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>decoder_input_ids</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>decoder_attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>head_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>decoder_head_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>cross_attn_head_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>encoder_outputs</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>List</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>inputs_embeds</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>decoder_inputs_embeds</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>labels</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>use_cache</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>output_attentions</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>output_hidden_states</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>return_dict</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
    <span class=p>)</span> <span class=o>-></span> <span class=n>Union</span><span class=p>[</span><span class=n>Tuple</span><span class=p>,</span> <span class=n>Seq2SeqSequenceClassifierOutput</span><span class=p>]:</span>
<span class=w>        </span><span class=sa>r</span><span class=sd>"""</span>
<span class=sd>        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):</span>
<span class=sd>            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,</span>
<span class=sd>            config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).</span>
<span class=sd>        """</span>
        <span class=n>return_dict</span> <span class=o>=</span> <span class=n>return_dict</span> <span class=k>if</span> <span class=n>return_dict</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>use_return_dict</span>
        <span class=k>if</span> <span class=n>labels</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=n>use_cache</span> <span class=o>=</span> <span class=kc>False</span>

        <span class=k>if</span> <span class=n>input_ids</span> <span class=ow>is</span> <span class=kc>None</span> <span class=ow>and</span> <span class=n>inputs_embeds</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=k>raise</span> <span class=ne>NotImplementedError</span><span class=p>(</span>
                <span class=sa>f</span><span class=s2>"Passing input embeddings is currently not supported for </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=vm>__class__</span><span class=o>.</span><span class=vm>__name__</span><span class=si>}</span><span class=s2>"</span>
            <span class=p>)</span>

        <span class=n>outputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span>
            <span class=n>input_ids</span><span class=p>,</span>
            <span class=n>attention_mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=p>,</span>
            <span class=n>decoder_input_ids</span><span class=o>=</span><span class=n>decoder_input_ids</span><span class=p>,</span>
            <span class=n>decoder_attention_mask</span><span class=o>=</span><span class=n>decoder_attention_mask</span><span class=p>,</span>
            <span class=n>head_mask</span><span class=o>=</span><span class=n>head_mask</span><span class=p>,</span>
            <span class=n>decoder_head_mask</span><span class=o>=</span><span class=n>decoder_head_mask</span><span class=p>,</span>
            <span class=n>cross_attn_head_mask</span><span class=o>=</span><span class=n>cross_attn_head_mask</span><span class=p>,</span>
            <span class=n>encoder_outputs</span><span class=o>=</span><span class=n>encoder_outputs</span><span class=p>,</span>
            <span class=n>inputs_embeds</span><span class=o>=</span><span class=n>inputs_embeds</span><span class=p>,</span>
            <span class=n>decoder_inputs_embeds</span><span class=o>=</span><span class=n>decoder_inputs_embeds</span><span class=p>,</span>
            <span class=n>use_cache</span><span class=o>=</span><span class=n>use_cache</span><span class=p>,</span>
            <span class=n>output_attentions</span><span class=o>=</span><span class=n>output_attentions</span><span class=p>,</span>
            <span class=n>output_hidden_states</span><span class=o>=</span><span class=n>output_hidden_states</span><span class=p>,</span>
            <span class=n>return_dict</span><span class=o>=</span><span class=n>return_dict</span><span class=p>,</span>
        <span class=p>)</span>
        <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>outputs</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>  <span class=c1># last hidden state</span>

        <span class=n>eos_mask</span> <span class=o>=</span> <span class=n>input_ids</span><span class=o>.</span><span class=n>eq</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>eos_token_id</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>hidden_states</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>

        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>unique_consecutive</span><span class=p>(</span><span class=n>eos_mask</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=mi>1</span><span class=p>)))</span> <span class=o>></span> <span class=mi>1</span><span class=p>:</span>
            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=s2>"All examples must have the same number of &LTeos> tokens."</span><span class=p>)</span>
        <span class=n>sentence_representation</span> <span class=o>=</span> <span class=n>hidden_states</span><span class=p>[</span><span class=n>eos_mask</span><span class=p>,</span> <span class=p>:]</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>hidden_states</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>hidden_states</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>))[</span>
            <span class=p>:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:</span>
        <span class=p>]</span>
        <span class=n>logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>classification_head</span><span class=p>(</span><span class=n>sentence_representation</span><span class=p>)</span>

        <span class=n>loss</span> <span class=o>=</span> <span class=kc>None</span>
        <span class=k>if</span> <span class=n>labels</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=n>labels</span> <span class=o>=</span> <span class=n>labels</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>logits</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>problem_type</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
                <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>num_labels</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
                    <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>problem_type</span> <span class=o>=</span> <span class=s2>"regression"</span>
                <span class=k>elif</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>num_labels</span> <span class=o>></span> <span class=mi>1</span> <span class=ow>and</span> <span class=p>(</span><span class=n>labels</span><span class=o>.</span><span class=n>dtype</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>long</span> <span class=ow>or</span> <span class=n>labels</span><span class=o>.</span><span class=n>dtype</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>int</span><span class=p>):</span>
                    <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>problem_type</span> <span class=o>=</span> <span class=s2>"single_label_classification"</span>
                <span class=k>else</span><span class=p>:</span>
                    <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>problem_type</span> <span class=o>=</span> <span class=s2>"multi_label_classification"</span>

            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>problem_type</span> <span class=o>==</span> <span class=s2>"regression"</span><span class=p>:</span>
                <span class=n>loss_fct</span> <span class=o>=</span> <span class=n>MSELoss</span><span class=p>()</span>
                <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>num_labels</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
                    <span class=n>loss</span> <span class=o>=</span> <span class=n>loss_fct</span><span class=p>(</span><span class=n>logits</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(),</span> <span class=n>labels</span><span class=o>.</span><span class=n>squeeze</span><span class=p>())</span>
                <span class=k>else</span><span class=p>:</span>
                    <span class=n>loss</span> <span class=o>=</span> <span class=n>loss_fct</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>
            <span class=k>elif</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>problem_type</span> <span class=o>==</span> <span class=s2>"single_label_classification"</span><span class=p>:</span>
                <span class=n>loss_fct</span> <span class=o>=</span> <span class=n>CrossEntropyLoss</span><span class=p>()</span>
                <span class=n>loss</span> <span class=o>=</span> <span class=n>loss_fct</span><span class=p>(</span><span class=n>logits</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>num_labels</span><span class=p>),</span> <span class=n>labels</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>))</span>
            <span class=k>elif</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>problem_type</span> <span class=o>==</span> <span class=s2>"multi_label_classification"</span><span class=p>:</span>
                <span class=n>loss_fct</span> <span class=o>=</span> <span class=n>BCEWithLogitsLoss</span><span class=p>()</span>
                <span class=n>loss</span> <span class=o>=</span> <span class=n>loss_fct</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>
        <span class=k>if</span> <span class=ow>not</span> <span class=n>return_dict</span><span class=p>:</span>
            <span class=n>output</span> <span class=o>=</span> <span class=p>(</span><span class=n>logits</span><span class=p>,)</span> <span class=o>+</span> <span class=n>outputs</span><span class=p>[</span><span class=mi>1</span><span class=p>:]</span>
            <span class=k>return</span> <span class=p>((</span><span class=n>loss</span><span class=p>,)</span> <span class=o>+</span> <span class=n>output</span><span class=p>)</span> <span class=k>if</span> <span class=n>loss</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=n>output</span>

        <span class=k>return</span> <span class=n>Seq2SeqSequenceClassifierOutput</span><span class=p>(</span>
            <span class=n>loss</span><span class=o>=</span><span class=n>loss</span><span class=p>,</span>
            <span class=n>logits</span><span class=o>=</span><span class=n>logits</span><span class=p>,</span>
            <span class=n>past_key_values</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>past_key_values</span><span class=p>,</span>
            <span class=n>decoder_hidden_states</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>decoder_hidden_states</span><span class=p>,</span>
            <span class=n>decoder_attentions</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>decoder_attentions</span><span class=p>,</span>
            <span class=n>cross_attentions</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>cross_attentions</span><span class=p>,</span>
            <span class=n>encoder_last_hidden_state</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>encoder_last_hidden_state</span><span class=p>,</span>
            <span class=n>encoder_hidden_states</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>encoder_hidden_states</span><span class=p>,</span>
            <span class=n>encoder_attentions</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>encoder_attentions</span><span class=p>,</span>
        <span class=p>)</span>


<span class=nd>@add_start_docstrings</span><span class=p>(</span>
<span class=w>    </span><span class=sd>"""</span>
<span class=sd>    BART Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear</span>
<span class=sd>    layer on top of the hidden-states output to compute `span start logits` and `span end logits`).</span>
<span class=sd>    """</span><span class=p>,</span>
    <span class=n>BART_START_DOCSTRING</span><span class=p>,</span>
<span class=p>)</span>
<span class=k>class</span> <span class=nc>BartForQuestionAnswering</span><span class=p>(</span><span class=n>BartPreTrainedModel</span><span class=p>):</span>
    <span class=n>_tied_weights_keys</span> <span class=o>=</span> <span class=p>[</span><span class=s2>"encoder.embed_tokens.weight"</span><span class=p>,</span> <span class=s2>"decoder.embed_tokens.weight"</span><span class=p>]</span>

    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>

        <span class=n>config</span><span class=o>.</span><span class=n>num_labels</span> <span class=o>=</span> <span class=mi>2</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>num_labels</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>num_labels</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>BartModel</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>qa_outputs</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>num_labels</span><span class=p>)</span>

        <span class=c1># Initialize weights and apply final processing</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>post_init</span><span class=p>()</span>

    <span class=nd>@add_start_docstrings_to_model_forward</span><span class=p>(</span><span class=n>BART_INPUTS_DOCSTRING</span><span class=p>)</span>
    <span class=nd>@add_code_sample_docstrings</span><span class=p>(</span>
        <span class=n>checkpoint</span><span class=o>=</span><span class=n>_CHECKPOINT_FOR_QA</span><span class=p>,</span>
        <span class=n>output_type</span><span class=o>=</span><span class=n>Seq2SeqQuestionAnsweringModelOutput</span><span class=p>,</span>
        <span class=n>config_class</span><span class=o>=</span><span class=n>_CONFIG_FOR_DOC</span><span class=p>,</span>
        <span class=n>expected_loss</span><span class=o>=</span><span class=n>_QA_EXPECTED_LOSS</span><span class=p>,</span>
        <span class=n>expected_output</span><span class=o>=</span><span class=n>_QA_EXPECTED_OUTPUT</span><span class=p>,</span>
    <span class=p>)</span>
    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span>
        <span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>decoder_input_ids</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>decoder_attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>head_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>decoder_head_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>cross_attn_head_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>encoder_outputs</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>List</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>start_positions</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>end_positions</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>inputs_embeds</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>decoder_inputs_embeds</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>use_cache</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>output_attentions</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>output_hidden_states</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>return_dict</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
    <span class=p>)</span> <span class=o>-></span> <span class=n>Union</span><span class=p>[</span><span class=n>Tuple</span><span class=p>,</span> <span class=n>Seq2SeqQuestionAnsweringModelOutput</span><span class=p>]:</span>
<span class=w>        </span><span class=sa>r</span><span class=sd>"""</span>
<span class=sd>        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):</span>
<span class=sd>            Labels for position (index) of the start of the labelled span for computing the token classification loss.</span>
<span class=sd>            Positions are clamped to the length of the sequence (*sequence_length*). Position outside of the sequence</span>
<span class=sd>            are not taken into account for computing the loss.</span>
<span class=sd>        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):</span>
<span class=sd>            Labels for position (index) of the end of the labelled span for computing the token classification loss.</span>
<span class=sd>            Positions are clamped to the length of the sequence (*sequence_length*). Position outside of the sequence</span>
<span class=sd>            are not taken into account for computing the loss.</span>
<span class=sd>        """</span>
        <span class=n>return_dict</span> <span class=o>=</span> <span class=n>return_dict</span> <span class=k>if</span> <span class=n>return_dict</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>use_return_dict</span>
        <span class=k>if</span> <span class=n>start_positions</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=ow>and</span> <span class=n>end_positions</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=n>use_cache</span> <span class=o>=</span> <span class=kc>False</span>

        <span class=n>outputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span>
            <span class=n>input_ids</span><span class=p>,</span>
            <span class=n>attention_mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=p>,</span>
            <span class=n>decoder_input_ids</span><span class=o>=</span><span class=n>decoder_input_ids</span><span class=p>,</span>
            <span class=n>decoder_attention_mask</span><span class=o>=</span><span class=n>decoder_attention_mask</span><span class=p>,</span>
            <span class=n>head_mask</span><span class=o>=</span><span class=n>head_mask</span><span class=p>,</span>
            <span class=n>decoder_head_mask</span><span class=o>=</span><span class=n>decoder_head_mask</span><span class=p>,</span>
            <span class=n>cross_attn_head_mask</span><span class=o>=</span><span class=n>cross_attn_head_mask</span><span class=p>,</span>
            <span class=n>encoder_outputs</span><span class=o>=</span><span class=n>encoder_outputs</span><span class=p>,</span>
            <span class=n>inputs_embeds</span><span class=o>=</span><span class=n>inputs_embeds</span><span class=p>,</span>
            <span class=n>decoder_inputs_embeds</span><span class=o>=</span><span class=n>decoder_inputs_embeds</span><span class=p>,</span>
            <span class=n>use_cache</span><span class=o>=</span><span class=n>use_cache</span><span class=p>,</span>
            <span class=n>output_attentions</span><span class=o>=</span><span class=n>output_attentions</span><span class=p>,</span>
            <span class=n>output_hidden_states</span><span class=o>=</span><span class=n>output_hidden_states</span><span class=p>,</span>
            <span class=n>return_dict</span><span class=o>=</span><span class=n>return_dict</span><span class=p>,</span>
        <span class=p>)</span>

        <span class=n>sequence_output</span> <span class=o>=</span> <span class=n>outputs</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>

        <span class=n>logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>qa_outputs</span><span class=p>(</span><span class=n>sequence_output</span><span class=p>)</span>
        <span class=n>start_logits</span><span class=p>,</span> <span class=n>end_logits</span> <span class=o>=</span> <span class=n>logits</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
        <span class=n>start_logits</span> <span class=o>=</span> <span class=n>start_logits</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span>
        <span class=n>end_logits</span> <span class=o>=</span> <span class=n>end_logits</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span>

        <span class=n>total_loss</span> <span class=o>=</span> <span class=kc>None</span>
        <span class=k>if</span> <span class=n>start_positions</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=ow>and</span> <span class=n>end_positions</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=c1># If we are on multi-GPU, split add a dimension</span>
            <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>start_positions</span><span class=o>.</span><span class=n>size</span><span class=p>())</span> <span class=o>></span> <span class=mi>1</span><span class=p>:</span>
                <span class=n>start_positions</span> <span class=o>=</span> <span class=n>start_positions</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
            <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>end_positions</span><span class=o>.</span><span class=n>size</span><span class=p>())</span> <span class=o>></span> <span class=mi>1</span><span class=p>:</span>
                <span class=n>end_positions</span> <span class=o>=</span> <span class=n>end_positions</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
            <span class=c1># sometimes the start/end positions are outside our model inputs, we ignore these terms</span>
            <span class=n>ignored_index</span> <span class=o>=</span> <span class=n>start_logits</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
            <span class=n>start_positions</span> <span class=o>=</span> <span class=n>start_positions</span><span class=o>.</span><span class=n>clamp</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>ignored_index</span><span class=p>)</span>
            <span class=n>end_positions</span> <span class=o>=</span> <span class=n>end_positions</span><span class=o>.</span><span class=n>clamp</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>ignored_index</span><span class=p>)</span>

            <span class=n>loss_fct</span> <span class=o>=</span> <span class=n>CrossEntropyLoss</span><span class=p>(</span><span class=n>ignore_index</span><span class=o>=</span><span class=n>ignored_index</span><span class=p>)</span>
            <span class=n>start_loss</span> <span class=o>=</span> <span class=n>loss_fct</span><span class=p>(</span><span class=n>start_logits</span><span class=p>,</span> <span class=n>start_positions</span><span class=p>)</span>
            <span class=n>end_loss</span> <span class=o>=</span> <span class=n>loss_fct</span><span class=p>(</span><span class=n>end_logits</span><span class=p>,</span> <span class=n>end_positions</span><span class=p>)</span>
            <span class=n>total_loss</span> <span class=o>=</span> <span class=p>(</span><span class=n>start_loss</span> <span class=o>+</span> <span class=n>end_loss</span><span class=p>)</span> <span class=o>/</span> <span class=mi>2</span>

        <span class=k>if</span> <span class=ow>not</span> <span class=n>return_dict</span><span class=p>:</span>
            <span class=n>output</span> <span class=o>=</span> <span class=p>(</span>
                <span class=n>start_logits</span><span class=p>,</span>
                <span class=n>end_logits</span><span class=p>,</span>
            <span class=p>)</span> <span class=o>+</span> <span class=n>outputs</span><span class=p>[</span><span class=mi>1</span><span class=p>:]</span>
            <span class=k>return</span> <span class=p>((</span><span class=n>total_loss</span><span class=p>,)</span> <span class=o>+</span> <span class=n>output</span><span class=p>)</span> <span class=k>if</span> <span class=n>total_loss</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=n>output</span>

        <span class=k>return</span> <span class=n>Seq2SeqQuestionAnsweringModelOutput</span><span class=p>(</span>
            <span class=n>loss</span><span class=o>=</span><span class=n>total_loss</span><span class=p>,</span>
            <span class=n>start_logits</span><span class=o>=</span><span class=n>start_logits</span><span class=p>,</span>
            <span class=n>end_logits</span><span class=o>=</span><span class=n>end_logits</span><span class=p>,</span>
            <span class=n>past_key_values</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>past_key_values</span><span class=p>,</span>
            <span class=n>decoder_hidden_states</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>decoder_hidden_states</span><span class=p>,</span>
            <span class=n>decoder_attentions</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>decoder_attentions</span><span class=p>,</span>
            <span class=n>cross_attentions</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>cross_attentions</span><span class=p>,</span>
            <span class=n>encoder_last_hidden_state</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>encoder_last_hidden_state</span><span class=p>,</span>
            <span class=n>encoder_hidden_states</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>encoder_hidden_states</span><span class=p>,</span>
            <span class=n>encoder_attentions</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>encoder_attentions</span><span class=p>,</span>
        <span class=p>)</span>


<span class=k>class</span> <span class=nc>BartDecoderWrapper</span><span class=p>(</span><span class=n>BartPreTrainedModel</span><span class=p>):</span>
<span class=w>    </span><span class=sd>"""</span>
<span class=sd>    This wrapper class is a helper class to correctly load pretrained checkpoints when the causal language model is</span>
<span class=sd>    used in combination with the [`EncoderDecoderModel`] framework.</span>
<span class=sd>    """</span>

    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span> <span class=o>=</span> <span class=n>BartDecoder</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>


<span class=nd>@add_start_docstrings</span><span class=p>(</span>
<span class=w>    </span><span class=sd>"""</span>
<span class=sd>    BART decoder with with a language modeling head on top (linear layer with weights tied to the input embeddings).</span>
<span class=sd>    """</span><span class=p>,</span>
    <span class=n>BART_START_DOCSTRING</span><span class=p>,</span>
<span class=p>)</span>
<span class=k>class</span> <span class=nc>BartForCausalLM</span><span class=p>(</span><span class=n>BartPreTrainedModel</span><span class=p>):</span>
    <span class=n>_tied_weights_keys</span> <span class=o>=</span> <span class=p>[</span><span class=s2>"lm_head.weight"</span><span class=p>]</span>

    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
        <span class=n>config</span> <span class=o>=</span> <span class=n>copy</span><span class=o>.</span><span class=n>deepcopy</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
        <span class=n>config</span><span class=o>.</span><span class=n>is_decoder</span> <span class=o>=</span> <span class=kc>True</span>
        <span class=n>config</span><span class=o>.</span><span class=n>is_encoder_decoder</span> <span class=o>=</span> <span class=kc>False</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>BartDecoderWrapper</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>lm_head</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

        <span class=c1># Initialize weights and apply final processing</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>post_init</span><span class=p>()</span>

    <span class=k>def</span> <span class=nf>get_input_embeddings</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>decoder</span><span class=o>.</span><span class=n>embed_tokens</span>

    <span class=k>def</span> <span class=nf>set_input_embeddings</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>value</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>decoder</span><span class=o>.</span><span class=n>embed_tokens</span> <span class=o>=</span> <span class=n>value</span>

    <span class=k>def</span> <span class=nf>get_output_embeddings</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>lm_head</span>

    <span class=k>def</span> <span class=nf>set_output_embeddings</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>new_embeddings</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>lm_head</span> <span class=o>=</span> <span class=n>new_embeddings</span>

    <span class=k>def</span> <span class=nf>set_decoder</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>decoder</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>decoder</span> <span class=o>=</span> <span class=n>decoder</span>

    <span class=k>def</span> <span class=nf>get_decoder</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>decoder</span>

    <span class=nd>@replace_return_docstrings</span><span class=p>(</span><span class=n>output_type</span><span class=o>=</span><span class=n>CausalLMOutputWithCrossAttentions</span><span class=p>,</span> <span class=n>config_class</span><span class=o>=</span><span class=n>_CONFIG_FOR_DOC</span><span class=p>)</span>
    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span>
        <span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>encoder_hidden_states</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>encoder_attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>head_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>cross_attn_head_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>past_key_values</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>List</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>inputs_embeds</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>labels</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>use_cache</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>output_attentions</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>output_hidden_states</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
        <span class=n>return_dict</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
    <span class=p>)</span> <span class=o>-></span> <span class=n>Union</span><span class=p>[</span><span class=n>Tuple</span><span class=p>,</span> <span class=n>CausalLMOutputWithCrossAttentions</span><span class=p>]:</span>
<span class=w>        </span><span class=sa>r</span><span class=sd>"""</span>
<span class=sd>        Args:</span>
<span class=sd>            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):</span>
<span class=sd>                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you</span>
<span class=sd>                provide it.</span>

<span class=sd>                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and</span>
<span class=sd>                [`PreTrainedTokenizer.__call__`] for details.</span>

<span class=sd>                [What are input IDs?](../glossary#input-ids)</span>
<span class=sd>            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class=sd>                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:</span>

<span class=sd>                - 1 for tokens that are **not masked**,</span>
<span class=sd>                - 0 for tokens that are **masked**.</span>

<span class=sd>                [What are attention masks?](../glossary#attention-mask)</span>
<span class=sd>            encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):</span>
<span class=sd>                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention</span>
<span class=sd>                if the model is configured as a decoder.</span>
<span class=sd>            encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class=sd>                Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used</span>
<span class=sd>                in the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:</span>
<span class=sd>            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):</span>
<span class=sd>                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:</span>

<span class=sd>                - 1 indicates the head is **not masked**,</span>
<span class=sd>                - 0 indicates the head is **masked**.</span>

<span class=sd>            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):</span>
<span class=sd>                Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:</span>

<span class=sd>                - 1 indicates the head is **not masked**,</span>
<span class=sd>                - 0 indicates the head is **masked**.</span>

<span class=sd>            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):</span>
<span class=sd>                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of</span>
<span class=sd>                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of</span>
<span class=sd>                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional</span>
<span class=sd>                tensors are only required when the model is used as a decoder in a Sequence to Sequence model.</span>

<span class=sd>                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the</span>
<span class=sd>                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.</span>

<span class=sd>                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those</span>
<span class=sd>                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of</span>
<span class=sd>                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.</span>
<span class=sd>            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):</span>
<span class=sd>                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,</span>
<span class=sd>                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored</span>
<span class=sd>                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.</span>
<span class=sd>            use_cache (`bool`, *optional*):</span>
<span class=sd>                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding</span>
<span class=sd>                (see `past_key_values`).</span>

<span class=sd>                - 1 for tokens that are **not masked**,</span>
<span class=sd>                - 0 for tokens that are **masked**.</span>
<span class=sd>            output_attentions (`bool`, *optional*):</span>
<span class=sd>                Whether or not to return the attentions tensors of all attention layers. See `attentions` under</span>
<span class=sd>                returned tensors for more detail.</span>
<span class=sd>            output_hidden_states (`bool`, *optional*):</span>
<span class=sd>                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors</span>
<span class=sd>                for more detail.</span>
<span class=sd>            return_dict (`bool`, *optional*):</span>
<span class=sd>                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.</span>

<span class=sd>        Returns:</span>

<span class=sd>        Example:</span>

<span class=sd>        ```python</span>
<span class=sd>        >>> from transformers import AutoTokenizer, BartForCausalLM</span>

<span class=sd>        >>> tokenizer = AutoTokenizer.from_pretrained("facebook/bart-base")</span>
<span class=sd>        >>> model = BartForCausalLM.from_pretrained("facebook/bart-base", add_cross_attention=False)</span>
<span class=sd>        >>> assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."</span>
<span class=sd>        >>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")</span>
<span class=sd>        >>> outputs = model(**inputs)</span>

<span class=sd>        >>> logits = outputs.logits</span>
<span class=sd>        >>> expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]</span>
<span class=sd>        >>> list(logits.shape) == expected_shape</span>
<span class=sd>        True</span>
<span class=sd>        ```"""</span>

        <span class=n>output_attentions</span> <span class=o>=</span> <span class=n>output_attentions</span> <span class=k>if</span> <span class=n>output_attentions</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>output_attentions</span>
        <span class=n>output_hidden_states</span> <span class=o>=</span> <span class=p>(</span>
            <span class=n>output_hidden_states</span> <span class=k>if</span> <span class=n>output_hidden_states</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>output_hidden_states</span>
        <span class=p>)</span>
        <span class=n>return_dict</span> <span class=o>=</span> <span class=n>return_dict</span> <span class=k>if</span> <span class=n>return_dict</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>use_return_dict</span>

        <span class=c1># decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)</span>
        <span class=n>outputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>decoder</span><span class=p>(</span>
            <span class=n>input_ids</span><span class=o>=</span><span class=n>input_ids</span><span class=p>,</span>
            <span class=n>attention_mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=p>,</span>
            <span class=n>encoder_hidden_states</span><span class=o>=</span><span class=n>encoder_hidden_states</span><span class=p>,</span>
            <span class=n>encoder_attention_mask</span><span class=o>=</span><span class=n>encoder_attention_mask</span><span class=p>,</span>
            <span class=n>head_mask</span><span class=o>=</span><span class=n>head_mask</span><span class=p>,</span>
            <span class=n>cross_attn_head_mask</span><span class=o>=</span><span class=n>cross_attn_head_mask</span><span class=p>,</span>
            <span class=n>past_key_values</span><span class=o>=</span><span class=n>past_key_values</span><span class=p>,</span>
            <span class=n>inputs_embeds</span><span class=o>=</span><span class=n>inputs_embeds</span><span class=p>,</span>
            <span class=n>use_cache</span><span class=o>=</span><span class=n>use_cache</span><span class=p>,</span>
            <span class=n>output_attentions</span><span class=o>=</span><span class=n>output_attentions</span><span class=p>,</span>
            <span class=n>output_hidden_states</span><span class=o>=</span><span class=n>output_hidden_states</span><span class=p>,</span>
            <span class=n>return_dict</span><span class=o>=</span><span class=n>return_dict</span><span class=p>,</span>
        <span class=p>)</span>

        <span class=n>logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>lm_head</span><span class=p>(</span><span class=n>outputs</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>

        <span class=n>loss</span> <span class=o>=</span> <span class=kc>None</span>
        <span class=k>if</span> <span class=n>labels</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=n>labels</span> <span class=o>=</span> <span class=n>labels</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>logits</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
            <span class=n>loss_fct</span> <span class=o>=</span> <span class=n>CrossEntropyLoss</span><span class=p>()</span>
            <span class=n>loss</span> <span class=o>=</span> <span class=n>loss_fct</span><span class=p>(</span><span class=n>logits</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>vocab_size</span><span class=p>),</span> <span class=n>labels</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>))</span>

        <span class=k>if</span> <span class=ow>not</span> <span class=n>return_dict</span><span class=p>:</span>
            <span class=n>output</span> <span class=o>=</span> <span class=p>(</span><span class=n>logits</span><span class=p>,)</span> <span class=o>+</span> <span class=n>outputs</span><span class=p>[</span><span class=mi>1</span><span class=p>:]</span>
            <span class=k>return</span> <span class=p>(</span><span class=n>loss</span><span class=p>,)</span> <span class=o>+</span> <span class=n>output</span> <span class=k>if</span> <span class=n>loss</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=n>output</span>

        <span class=k>return</span> <span class=n>CausalLMOutputWithCrossAttentions</span><span class=p>(</span>
            <span class=n>loss</span><span class=o>=</span><span class=n>loss</span><span class=p>,</span>
            <span class=n>logits</span><span class=o>=</span><span class=n>logits</span><span class=p>,</span>
            <span class=n>past_key_values</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>past_key_values</span><span class=p>,</span>
            <span class=n>hidden_states</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>hidden_states</span><span class=p>,</span>
            <span class=n>attentions</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>attentions</span><span class=p>,</span>
            <span class=n>cross_attentions</span><span class=o>=</span><span class=n>outputs</span><span class=o>.</span><span class=n>cross_attentions</span><span class=p>,</span>
        <span class=p>)</span>

    <span class=k>def</span> <span class=nf>prepare_inputs_for_generation</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span> <span class=n>input_ids</span><span class=p>,</span> <span class=n>past_key_values</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>attention_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>use_cache</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span>
    <span class=p>):</span>
        <span class=c1># if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly</span>
        <span class=k>if</span> <span class=n>attention_mask</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
            <span class=n>attention_mask</span> <span class=o>=</span> <span class=n>input_ids</span><span class=o>.</span><span class=n>new_ones</span><span class=p>(</span><span class=n>input_ids</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>

        <span class=k>if</span> <span class=n>past_key_values</span><span class=p>:</span>
            <span class=n>past_length</span> <span class=o>=</span> <span class=n>past_key_values</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span>

            <span class=c1># Some generation methods already pass only the last input ID</span>
            <span class=k>if</span> <span class=n>input_ids</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>></span> <span class=n>past_length</span><span class=p>:</span>
                <span class=n>remove_prefix_length</span> <span class=o>=</span> <span class=n>past_length</span>
            <span class=k>else</span><span class=p>:</span>
                <span class=c1># Default to old behavior: keep only final ID</span>
                <span class=n>remove_prefix_length</span> <span class=o>=</span> <span class=n>input_ids</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=mi>1</span>

            <span class=n>input_ids</span> <span class=o>=</span> <span class=n>input_ids</span><span class=p>[:,</span> <span class=n>remove_prefix_length</span><span class=p>:]</span>
        <span class=c1># first step, decoder_cached_states are empty</span>
        <span class=k>return</span> <span class=p>{</span>
            <span class=s2>"input_ids"</span><span class=p>:</span> <span class=n>input_ids</span><span class=p>,</span>  <span class=c1># encoder_outputs is defined. input_ids not needed</span>
            <span class=s2>"attention_mask"</span><span class=p>:</span> <span class=n>attention_mask</span><span class=p>,</span>
            <span class=s2>"past_key_values"</span><span class=p>:</span> <span class=n>past_key_values</span><span class=p>,</span>
            <span class=s2>"use_cache"</span><span class=p>:</span> <span class=n>use_cache</span><span class=p>,</span>
        <span class=p>}</span>

    <span class=nd>@staticmethod</span>
    <span class=k>def</span> <span class=nf>_reorder_cache</span><span class=p>(</span><span class=n>past_key_values</span><span class=p>,</span> <span class=n>beam_idx</span><span class=p>):</span>
        <span class=n>reordered_past</span> <span class=o>=</span> <span class=p>()</span>
        <span class=k>for</span> <span class=n>layer_past</span> <span class=ow>in</span> <span class=n>past_key_values</span><span class=p>:</span>
            <span class=n>reordered_past</span> <span class=o>+=</span> <span class=p>(</span>
                <span class=nb>tuple</span><span class=p>(</span><span class=n>past_state</span><span class=o>.</span><span class=n>index_select</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>beam_idx</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>past_state</span><span class=o>.</span><span class=n>device</span><span class=p>))</span> <span class=k>for</span> <span class=n>past_state</span> <span class=ow>in</span> <span class=n>layer_past</span><span class=p>),</span>
            <span class=p>)</span>
        <span class=k>return</span> <span class=n>reordered_past</span>
</code></pre></div></td></tr></table></div><hr><blockquote class=page-copyright><span><i class=md-icon><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="m22.7 19-9.1-9.1c.9-2.3.4-5-1.5-6.9-2-2-5-2.4-7.4-1.3L9 6 6 9 1.6 4.7C.4 7.1.9 10.1 2.9 12.1c1.9 1.9 4.6 2.4 6.9 1.5l9.1 9.1c.4.4 1 .4 1.4 0l2.3-2.3c.5-.4.5-1.1.1-1.4Z"/></svg></i>本页面最近更新：</span><span class=facts_modified></span>，<a class=edit_history>更新历史</a><br><span><i class=md-icon><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg></i>发现错误？想一起完善？ <a class=page_edit_url href=None title=edit.link.title>在 GitHub 上编辑此页！</a></span><br><span><i class=md-icon><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M16 17v2H2v-2s0-4 7-4 7 4 7 4m-3.5-9.5A3.5 3.5 0 1 0 9 11a3.5 3.5 0 0 0 3.5-3.5m3.44 5.5A5.32 5.32 0 0 1 18 17v2h4v-2s0-3.63-6.06-4M15 4a3.39 3.39 0 0 0-1.93.59 5 5 0 0 1 0 5.82A3.39 3.39 0 0 0 15 11a3.5 3.5 0 0 0 0-7Z"/></svg></i>本页面贡献者：</span><span class=page_contributors>OI-wiki</span><br><span><i class=md-icon><svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M10.08 10.86c.05-.33.16-.62.3-.86.3-.56.81-.85 1.5-.86.45 0 .86.2 1.15.49.28.31.47.74.47 1.17h1.8c-.02-.47-.11-.9-.3-1.3-.15-.38-.38-.72-.68-1-1.45-1.34-4.14-1.15-5.37.37-1.29 1.67-1.32 4.59-.01 6.26 1.21 1.49 3.86 1.7 5.3.37.31-.25.56-.56.76-.92.16-.36.27-.74.28-1.15H13.5c0 .21-.07.4-.16.57-.09.19-.21.34-.34.47-.33.26-.72.4-1.14.4-.36-.01-.66-.08-.89-.23a1.41 1.41 0 0 1-.59-.64c-.5-.9-.42-2.15-.3-3.14M12 2C6.5 2 2 6.5 2 12c.53 13.27 19.5 13.26 20 0 0-5.5-4.5-10-10-10m0 18c-4.41 0-8-3.59-8-8 .44-10.61 15.56-10.61 16 0 0 4.41-3.59 8-8 8Z"/></svg></i>本页面的全部内容在 <strong></strong> 协议之条款下提供，附加条款亦可能应用</span></blockquote></article></div></div></main><script>function scrollFunction(){20<document.body.scrollTop||20<document.documentElement.scrollTop?document.getElementById("myBtn").style.display="block":document.getElementById("myBtn").style.display="none"}function topFunction(){document.body.scrollTop=0,document.documentElement.scrollTop=0}window.onscroll=function(){scrollFunction()}</script><button class=data-tip-left data-tip=回到顶部 id=myBtn onclick=topFunction()><svg class="Zi Zi--BackToTop data-tip-left"viewbox="0 0 24 24"data-tip=回到顶部 fill=currentColor height=24 width=24><path d="M16.036 19.59a1 1 0 0 1-.997.995H9.032a.996.996 0 0 1-.997-.996v-7.005H5.03c-1.1 0-1.36-.633-.578-1.416L11.33 4.29a1.003 1.003 0 0 1 1.412 0l6.878 6.88c.782.78.523 1.415-.58 1.415h-3.004v7.005z"></path></svg></button><footer class=md-footer><div class="md-footer-meta md-typeset"><div class="md-footer-meta__inner md-grid"><div class=md-copyright>Made with <a href=https://squidfunk.github.io/mkdocs-material/ rel=noopener target=_blank> Material for MkDocs <div id=miitbeian></div> </a></div><div class=build_date_utc style=float:right><a href=https://github.com/OI-wiki/OI-wiki> 最近更新：, 2024-07-21 </a></div><script>"oi-wiki.com"==window.location.hostname&&(document.getElementById("miitbeian").innerHTML='<a href="http://beian.miit.gov.cn/">黑ICP备19005132号-2</a>'),console.log("%c OI Wiki %c  %c","background:#35495e ; padding: 1px; border-radius: 3px 0 0 3px;  color: #fff","background:#41b883 ; padding: 1px; border-radius: 0 3px 3px 0;  color: #fff","background:transparent"),console.log("少年，恭喜你囍提彩蛋，我们在做一些 OI 相关的有趣的事情，如果您对此感兴趣，欢迎访问 https://join-us.oi-wiki.org")</script></div></div></footer></div><div class=md-dialog data-md-component=dialog><div class="md-dialog__inner md-typeset"></div></div><script id=__config type=application/json>{"base": "../../..", "features": ["content.action.edit", "navigation.tabs", "navigation.instant"], "search": "../../../assets/javascripts/workers/search.b6c651e9.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script><script src=../../../assets/javascripts/bundle.69a41b14.min.js></script><script src=../../../_static/js/math-csr.js?math-csr></script><script src=../../../assets/vendor/mathjax/es5/tex-mml-chtml.js?math-csr></script><script>"use strict";"serviceWorker"in navigator&&navigator.serviceWorker.register("/service-worker.js",{scope:"/"}).then(function(e){console.log("PWA Registration succeeded. Scope is "+e.scope)}).catch(function(e){console.log("PWA Registration failed with "+e)})</script></body></html>